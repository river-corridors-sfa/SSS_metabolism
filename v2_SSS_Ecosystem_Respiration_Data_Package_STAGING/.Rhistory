library(streamMetabolizer)
library(dplyr)
library(unitted)
library(ggplot2)
library(tidyr)
library(devtools)
library(rstan)
library(lubridate)
print("PARENT_ID: ")
PARENT_ID
print("SITE_ID: ")
SITE_ID
osat<- function(temp, bp) {
tstd<-log((298.15-temp) / (273.15 + temp))
a0<-2.00907
a1<-3.22014
a2<-4.0501
a3<-4.94457
a4<- -0.256847
a5<- 3.88767
u<-10^(8.10765-(1750.286/(235+temp)))
sato<-(exp(a0 + a1*tstd + a2*tstd^2 + a3*tstd^3 + a4*tstd^4+a5*tstd^5))*((bp-u)/(760-u))*1.42905
sato
}
####function returns mm of Hg - not using because we have logged time-series BP
#bpcalc<- function(bpst, alt) {
#  bpst*25.4*exp((-9.80665*0.0289644*alt)/(8.31447*(273.15+15)))
#}
```
```{r data2, echo = TRUE, warning=FALSE, message=FALSE}
data.path = "/SSS_Respiration_Data_Package/Inputs/Sensor_Files" #----------------
setwd(data.path)
getwd()
#setwd(data.path)
dat = read.csv(paste(data.path,PARENT_ID,"_Temp_DO_Press_Depth.csv",sep=''),header=T,skip=8)
PARENT_ID = 'SSS0001"
print("SITE_ID: ")
PARENT_ID = "SSS0001"
print("SITE_ID: ")
print("PARENT_ID: ")
PARENT_ID = "SSS0001"
print("SITE_ID: ")
SITE_ID = 'S63'
data.path = "/SSS_Respiration_Data_Package/Inputs/Sensor_Files/" #----------------
#setwd(data.path)
dat = read.csv(paste(data.path,PARENT_ID,"_Temp_DO_Press_Depth.csv",sep=''),header=T,skip=8)
paste(data.path,PARENT_ID,"_Temp_DO_Press_Depth.csv",sep='')
data.path = "/Inputs/Sensor_Files/" #----------------
#setwd(data.path)
dat = read.csv(paste(data.path,PARENT_ID,"_Temp_DO_Press_Depth.csv",sep=''),header=T,skip=8)
paste(data.path,PARENT_ID,"_Temp_DO_Press_Depth.csv",sep='')
data.path = "Inputs/Sensor_Files/" #----------------
#setwd(data.path)
dat = read.csv(paste(data.path,PARENT_ID,"_Temp_DO_Press_Depth.csv",sep=''),header=T,skip=8)
data.path = "Inputs\Sensor_Files\" #----------------
#setwd(data.path)
dat = read.csv(paste(data.path,PARENT_ID,"_Temp_DO_Press_Depth.csv",sep=''),header=T,skip=8)
data.path = "Inputs/Sensor_Files/" #----------------
#setwd(data.path)
dat = read.csv(paste(data.path,PARENT_ID,"_Temp_DO_Press_Depth.csv",sep=''),header=T,skip=8)
data.path = "../Inputs/Sensor_Files/" #----------------
#setwd(data.path)
dat = read.csv(paste(data.path,PARENT_ID,"_Temp_DO_Press_Depth.csv",sep=''),header=T,skip=8)
getwd()
paste(data.path,PARENT_ID,"_Temp_DO_Press_Depth.csv",sep='')
#setwd(data.path)
dat = read.csv(paste0(data.path,PARENT_ID,"_Temp_DO_Press_Depth.csv"),header=T,skip=8)
PARENT_ID = "SSS001"
#setwd(data.path)
dat = read.csv(paste0(data.path,PARENT_ID,"_Temp_DO_Press_Depth.csv"),header=T,skip=8)
paste0(data.path,PARENT_ID,"_Temp_DO_Press_Depth.csv")
data.path = "/Inputs/Sensor_Files/" #----------------
#setwd(data.path)
dat = read.csv(paste0(data.path,PARENT_ID,"_Temp_DO_Press_Depth.csv"),header=T,skip=8)
data.path = "Inputs/Sensor_Files/" #----------------
data.path="/SSS_Respiration_Data_Package/Inputs/"
#setwd(data.path)
dat = read.csv(paste0(data.path,PARENT_ID,"_Temp_DO_Press_Depth.csv"),header=T,skip=8)
#setwd(data.path)
dat = read.csv(paste0(data.path,"SSS001_Temp_DO_Press_Depth.csv"),header=T,skip=8)
data.path = "Inputs/Sensor_Files/" #----------------
#setwd(data.path)
dat = read.csv(paste0(data.path,"SSS001_Temp_DO_Press_Depth.csv"),header=T,skip=8)
#setwd(data.path)
dat = read.csv(paste0(data.path,PARENT_ID,"_Temp_DO_Press_Depth.csv"),header=T,skip=8)
data.path="/SSS_Respiration_Data_Package/Inputs/"
#setwd(data.path)
dat = read.csv(paste0(data.path,'Sensor_Files/',PARENT_ID,"_Temp_DO_Press_Depth.csv"),header=T,skip=8)
data.path = "Inputs/" #----------------
#setwd(data.path)
dat = read.csv(paste0(data.path,'Sensor_Files/',PARENT_ID,"_Temp_DO_Press_Depth.csv"),header=T,skip=8)
setwd(data.path)
K600estimates=read.csv('SSS_k600.csv',header=T,skip=3)
K600estimates=read.csv('v2_SSS_k600.csv',header=T,skip=3)
K600estimate<-K600estimates[K600estimates$Site_ID==SITE_ID,2]
print("k600 estimate: ")
K600estimate
output.path="/VGC_Outputs_Testing/"
#---------------------------------------------------------------------------------------------
file.name = paste('SSS_SM_',PARENT_ID,'_',SITE_ID,'_final',sep='')
#DOWNSAMPLE
samplingmins=15
dat = dat[seq(1, nrow(dat), samplingmins), ]
View(dat)
#data is always collected in pacific-standard-time, so conversion to UTC is +8 hours
dat$timeUTC<-as.POSIXct(dat$DateTime)+hours(8)
dat$timeUTC<-force_tz(dat$timeUTC,tzone='UTC')
dat$solar.time<-convert_UTC_to_solartime(dat$timeUTC, longitude= dat$Longitude[1], time.type="mean solar") #----------------------------------
View(dat)
dat$light<- calc_light(dat$solar.time, latitude=dat$Latitude[1], longitude=dat$Longitude[1], max.PAR =2300, attach.units = F) #------------------
temp = dat
dat = cbind.data.frame(temp$solar.time,temp$Dissolved_Oxygen,temp$DO.sat,temp$Temperature,temp$light,temp$Depth)
View(dat)
dat$DO.sat=osat(dat$Temperature,dat$Pressure) ###NOTE VGC HERE CHANGE HERE BOB'S EQUATION ####
temp = dat
dat = cbind.data.frame(temp$solar.time,temp$Dissolved_Oxygen,temp$DO.sat,temp$Temperature,temp$light,temp$Depth)
colnames(dat) = c("solar.time","DO.obs","DO.sat","temp.water","light","depth")
dat %>% unitted::v() %>%
mutate(DO.pctsat = 100 * (DO.obs / DO.sat)) %>%
select(solar.time, starts_with('DO')) %>%
gather(type, DO.value, starts_with('DO')) %>%
mutate(units=ifelse(type == 'DO.pctsat', 'DO\n(% sat)', 'DO\n(mg/L)')) %>%
ggplot(aes(x=solar.time, y=DO.value, color=type)) + geom_line() +
facet_grid(units ~ ., scale='free_y') + theme_bw() +
scale_color_discrete('variable')
labels = c(temp.water='water temp\n(deg C)', light='PAR\n(umol m^-2 s^-1)')
dat %>% unitted::v() %>%
select(solar.time, temp.water, light) %>%
gather(type, value, temp.water, light) %>%
mutate(
type=ordered(type, levels=c('temp.water','light')),
units=ordered(labels[type], unname(labels))) %>%
ggplot(aes(x=solar.time, y=value, color=type)) + geom_line() +
facet_grid(units ~ ., scale='free_y') + theme_bw() +
scale_color_discrete('variable')
bayes_name = mm_name(type='bayes',
pool_K600='normal',
err_obs_iid=TRUE,
err_proc_iid=TRUE)
bayes_name
# Changing the specs
bayes_specs = specs(bayes_name, K600_daily_meanlog_meanlog=log(K600estimate), K600_daily_meanlog_sdlog=0.7, K600_daily_sdlog_sigma=0.02, burnin_steps=1000,
saved_steps=1000)
mm = metab(bayes_specs, data=dat)#
mm = metab(bayes_specs, data=dat)#
mm = metab(bayes_specs, data=dat)#
mm = metab(bayes_specs, data=dat)#
mm = metab(bayes_specs, data=dat)#
# Chunk 1: setup
knitr::opts_chunk$set
#knitr::opts_chunk$set(eval = FALSE)
PARENT_ID=params$PARENT_ID
SITE_ID=params$SITE_ID
# Chunk 2: install
# install.packages(remotes); library(remotes)
# remotes::install_github('appling/unitted', force = TRUE)
#remotes::install_github("USGS-R/streamMetabolizer", force = TRUE)
# install.packages("rstan", dependencies = FALSE)
# install.packages('devtools')
# If you have trouble installing rstan, try the installation codes below:
#devtools::install_github("stan-dev/rstan", ref = "develop", subdir = "rstan/rstan", force = TRUE)
#install.packages("rstan", type = "source")
# Run the line below if you have trouble installing devtools
#devtools::install_github("stan-dev/rstan", ref = "develop", subdir = "rstan/rstan")
# Chunk 3: libraries
#rm(list=ls(all=T))
library(streamMetabolizer)
library(dplyr)
library(unitted)
library(ggplot2)
library(tidyr)
library(devtools)
library(rstan)
library(lubridate)
# Correctly installing rstan can be problematic, see GitHub for issues
# Chunk 4: data
#-------------------------------------
#PARENT_ID='' #this is where you would specify a site if you were not running this as a loop
#-------------------------------------
print("PARENT_ID: ")
PARENT_ID = "SSS001"
print("SITE_ID: ")
SITE_ID = 'S63'
osat<- function(temp, bp) {
tstd<-log((298.15-temp) / (273.15 + temp))
a0<-2.00907
a1<-3.22014
a2<-4.0501
a3<-4.94457
a4<- -0.256847
a5<- 3.88767
u<-10^(8.10765-(1750.286/(235+temp)))
sato<-(exp(a0 + a1*tstd + a2*tstd^2 + a3*tstd^3 + a4*tstd^4+a5*tstd^5))*((bp-u)/(760-u))*1.42905
sato
}
####function returns mm of Hg - not using because we have logged time-series BP
#bpcalc<- function(bpst, alt) {
#  bpst*25.4*exp((-9.80665*0.0289644*alt)/(8.31447*(273.15+15)))
#}
# Chunk 5: data2
#bp<-bpcalc(29.9, 489/3.28)# elevation at satus (alt in meters) #---------------------------------------------------------------------
data.path = "Inputs/" #----------------
#----------------
#setwd(data.path)
dat = read.csv(paste0(data.path,'Sensor_Files/',PARENT_ID,"_Temp_DO_Press_Depth.csv"),header=T,skip=8)
setwd(data.path)
K600estimates=read.csv('v2_SSS_k600.csv',header=T,skip=3)
K600estimate<-K600estimates[K600estimates$Site_ID==SITE_ID,2]
print("k600 estimate: ")
K600estimate
output.path="/VGC_Outputs_Testing/"
#---------------------------------------------------------------------------------------------
file.name = paste('SSS_SM_',PARENT_ID,'_',SITE_ID,'_final',sep='')
#dat = na.omit(dat)
#dat=dat[450:33311-300,] #-------------------------------------------------------------------------------------------------------------
# Change date time format
#DOWNSAMPLE
samplingmins=15
dat = dat[seq(1, nrow(dat), samplingmins), ]
#colnames(dat)[10]="Unix.Timestamp"
#dat$timeUTC<-as_datetime(dat$Unix.Timestamp)
#data is always collected in pacific-standard-time, so conversion to UTC is +8 hours
dat$timeUTC<-as.POSIXct(dat$DateTime)+hours(8)
dat$timeUTC<-force_tz(dat$timeUTC,tzone='UTC')
dat$solar.time<-convert_UTC_to_solartime(dat$timeUTC, longitude= dat$Longitude[1], time.type="mean solar") #----------------------------------
dat$light<- calc_light(dat$solar.time, latitude=dat$Latitude[1], longitude=dat$Longitude[1], max.PAR =2300, attach.units = F) #------------------
dat$DO.sat=osat(dat$Temperature,dat$Pressure) ###NOTE VGC HERE CHANGE HERE BOB'S EQUATION ####
# Selecting the data types that are needed for stream metabolizer and changing header names. Running the model with K600_pooling = normal does not require discharge input
temp = dat
dat = cbind.data.frame(temp$solar.time,temp$Dissolved_Oxygen,temp$DO.sat,temp$Temperature,temp$light,temp$Depth)
colnames(dat) = c("solar.time","DO.obs","DO.sat","temp.water","light","depth")
# Chunk 6: core
parallel::detectCores()
# Chunk 7: inspect
dat %>% unitted::v() %>%
mutate(DO.pctsat = 100 * (DO.obs / DO.sat)) %>%
select(solar.time, starts_with('DO')) %>%
gather(type, DO.value, starts_with('DO')) %>%
mutate(units=ifelse(type == 'DO.pctsat', 'DO\n(% sat)', 'DO\n(mg/L)')) %>%
ggplot(aes(x=solar.time, y=DO.value, color=type)) + geom_line() +
facet_grid(units ~ ., scale='free_y') + theme_bw() +
scale_color_discrete('variable')
labels = c(temp.water='water temp\n(deg C)', light='PAR\n(umol m^-2 s^-1)')
dat %>% unitted::v() %>%
select(solar.time, temp.water, light) %>%
gather(type, value, temp.water, light) %>%
mutate(
type=ordered(type, levels=c('temp.water','light')),
units=ordered(labels[type], unname(labels))) %>%
ggplot(aes(x=solar.time, y=value, color=type)) + geom_line() +
facet_grid(units ~ ., scale='free_y') + theme_bw() +
scale_color_discrete('variable')
labels = c(temp.water='water temp\n(deg C)', depth='water depth')
dat %>% unitted::v() %>%
select(solar.time, temp.water, depth) %>%
gather(type, value, temp.water, depth) %>%
mutate(
type=ordered(type, levels=c('temp.water','depth')),
units=ordered(labels[type], unname(labels))) %>%
ggplot(aes(x=solar.time, y=value, color=type)) + geom_line() +
facet_grid(units ~ ., scale='free_y') + theme_bw() +
scale_color_discrete('variable')
bayes_name = mm_name(type='bayes',
pool_K600='normal',
err_obs_iid=TRUE,
err_proc_iid=TRUE)
bayes_name
# Changing the specs
bayes_specs = specs(bayes_name, K600_daily_meanlog_meanlog=log(K600estimate), K600_daily_meanlog_sdlog=0.7, K600_daily_sdlog_sigma=0.02, burnin_steps=1000,
saved_steps=1000)
mm = metab(bayes_specs, data=dat)#
mm = metab(bayes_specs, data=dat)#
data1<-read.csv("/SSS_Data_Package/SSS_Water_Depth_Summary.csv",skip=8) #from this data package: https://data.ess-dive.lbl.gov/datasets/doi:10.15485/1969566
data1<-read.csv("/Inputs/SSS_Water_Depth_Summary.csv",skip=8) #from this data package: https://data.ess-dive.lbl.gov/datasets/doi:10.15485/1969566
getwd()
data1<-read.csv("/Inputs/SSS_Water_Depth_Summary.csv",skip=8) #from this data package: https://data.ess-dive.lbl.gov/datasets/doi:10.15485/1969566
data2<-read.csv("/Inputs/SSS_Slope_Discharge_Velocity.csv", skip = 6) #We obtained the streamflow and flow velocity from NOAA National Water Model (NWM) retrospective dataset using version 2.1 of NWM. The dataset provides a 42-year (February through December 2020) hourly simulation at all sampling locations (except COMID 24125857). We computed the monthly averaged streamflow and velocity in August across these simulation years. Slope was extracted from NHDplus v2.1
data2<-read.csv("/Inputs/v2_SSS_Slope_Discharge_Velocity.csv", skip = 6) #We obtained the streamflow and flow velocity from NOAA National Water Model (NWM) retrospective dataset using version 2.1 of NWM. The dataset provides a 42-year (February through December 2020) hourly simulation at all sampling locations (except COMID 24125857). We computed the monthly averaged streamflow and velocity in August across these simulation years. Slope was extracted from NHDplus v2.1
data2<-read.csv("Inputs/v2_SSS_Slope_Discharge_Velocity.csv", skip = 6) #We obtained the streamflow and flow velocity from NOAA National Water Model (NWM) retrospective dataset using version 2.1 of NWM. The dataset provides a 42-year (February through December 2020) hourly simulation at all sampling locations (except COMID 24125857). We computed the monthly averaged streamflow and velocity in August across these simulation years. Slope was extracted from NHDplus v2.1
data1<-read.csv("Inputs/SSS_Water_Depth_Summary.csv",skip=8) #from this data package: https://data.ess-dive.lbl.gov/datasets/doi:10.15485/1969566
colnames(data2)<-c("Site_ID","comid","slope","discharge","velocity")
View(data1)
data1$Average_Depth_m=data1$Average_Depth/100
data1 <-data1[order(data1$Site_ID),]
data2 <-data2[order(data2$Site_ID),]
data = merge(data1,data2, by = 'Site_ID')
library(tidyverse);library(dplyr)
data = data %>% select("Site_ID","Average_Depth_m","discharge","velocity","slope")
View(data)
data$k600_md<-4725*(data$velocity*data$slope)^0.86*data$discharge^-0.14*data$Average_Depth_m^0.66
data$K600_1_over_d = data$k600_md/data$Average_Depth_m
hist(data$K600_1_over_d)
hist(data$k600_md)
data$K600_1_per_d = data$k600_md/data$Average_Depth_m
output<-data.frame(data[c("Site_ID","K600_1_per_d")])
colnames(output)=c("Site_ID","K600")
output$K600<-round(output$K600,2)
write_csv(header, 'v2_SSS_k600.csv', col_names = F)
header <- tibble::tibble(header = c('# HeaderRows_4',
'# HeaderRows_Format: Column_Header; Unit; InstallationMethod_ID; Instrument_Summary',
'k600; meters_per_day; N/A; Calculated using (1) depth (2) flow (3) velocity and (4) slope as inputs for the Raymond et al. (2012) table 2 equation 7.'))
View(header)
header <- tibble::tibble(header = c('# HeaderRows_4',
'# HeaderRows_Format: Column_Header; Unit; InstallationMethod_ID; Instrument_Summary',
'K600; 1_per_day; N/A; Calculated using (1) depth (2) flow (3) velocity and (4) slope as inputs for the Raymond et al. (2012) table 2 equation 7. The result from that calculation is then divided by (1) depth'))
write_csv(header, 'v2_SSS_k600.csv', col_names = F)
write_csv(output, 'v2_SSS_k600.csv', append = T, col_names = T)
write_csv(header, 'v2_SSS_K600.csv', col_names = F)
write_csv(output, 'v2_SSS_K600.csv', append = T, col_names = T)
write_csv(header, 'Inputs/v2_SSS_K600.csv', col_names = F)
write_csv(output, 'Inputs/v2_SSS_K600.csv', append = T, col_names = T)
header <- tibble::tibble(header = c('# HeaderRows_4',
'# HeaderRows_Format: Column_Header; Unit; InstallationMethod_ID; Instrument_Summary',
'K600; 1_per_day; N/A; Calculated using (1) depth (2) flow (3) velocity and (4) slope as inputs for the Raymond et al. (2012) table 2 equation 7. The result from that calculation is then divided by depth'))
write_csv(header, 'Inputs/v2_SSS_K600.csv', col_names = F)
write_csv(output, 'Inputs/v2_SSS_K600.csv', append = T, col_names = T)
metadata.path="C:/Users/gara009/OneDrive - PNNL/Documents/GitHub/SSS_metabolism/Published_Data/SSS_Data_Package/" #from this data package: https://data.ess-dive.lbl.gov/datasets/doi:10.15485/1969566
output.path="VGC_Outputs_Testing/"
setwd(output.path)
getw()
getwd()
setwd("~/GitHub/SSS_metabolism/v2_SSS_Ecosystem_Respiration_Data_Package_STAGING")
paste(output.path,'v2_',PARENT_ID,'_',SITE_ID,'_SM_output.HTML',sep="")
##Matt Kaufman matthew.kaufman@pnnl.gov Pacific Northwest National Laboratory
##This script loads the list of SSS sites from the published metadata and loops through them all,
##running the stream metabolizer template for each one
metadata.path="C:/Users/gara009/OneDrive - PNNL/Documents/GitHub/SSS_metabolism/Published_Data/SSS_Data_Package/" #from this data package: https://data.ess-dive.lbl.gov/datasets/doi:10.15485/1969566
output.path="VGC_Outputs_Testing/"
metadata=read.csv(paste(metadata.path,'SSS_Metadata_IGSN-Mapping.csv',sep=''),skip=1,header=T)
metadata <- metadata[grepl("Water", metadata$Sample_Name, ignore.case = TRUE),]
SITE_LIST<-data.frame(substring(metadata$Sample_Name,0,6),metadata$Locality)
colnames(SITE_LIST)<-c('PARENT_ID','SITE_ID')
#for(i in 1:length(SITE_LIST[,1])) {
for(i in 1:1) {
PARENT_ID<-SITE_LIST[i,1]
SITE_ID<-SITE_LIST[i,2]
htmlfilename=paste(output.path,'v2_',PARENT_ID,'_',SITE_ID,'_SM_output.HTML',sep="")
rmarkdown::render("v2_SSS_SM_final_template.Rmd",
output_file = htmlfilename,
params = list(
PARENT_ID=PARENT_ID,
SITE_ID=SITE_ID
))
}
##Matt Kaufman matthew.kaufman@pnnl.gov Pacific Northwest National Laboratory
##This script loads the list of SSS sites from the published metadata and loops through them all,
##running the stream metabolizer template for each one
metadata.path="C:/Users/gara009/OneDrive - PNNL/Documents/GitHub/SSS_metabolism/Published_Data/SSS_Data_Package/" #from this data package: https://data.ess-dive.lbl.gov/datasets/doi:10.15485/1969566
output.path="VGC_Outputs_Testing/"
metadata=read.csv(paste(metadata.path,'SSS_Metadata_IGSN-Mapping.csv',sep=''),skip=1,header=T)
metadata <- metadata[grepl("Water", metadata$Sample_Name, ignore.case = TRUE),]
SITE_LIST<-data.frame(substring(metadata$Sample_Name,0,6),metadata$Locality)
colnames(SITE_LIST)<-c('PARENT_ID','SITE_ID')
#for(i in 1:length(SITE_LIST[,1])) {
for(i in 1:1) {
PARENT_ID<-SITE_LIST[i,1]
SITE_ID<-SITE_LIST[i,2]
htmlfilename=paste(output.path,'v2_',PARENT_ID,'_',SITE_ID,'_SM_output.HTML',sep="")
rmarkdown::render("Scripts/v2_SSS_SM_final_template.Rmd",
output_file = htmlfilename,
params = list(
PARENT_ID=PARENT_ID,
SITE_ID=SITE_ID
))
}
getwd()
output.path= "VGC_Outputs_Testing/"
##Matt Kaufman matthew.kaufman@pnnl.gov Pacific Northwest National Laboratory
##This script loads the list of SSS sites from the published metadata and loops through them all,
##running the stream metabolizer template for each one
metadata.path="C:/Users/gara009/OneDrive - PNNL/Documents/GitHub/SSS_metabolism/Published_Data/SSS_Data_Package/" #from this data package: https://data.ess-dive.lbl.gov/datasets/doi:10.15485/1969566
output.path= "VGC_Outputs_Testing/"
metadata=read.csv(paste(metadata.path,'SSS_Metadata_IGSN-Mapping.csv',sep=''),skip=1,header=T)
metadata <- metadata[grepl("Water", metadata$Sample_Name, ignore.case = TRUE),]
SITE_LIST<-data.frame(substring(metadata$Sample_Name,0,6),metadata$Locality)
colnames(SITE_LIST)<-c('PARENT_ID','SITE_ID')
i = 1
PARENT_ID<-SITE_LIST[i,1]
SITE_ID<-SITE_LIST[i,2]
htmlfilename=paste(output.path,'v2_',PARENT_ID,'_',SITE_ID,'_SM_output.HTML',sep="")
rmarkdown::render("Scripts/v2_SSS_SM_final_template.Rmd",
output_file = htmlfilename,
params = list(
PARENT_ID=PARENT_ID,
SITE_ID=SITE_ID
))
htmlfilename=paste0(output.path,'v2_',PARENT_ID,'_',SITE_ID,'_SM_output.HTML')
rmarkdown::render("Scripts/v2_SSS_SM_final_template.Rmd",
output_file = htmlfilename,
params = list(
PARENT_ID=PARENT_ID,
SITE_ID=SITE_ID
))
paste0(output.path,'v2_',PARENT_ID,'_',SITE_ID,'_SM_output.HTML')
htmlfilename=paste0(getwd(),output.path,'v2_',PARENT_ID,'_',SITE_ID,'_SM_output.HTML')
rmarkdown::render("Scripts/v2_SSS_SM_final_template.Rmd",
output_file = htmlfilename,
params = list(
PARENT_ID=PARENT_ID,
SITE_ID=SITE_ID
))
htmlfilename=paste0(getwd(),"/",output.path,'v2_',PARENT_ID,'_',SITE_ID,'_SM_output.HTML')
rmarkdown::render("Scripts/v2_SSS_SM_final_template.Rmd",
output_file = htmlfilename,
params = list(
PARENT_ID=PARENT_ID,
SITE_ID=SITE_ID
))
