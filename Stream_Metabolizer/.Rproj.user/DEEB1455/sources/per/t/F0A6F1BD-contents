---
title: "stream metabolizer SSS template"
author: "Vanessa Garayburu-Caruso and Matt Kaufman"
date: "`r Sys.Date()`"
output: html_document
#output_file: 'NA'
params:
  PARENT_ID: 'NA'
  SITE_ID: 'NA'

---

```{r setup, include=FALSE}
#knitr::opts_chunk$set
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # Sets working Directory to the R project
PARENT_ID=params$PARENT_ID
SITE_ID=params$SITE_ID
```
# StreamMetabolizer

Vanessa Garayburu-Caruso and Matt Kaufman, vanessa-garayburu-caruso@pnnl.gov, Pacific Northwest National Laboratory

This code is the template file that controls most of the stream metabolizer runs for the 2022 spatial study dataset. It takes in time-series dissolved oxygen, depth, and barometric pressure data, as well as single-point location data and an estimate of K600. From all of that, it estimates GPP, ER, and K600 on a daily basis over the course of the deployment. 

## 1. Install packages and loading libraries
If this is your first time running this code or streamMetabolizer, make sure to install the appropriate packages below. uncomment them in the chunk before Knitting.

```{r install, echo = TRUE, warning=FALSE, message=FALSE}
# install.packages(remotes); library(remotes)
# remotes::install_github('appling/unitted', force = TRUE)
#library('chron')
# remotes::install_github("DOI-USGS/streamMetabolizer", force = TRUE, build_vignettes = TRUE)
# install.packages("rstan", dependencies = FALSE)
# install.packages('devtools')

# If you have trouble installing rstan, try the installation codes below: 

#devtools::install_github("stan-dev/rstan", ref = "develop", subdir = "rstan/rstan", force = TRUE)
#install.packages("rstan", type = "source")

# Run the line below if you have trouble installing devtools
#devtools::install_github("stan-dev/rstan", ref = "develop", subdir = "rstan/rstan")
```

### Loading libraries
```{r libraries, echo = FALSE, warning=FALSE, message=FALSE}
library(streamMetabolizer)
library(dplyr)
library(unitted)
library(ggplot2)
library(tidyr)
library(devtools)
library(rstan)
library(lubridate)
library(gridExtra) # for arranging plots
library(plotly)
library(tidyverse)
# Correctly installing rstan can be problematic, see GitHub for issues
```

## 2. Setting up the data
Read in the data and change units to match the needs of stream metabolize

Note: Make sure to always double check that your date time column is in the date time format and not as character

```{r data, echo = TRUE, warning=FALSE, message=FALSE}

#Note:
#PARENT_ID='' #this is where you would specify a site if you were not running this as a loop


print("PARENT_ID: ")
PARENT_ID

print("SITE_ID: ")
SITE_ID

```

```{r data2, echo = TRUE, warning=FALSE, message=FALSE}

# Read in data 
data.path = "Inputs/"

dat = read_csv(paste0(data.path,'Sensor_Files/v2_',PARENT_ID,"_Temp_DO_Press_Depth.csv"),comment = '#', na = c('N/A', '', 'NA', '-9999'))

 
K600estimates=read.csv(paste0(data.path,'v2_SSS_K600.csv'),header=T,skip=3)
K600estimate<-K600estimates[K600estimates$Site_ID==SITE_ID,2]
 
print("K600 estimate: ")
K600estimate

# Set up Output directory and file name

output.path="Outputs/"

file.name = paste('v2_SSS_SM_',PARENT_ID,'_',SITE_ID,'_final',sep='')

# Get data ready for streamMetabolizer
 
#DateTime: data is always collected in pacific-standard-time, so conversion to UTC is +8 hours
dat$timeUTC<-as.POSIXct(dat$DateTime)+hours(8)
dat$timeUTC<-force_tz(dat$timeUTC,tzone='UTC')
dat$solar.time<-convert_UTC_to_solartime(dat$timeUTC, longitude= dat$Longitude[1], time.type="mean solar") 

# Calculate light
dat$light <- calc_light(dat$solar.time, latitude=dat$Latitude[1], longitude=dat$Longitude[1], max.PAR =2300, attach.units = F) 

# Calculate DO sat: cal_DO_stat requires barometric pressure in millibars, or a united object of barometric pressure.
dat$DO.sat=calc_DO_sat(dat$Temperature,dat$Pressure, model = 'garcia-benson') 


# Selecting the data types that are needed for streamMetabolizer and changing header names. Running the model with K600_pooling = normal does not require discharge input

dat = dat %>% dplyr::select("solar.time" = solar.time,"DO.obs" = Dissolved_Oxygen,"DO.sat" = DO.sat,"temp.water" = Temperature,"light" = light,"depth" = Depth)

```

Check the number of cores you have in your computer. Based on the number that it prints, set the number of cores you want to dedicate to the metabolism run. It is recommended to set 2-4 cores less than you have in your computer to minimize the chances of R crashing. It is also recommended to select a pair number for your run. 

```{r core, echo = TRUE, warning=FALSE, message=FALSE}
parallel::detectCores()

```

## 3. Inspect Data
```{r inspect, echo = FALSE, warning=FALSE, message=FALSE}
# Clear any previous plot objects
rm(plot)
rm(plotly_plot)
unlink("plotly_plot.html")

labels <- c(
  DO.obs = 'DO.obs\n(mg/L)', 
  DO.sat = 'DO.sat\n(mg/L)', 
  DO.pctsat = 'DO\n(% sat)', 
  temp.water = 'water temp\n(deg C)', 
  light = 'PAR\n(umol m^-2 s^-1)', 
  depth = 'water depth (m)'
)

# Create the ggplot object
plot <- dat %>% 
  unitted::v() %>%
  mutate(DO.pctsat = 100 * (DO.obs / DO.sat)) %>%
  select(solar.time, starts_with('DO'), temp.water, light, depth) %>%
  gather(type, value, -solar.time) %>%
  mutate(
    units = factor(type, levels = names(labels), labels = unname(labels)),
    type = factor(type, levels = names(labels))
  ) %>%
  ggplot(aes(x = solar.time, y = value, color = type)) + 
  geom_point() + 
  facet_grid(units ~ ., scale = 'free_y') + 
  theme_bw() + 
  scale_color_discrete('variable') +
  theme(legend.position = "none") # Turn off legend
plot 

# Convert ggplot to plotly object without printing the ggplot
plotly_plot <- ggplotly(plot)
rm(plot)  # Remove ggplot to prevent any display

# Save the plotly object as an HTML file
inspection.name <- paste0('Data_Inspection_', PARENT_ID, '_', SITE_ID)
inspection.path = 'Outputs/Data_Inspection/'
full.inspection.path <- paste0(inspection.path, inspection.name, ".html")

invisible(htmlwidgets::saveWidget(as_widget(plotly_plot), full.inspection.path))

# Clean up the plotly object
rm(plotly_plot)
```

## 4. Configure the model
```{r modelsetup, echo = TRUE, warning=TRUE}
# Set the model

bayes_name = mm_name(type='bayes',
                     pool_K600='normal', 
                     err_obs_iid=TRUE, 
                     err_proc_iid=TRUE)
bayes_name
 # Options for pool K600 are binned, linear, none and normal. If normal is specified, discharge doesn't need to be provided

# Changing the specs
s1 = c('S56N','S32','S49R', 'S11','S52','T05P','S17R','S23','S29','S03') # Run at 0.02
sites = c('S37','S10','S15','U20', 'S39') # Run at 0.08
s2 = c('S31','S45','S24','S41R','T03') # Run at 0.01
s3 = c('T02','W10') # Run at 0.1


if (SITE_ID %in% sites){
  bayes_specs = specs(bayes_name, K600_daily_meanlog_meanlog=log(K600estimate), K600_daily_meanlog_sdlog=0.7, K600_daily_sdlog_sigma=0.08, burnin_steps=2000, 
                  saved_steps=2000)
} else if(SITE_ID %in% s1){
  bayes_specs = specs(bayes_name, K600_daily_meanlog_meanlog=log(K600estimate), K600_daily_meanlog_sdlog=0.7, K600_daily_sdlog_sigma=0.02, burnin_steps=2000, 
                  saved_steps=2000) 
}else if(SITE_ID %in% s2){
  bayes_specs = specs(bayes_name, K600_daily_meanlog_meanlog=log(K600estimate), K600_daily_meanlog_sdlog=0.7, K600_daily_sdlog_sigma=0.01, burnin_steps=2000, 
                  saved_steps=2000) 
}else if(SITE_ID %in% s3){
  bayes_specs = specs(bayes_name, K600_daily_meanlog_meanlog=log(K600estimate), K600_daily_meanlog_sdlog=0.7, K600_daily_sdlog_sigma=0.5, burnin_steps=2000, 
                  saved_steps=2000) 
}else {
bayes_specs = specs(bayes_name, K600_daily_meanlog_meanlog=log(K600estimate), K600_daily_meanlog_sdlog=0.7, K600_daily_sdlog_sigma=0.05, burnin_steps=2000, 
                  saved_steps=2000)
}


```

## 5. Fit the model and save the results

Some key parameters to look at in the mm output are:  
- $daily (includes metabolism estimates) 
- $overall (includes error information) 
- $KQ_overall (included the relationship between K600-Q)

```{r fit, echo = TRUE, warning=TRUE}
mm = metab(bayes_specs, data=dat)# 

# Saving key data
preds = mm@fit$daily
preds_output<-preds
#str(preds)
write.csv(preds_output,paste0(output.path,'v2_',PARENT_ID,"_",SITE_ID,"_SM_final_daily_prediction_results.csv"),row.names = FALSE) 

instant = mm@fit$inst
instant_output<-instant
#str(instant)
write.csv(instant_output,paste0(output.path,'v2_',PARENT_ID,"_",SITE_ID,"_SM_final_instant_fit_results.csv"),row.names = FALSE)

Overall = mm@fit$overall
Overall_output<-Overall
#str(Overall)
write.csv(Overall_output,paste0(output.path,'v2_',PARENT_ID,"_",SITE_ID,"_SM_final_overall_fit_results.csv"),row.names = FALSE)

full = predict_DO(mm)
full_output<-full
#str(KQ)
write.csv(full_output,paste0(output.path,'v2_',PARENT_ID,"_",SITE_ID,"_SM_final_full_prediction_results.csv"),row.names = FALSE)

```

## 6. Inspect GPP, ER and K600

### Daily predictions of modeled GPP and ER

The goal is for the predictions (lines) and observations (points) to be very similar. 

```{r inspect11, echo = FALSE, warning=FALSE, message=FALSE}
# Get daily predictions of GPP and ER
# Clear any previous plot objects
rm(predictions_plot, fit_plots, fit_plots_2)
unlink("plotly_plot.html")
unlink("fit_plots_2.html")
# Generate predictions plot and do not display it
predictions_plot <- plot_metab_preds(mm)
invisible(predictions_plot)  # Prevents display
predictions_plot

# Generate the first fitted daily parameters plot without displaying it
fit_plots <- plot_DO_preds(mm)
fit_plots  # Prevents display

# Convert the ggplot to a plotly object
fit_plots_2 <- ggplotly(fit_plots)
rm(fit_plots)  # Clean up the original ggplot object

# Save the plotly object as an HTML file
fits.name <- paste0('SM_Model_Fits_', PARENT_ID, '_', SITE_ID)
fits.path <- 'Outputs/Model_Fits/'
full.fits.path <- paste0(fits.path, fits.name, ".html")

invisible(htmlwidgets::saveWidget(as_widget(fit_plots_2), full.fits.path))

# Clean up the plotly object
rm(fit_plots_2)

```

Ideally, good model results should have n_eff > 100 and Rhat < = 1.1. Below is a summary of these metrics for daily GPP, ER and K600. These values indicate the quality of the computation. 


```{r inspect2, echo = FALSE, warning=FALSE, message=FALSE}

# Extract relevant data
fit_daily <- get_fit(mm)$daily

# Daily Rhat summary
rhat_d <- fit_daily %>%
  select(ends_with('Rhat')) %>%
  pivot_longer(everything(), names_to = "parameter", values_to = "Rhat")%>%
    na.omit() 

# Daily n_eff summary
neff_d <- fit_daily %>%
  select(ends_with('n_eff')) %>%
  pivot_longer(everything(), names_to = "parameter", values_to = "n_eff") %>%
    na.omit() 

# Plot the distributions

plot_rhat <- ggplot(rhat_d, aes(x = Rhat, fill = parameter)) +
  geom_histogram(binwidth = 0.05, alpha = 0.7, position = 'identity') +
  geom_vline(xintercept = 1.1, color = "red", linetype = "dashed") +
  ggtitle("Distribution of Rhat Values") +
  theme_bw()+
  facet_wrap(~parameter)+
  theme(legend.position = "none") # Turn off legend
plot_rhat

plot_neff <- ggplot(neff_d, aes(x = n_eff, fill = parameter)) +
  geom_histogram(binwidth = 10, alpha = 0.7, position = 'identity') +
  geom_vline(xintercept = 100, color = "red", linetype = "dashed") +
  ggtitle("Distribution of n_eff Values") +
  theme_bw()+
  facet_wrap(~parameter)+
  theme(legend.position = "none") # Turn off legend

plot_neff

# Creating summary tables
rhat_summary <- rhat_d %>%
  group_by(parameter) %>%
  summarise(
    min = min(Rhat, na.rm = TRUE),
    mean = mean(Rhat, na.rm = TRUE),
    median = median(Rhat, na.rm = TRUE),
    max = max(Rhat, na.rm = TRUE),
    below_threshold = sum(Rhat <= 1.1, na.rm = TRUE) / n()
  )

neff_summary <- neff_d %>%
  group_by(parameter) %>%
  summarise(
    min = min(n_eff, na.rm = TRUE),
    mean = mean(n_eff, na.rm = TRUE),
    median = median(n_eff, na.rm = TRUE),
    max = max(n_eff, na.rm = TRUE),
    above_threshold = sum(n_eff > 100, na.rm = TRUE) / n()
  )


# Printing the summaries
print(rhat_summary)
print(neff_summary)
```

## 7. Inspect errors and their standard deviations

err_obs_iid_Rhat should have a value < 1.1 and err_obs_iid_sigma_Rhat ~ 1.05
.
### Inspect err_obs_iid_Rhat and err_obs_iid_sigma_Rhat
```{r inspect3, echo = TRUE, warning=FALSE, message=FALSE}
# Extracting the relevant data
inst_rhat <- get_fit(mm)$inst %>% select(ends_with('Rhat'))
overall_rhat <- get_fit(mm)$overall %>% select(ends_with('Rhat'))

# Combine the data into long format for easier plotting
rhat_data <- bind_rows(inst_rhat %>% 
                         gather(key="parameter", value="Rhat") %>% 
                         mutate(type="err_obs_iid_Rhat"),
                       overall_rhat %>% 
                         gather(key="parameter", value="Rhat") %>% 
                         mutate(type="err_obs_iid_sigma_Rhat"),
                       .id = "source")

# Plot for Rhat values
plot_rhat <- ggplot(rhat_data, aes(x=Rhat, fill=type)) + 
  geom_histogram(binwidth=0.01, alpha=0.7, position="identity") +
  labs(title="Distribution of Rhat values", x="Rhat", y="Count") +
  theme_minimal() +
  geom_vline(xintercept=1.1, linetype="dashed", color="red") +
  annotate("text", x=1.1, y=max(table(round(rhat_data$Rhat, 2))), label="1.1", vjust=-1, color="red") +
  scale_fill_manual(values=c("steelblue", "purple"), 
                    labels=c("err_obs_iid_Rhat", "err_obs_iid_sigma_Rhat")) +
  facet_wrap(~type)+
    theme(legend.position = "none") # Turn off legend


inst_neff <- get_fit(mm)$inst %>% select(ends_with('n_eff'))
overall_neff <- get_fit(mm)$overall %>% select(ends_with('n_eff'))

# Combine the data into long format for easier plotting, and label the type
neff_data <- bind_rows(inst_neff %>% 
                         gather(key="parameter", value="n_eff") %>% 
                         mutate(type="err_obs_iid_n_eff"),
                       overall_neff %>% 
                         gather(key="parameter", value="n_eff") %>% 
                         mutate(type="err_obs_iid_sigma_n_eff"),
                       .id = "source")

# Plot for n_eff values
plot_neff <- ggplot(neff_data, aes(x=n_eff, fill=type)) + 
  geom_histogram(binwidth=50, alpha=0.7, position="identity") +
  labs(title="Distribution of Effective Sample Sizes (n_eff)", x="n_eff", y="Count") +
  theme_minimal() +
  scale_fill_manual(values=c("orange", "green"), 
                    labels=c("err_obs_iid_n_eff", "err_obs_iid_sigma_n_eff"))+facet_wrap(~type)+
    theme(legend.position = "none") # Turn off legend


# Summary tables (to be displayed separately or alongside the plots)
summary_rhat <- rhat_data %>% 
  group_by(source, parameter) %>% 
  summarise(mean_Rhat = mean(Rhat), median_Rhat = median(Rhat), sd_Rhat = sd(Rhat), .groups='drop')

summary_neff <- neff_data %>% 
  group_by(source, parameter) %>% 
  summarise(mean_neff = mean(n_eff), median_neff = median(n_eff), sd_neff = sd(n_eff), .groups='drop')

# Print the plots and summaries
print(plot_rhat)
print(plot_neff)
print(summary_rhat)
print(summary_neff)

```

## 8. Inspect the relationships between variables

### Relationship between k600 and Q: we do not have Q in this experiment, so use depth as a proxy

The first step is to plot K600 and depth. To find this relationship we have to calculate the mean depth per day and then plot against K600.

```{r ins, echo = TRUE, warning=FALSE, message=FALSE}
    dat$solar.time = as.Date(dat$solar.time)
    meanDday = aggregate(dat["depth"],by = dat["solar.time"],mean)
    meanDday=meanDday[meanDday$solar.time %in% preds$date,]
    K600D = lm(preds$K600_daily_mean~meanDday$depth)

    plot(preds$K600_daily_mean,meanDday$depth,
         ylab="D_daily_mean",xlab="Daily_mean_K600")
    abline(lm(meanDday$depth~preds$K600_daily_mean))
      legend("topright", bty="n", legend=paste("R2 =", format(summary(K600D)$r.squared, digits=4)))
legend("top", bty="n", legend=paste("p = ", format(summary(K600D)$coefficients[8], digits=4)))

    summary(K600D)


```

### Relationship between k600 and ER

This relationship is key. A primary metric is to look for covariance between ER and K.  These have high equifinality which means that any combination of ER or K can provide an equally good fit to the model. If these covary strongly (r=0.6 or higher) then that may not be good for examining controls on variation in ER. If you find **no relationship**, that is a **very good** thing. If there is a relationship, that is if R2 is high, it means that the model can't parse reaeration from respiration. In this case, ER and k600 are working in opposite for O2, so ideally there is no relationship in the model outputs. If they covary, you can still use the period-avarage data, but day-to-day variation may not be trustworthy.

```{r ins plot, echo = TRUE, warning=FALSE, message=FALSE}
K600ER = lm(K600_daily_mean~ER_mean, data=preds)
   
    plot(preds$K600_daily_mean,preds$ER_mean,ylab="ER_mean",xlab="Daily_mean_K600")
    abline(lm(ER_mean~K600_daily_mean, data=preds))
    legend("topright", bty="n", legend=paste("R2 =", format(summary(K600ER)$r.squared, digits=4)))
legend("top", bty="n", legend=paste("p = ", format(summary(K600ER)$coefficients[8], digits=4)))
    
 summary(K600ER)
```

### Relationship between k600 and GPP
High GPP is needed to estimate K well, so K should get more variable as GPP decreases.

```{r ins plot2, echo = TRUE, warning=FALSE, message=FALSE}
K600GPP = lm(GPP_mean~K600_daily_mean, data=preds)

    
    plot(preds$K600_daily_mean,preds$GPP_mean,ylab="GPP_mean",xlab="Daily_mean_K600")
    abline(lm(GPP_mean~K600_daily_mean, data=preds))
    legend("topright", bty="n", legend=paste("R2 =", format(summary(K600GPP)$r.squared, digits=4)))
legend("top", bty="n", legend=paste("p = ", format(summary(K600GPP)$coefficients[8], digits=4)))
     
    summary(K600GPP)

```

### Time series daily mean k600 

```{r ins4, echo = TRUE, warning=FALSE, message=FALSE}
    preds$date = as.Date(preds$date)
  
    plot(preds$date,preds$K600_daily_mean,
         xlab="Date",ylab="Daily_mean_K600")
    
```
