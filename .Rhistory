boye_dir
dir <- 'C:/Users/forb086/OneDrive - PNNL/Data Generation and Files/RC4/Boye_Files/WROL/'
files <- list.files(boye_dir, 'Check_for_Duplicates', full.names = T)
boye_dir <- paste0(dir, RC, '/Boye_Files/', study_code, '/')
files <- list.files(boye_dir, 'Check_for_Duplicates', full.names = T)
boye_dir
dir <- 'C:/Users/forb086/OneDrive - PNNL/Data Generation and Files/'
boye_dir <- paste0(dir, RC, '/Boye_Files/', study_code, '/')
files <- list.files(boye_dir, 'Check_for_Duplicates', full.names = T)
boye_dir
hub <- read_excel(hub_dir)
colnames_lookup <- read_csv(colnames_lookup_dir, skip = 1)
for (file in files) {
data <- read_csv(file)
if('TRUE' %in% data$duplicate){
cat(
red$bold(
'Wait! You have duplicates in your data.\n',
'You will need to remove your duplicates \n',
'before you are able to proceed\n'
)
)
} else{
data <- data %>%
select(-Date_of_Run, -Method_Notes, -duplicate)
data_type <- unlist(str_split(basename(file), '_'))[2]
data_columns <- data %>%
select(-Sample_ID, -Methods_Deviation)%>%
colnames()
# ========================= get typical codes ==============================
filter_hub <- hub %>%
filter(RC == RC,
Study_Code == study_code,
Data_Package_Unit == material)
typical_code_number <-  filter_hub %>%
select(contains(data_type)) %>%
pull(1)
# ========================= build header rows ==============================
boye_file_headers <- tibble(
'Field_Name' = c('Unit', 'Unit_Basis', 'MethodID_Analysis', 'MethodID_Inspection',
'MethodID_Storage', 'MethodID_Preservation', 'MethodID_Preparation',
'MethodID_DataProcessing', 'Analysis_DetectionLimit',
'Analysis_Precision', 'Data_Status'),
'Sample_Name' = c('N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A',
'-9999', '-9999', 'N/A'),
'Material' = c('N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A',
'-9999', '-9999', 'N/A')
)
typical_codes <- read_excel(typical_codes_dir, sheet = paste0(data_type,"_Typical"))
for (column in data_columns) {
order <- c('MethodID_Analysis', 'MethodID_Inspection',
'MethodID_Storage', 'MethodID_Preservation',
'MethodID_Preparation', 'MethodID_DataProcessing')
column_typical_codes <- typical_codes %>%
filter(str_detect(Method_ID, typical_code_number)) %>%
slice(match(order, Method_Type))%>%
select(Method_ID)%>%
pull(n = 1)
unit <- colnames_lookup %>%
filter(col.in == column) %>%
select(Unit)%>%
pull(n=1)
unit_basis <- colnames_lookup %>%
filter(col.in == column) %>%
select(Unit_Basis)%>%
pull(n=1)
data_status <- colnames_lookup %>%
filter(col.in == column) %>%
select(Data_Status)%>%
pull(n=1)
boye_file_headers <- boye_file_headers %>%
add_column(
!!column := c(unit, unit_basis, column_typical_codes, '-9999', '-9999', data_status))
}
boye_file_headers <- boye_file_headers %>%
add_column('Methods_Deviation' = 'N/A')
#=============================== get LOD ==================================
if (data_type == "TSS"){
LOD <- read_xlsx(tss_LOD_file)
LOD_dates_file_info <-file.info(list.files(path = boye_dir,pattern = "TSS_LOD", full.names = T)) %>%
rownames_to_column('File_Name') %>%
tibble()
# get most recent LOD file
LOD_dates_file <- LOD_dates_file_info %>%
arrange(mtime) %>%
tail(1)%>%
select('File_Name')%>%
pull(1)
LOD_dates <- read_csv(LOD_dates_file)
LOD_min <- LOD %>%
filter(Date_start_YYYYMMDD <= LOD_dates$Date_start[1] & Date_end_YYYYMMDD >= LOD_dates$Date_start[1]) %>%
select(LOD_TSS_mg_per_L)%>%
pull(1)%>%
round(2)
LOD_max <- LOD %>%
filter(Date_start_YYYYMMDD <= LOD_dates$Date_end[1] & Date_end_YYYYMMDD >= LOD_dates$Date_end[1]) %>%
select(LOD_TSS_mg_per_L)%>%
pull(1)%>%
round(2)
if(LOD_min == LOD_max){
LOD_final = LOD_max
} else{
LOD_final = paste0(LOD_min[1],"-",LOD_max[1])
}
boye_file_headers <- boye_file_headers %>%
assign_in(list(column, 9), LOD_final)
} else {
if(data_type == 'Ions'){
LOD <- read_csv(ion_LOD_file)
LOD_dates_file_info <-file.info(list.files(path = boye_dir,pattern = "LOD", full.names = T)) %>%
rownames_to_column('File_Name') %>%
tibble()
# get most recent LOD file
LOD_dates_file <- LOD_dates_file_info %>%
arrange(mtime) %>%
tail(1)%>%
select('File_Name')%>%
pull(1)
LOD_dates <- read_csv(LOD_dates_file)
for (ion_column in data_columns) {
analyte <- unlist(str_split(ion_column, '_'))[1]
LOD_min <- LOD %>%
filter(Analyte == analyte,
Date_start_YYYYMMDD <= LOD_dates$Date_start[1] & Date_end_YYYYMMDD >= LOD_dates$Date_start[1]) %>%
select(LOD_ppm)%>%
pull(1)%>%
round(2)
LOD_max <- LOD %>%
filter(Analyte == analyte,
Date_start_YYYYMMDD <= LOD_dates$Date_end[1] & Date_end_YYYYMMDD >= LOD_dates$Date_end[1]) %>%
select(LOD_ppm)%>%
pull(1)%>%
round(2)
if(LOD_min == LOD_max){
LOD_final = LOD_max
} else{
LOD_final = paste0(LOD_min[1],"-",LOD_max[1])
}
boye_file_headers <- boye_file_headers %>%
assign_in(list(ion_column, 9), LOD_final)
}
} else {
LOD <- read_excel(LOD_file_dir)
LOD_dates_file_info <-file.info(list.files(path = boye_dir,pattern = "LOD", full.names = T)) %>%
rownames_to_column('File_Name') %>%
tibble()
# get most recent LOD file
LOD_dates_file <- LOD_dates_file_info %>%
arrange(mtime) %>%
tail(1)%>%
select('File_Name')%>%
pull(1)
LOD_dates <- read_csv(LOD_dates_file)
for (data_column in data_columns) {
analyte <- unlist(str_split(data_column, '_'))[1]
LOD_min <- LOD %>%
select(contains(analyte) & contains('LOD'), Date_start_YYYYMMDD, Date_end_YYYYMMDD) %>%
filter(Date_start_YYYYMMDD <= LOD_dates$Date_start[1] & Date_end_YYYYMMDD >= LOD_dates$Date_start[1]) %>%
pull(1)%>%
round(2)
LOD_max <- LOD %>%
select(contains(analyte) & contains('LOD'), Date_start_YYYYMMDD, Date_end_YYYYMMDD) %>%
filter(Date_start_YYYYMMDD <= LOD_dates$Date_end[1] & Date_end_YYYYMMDD >= LOD_dates$Date_end[1]) %>%
pull(1)%>%
round(2)
if(LOD_min == LOD_max){
LOD_final = LOD_max
} else{
LOD_final = paste0(LOD_min[1],"-",LOD_max[1])
}
boye_file_headers <- boye_file_headers %>%
assign_in(list(data_column, 9), LOD_final)
}
}
}
# ======================== fix column headers ==============================
for (colname in data_columns) {
colnames_lookup_filter <- colnames_lookup %>%
dplyr::filter(col.in == colname)
new_name <- colnames_lookup_filter$col.out
data <- data %>%
dplyr::rename(!!new_name := all_of(colname))
boye_file_headers <- boye_file_headers %>%
dplyr::rename(!!new_name := all_of(colname))
}
# ========================== finish formatting =============================
if (material == 'Water'){
data <- data %>%
add_column(Material = 'Liquid>aqueous', .after = 'Sample_ID')%>%
add_column(Field_Name = 'N/A', .before = 'Sample_ID')
} else {
data <- data %>%
add_column(Material = material, .after = 'Sample_ID')%>%
add_column(Field_Name = 'N/A', .before = 'Sample_ID')
}
data <- data %>%
dplyr::mutate(across(everything(), as.character))%>%
dplyr::mutate(across(contains('X'), replace_na, replace = '-9999'))%>%
dplyr::mutate(across(!contains('X'), replace_na, replace = 'N/A'))%>%
arrange(Sample_ID)
data$Field_Name[1] <- '#Start_Data'
colnames(data) <- gsub('X', '', colnames(data))
colnames(boye_file_headers) <- gsub('X', '', colnames(boye_file_headers))
data[nrow(data)+1,1] = "#End_Data"
columns <- length(data)-1
header_rows <- length(boye_file_headers$Field_Name) + 1
top <- tibble('one' = as.character(),
'two' = as.numeric()) %>%
add_row(one = '#Columns',
two = columns) %>%
add_row(one = '#Header_Rows',
two = header_rows)
# =================================== Write File ===============================
out_name <- glue('{boye_dir}{study_code}_{data_type}_Boye_{Sys.Date()}.csv' )
write_csv(top, out_name, col_names = F)
write_csv(boye_file_headers, out_name, append = T, col_names = T)
write_csv(data, out_name, append = T, na = '')
}
}
source("C:/Users/forb086/OneDrive - PNNL/Data Generation and Files/R code/Data_Package_Code/Boye_Code/02_Boye_Formatting.R", echo=TRUE)
source("C:/Users/forb086/OneDrive - PNNL/Data Generation and Files/R code/Data_Package_Code/Boye_Code/03_Methods_Codes.R", echo=TRUE)
library(tidyverse)
library(janitor)
rm(list=ls(all=T))
dir <- 'C:/Users/forb086/OneDrive - PNNL/Data Generation and Files/'
RC <- 'RC4'
study_code <- 'WROL'
material <- 'Water'
boye_dir <- paste0(dir, RC, '/Boye_Files/', study_code, '/')
analyte_files <- list.files(boye_dir, 'Boye', full.names = T)
analyte_files <- analyte_files[!grepl('ReadyForBoye',analyte_files)]
qaqc_files <- list.files(boye_dir, 'CombinedQAQC', full.names = T)
combine <- read_csv(analyte_files[1], skip = 2)%>%
filter(!Sample_Name %in% c('N/A', '-9999')) %>%
select("Field_Name", "Sample_Name", "Material") %>%
mutate(Field_Name = NA,
Sample_Name = str_remove(Sample_Name, '-1[:alpha:]'),
Sample_Name = str_remove(Sample_Name, '-2[:alpha:]'),
Sample_Name = str_remove(Sample_Name, '-3[:alpha:]'),
Sample_Name = str_remove(Sample_Name, '-4[:alpha:]'),
Sample_Name = str_remove(Sample_Name, '-5[:alpha:]'),
Sample_Name = str_remove(Sample_Name, '-6[:alpha:]'),
Sample_Name = str_remove(Sample_Name, '-1'),
Sample_Name = str_remove(Sample_Name, '-2'),
Sample_Name = str_remove(Sample_Name, '-3'),
Sample_Name = str_remove(Sample_Name, '_DIC'),
Sample_Name = str_remove(Sample_Name, '_ION'),
Sample_Name = str_remove(Sample_Name, '_OCN'),
Sample_Name = str_remove(Sample_Name, '_TN'),
Sample_Name = str_remove(Sample_Name, '_ICR'),
Sample_Name = str_remove(Sample_Name, '_TSS'),
Sample_Name = str_remove(Sample_Name, '_SFE'),
Sample_Name = str_remove(Sample_Name, '_INC'))%>%
add_column(Mean_Missing_Reps = NA) %>%
distinct()
source("C:/Users/forb086/OneDrive - PNNL/Data Generation and Files/R code/Data_Package_Code/Boye_Code/04_Boye_Summary_File.R", echo=TRUE)
library(tidyverse)
library(janitor)
rm(list=ls(all=T))
# ================================= User inputs ================================
dir <- 'C:/Users/forb086/OneDrive - PNNL/Data Generation and Files/'
RC <- 'RC4'
study_code <- 'WROL'
material <- 'Water'
# ====================================== Build dir name ========================
boye_dir <- paste0(dir, RC, '/Boye_Files/', study_code, '/')
# ================================= Wrangle data and summarize =================
analyte_files <- list.files(boye_dir, 'Boye', full.names = T)
analyte_files <- analyte_files[!grepl('ReadyForBoye',analyte_files)]
qaqc_files <- list.files(boye_dir, 'CombinedQAQC', full.names = T)
# ====================== create combined data frames ===========================
combine <- read_csv(analyte_files[1], skip = 2)%>%
filter(!Sample_Name %in% c('N/A', '-9999')) %>%
select("Field_Name", "Sample_Name", "Material") %>%
mutate(Field_Name = NA,
Sample_Name = str_remove(Sample_Name, '-1[:alpha:]'),
Sample_Name = str_remove(Sample_Name, '-2[:alpha:]'),
Sample_Name = str_remove(Sample_Name, '-3[:alpha:]'),
Sample_Name = str_remove(Sample_Name, '-4[:alpha:]'),
Sample_Name = str_remove(Sample_Name, '-5[:alpha:]'),
Sample_Name = str_remove(Sample_Name, '-6[:alpha:]'),
Sample_Name = str_remove(Sample_Name, '-1'),
Sample_Name = str_remove(Sample_Name, '-2'),
Sample_Name = str_remove(Sample_Name, '-3'),
Sample_Name = str_remove(Sample_Name, '_DIC'),
Sample_Name = str_remove(Sample_Name, '_ION'),
Sample_Name = str_remove(Sample_Name, '_OCN'),
Sample_Name = str_remove(Sample_Name, '_TN'),
Sample_Name = str_remove(Sample_Name, '_ICR'),
Sample_Name = str_remove(Sample_Name, '_TSS'),
Sample_Name = str_remove(Sample_Name, '_SFE'),
Sample_Name = str_remove(Sample_Name, '_INC'))%>%
add_column(Mean_Missing_Reps = NA) %>%
distinct()
combine_headers <- read_csv(analyte_files[1], n_max = 11, skip = 2) %>%
select("Field_Name", "Sample_Name", "Material")
# ======================================= DIC ==================================
NPOC_TN_file <- analyte_files[grepl("NPOC_TN", analyte_files)]
length(NPOC_TN_file) > 0
NPOC_file <- analyte_files[grepl("NPOC_Boye", analyte_files)]
combine <- combine %>%
full_join(NPOC_TN_summary, by = c("Field_Name", "Sample_Name", "Material")) %>%
arrange(Sample_Name) %>%
unite(Mean_Missing_Reps, Mean_Missing_Reps.x, Mean_Missing_Reps.y, remove = T, na.rm = T)%>%
mutate(Mean_Missing_Reps = ifelse(str_detect(Mean_Missing_Reps, 'TRUE'), TRUE, FALSE))%>%
filter(!is.na(Sample_Name))
length(NPOC_file) > 0
NPOC_boye_headers <- read_csv(NPOC_file, n_max = 11, skip = 2)%>%
select(-'Methods_Deviation')
NPOC_NPOC_qaqc <- qaqc_files[grepl("NPOC", qaqc_files)] %>%
read_csv() %>%
clean_names(replace = c('outlier' = 'Outlier'), case = 'none') %>%
filter(NPOC_Outlier == T)
qaqc_files[grepl("NPOC", qaqc_files)]
View(NPOC_NPOC_qaqc)
NPOC_data <- read_csv(NPOC_file, skip = 2, na = '-9999') %>%
filter(!Sample_Name %in% c('N/A', '-9999', NA),
Field_Name != '#End_Data') %>%
mutate(Field_Name = 'N/A',
`00681_NPOC_mg_per_L_as_C` = ifelse(Sample_Name %in% NPOC_NPOC_qaqc$Sample_ID, NA, as.numeric(`00681_NPOC_mg_per_L_as_C`)),
Sample_Name = str_remove(Sample_Name, '-1'),
Sample_Name = str_remove(Sample_Name, '-2'),
Sample_Name = str_remove(Sample_Name, '-3'),
Sample_Name = str_remove(Sample_Name, '_OCN'),
Sample_Name = str_remove(Sample_Name, '_ICR')
)
View(NPOC_data)
source("C:/Users/forb086/OneDrive - PNNL/Data Generation and Files/R code/Data_Package_Code/Boye_Code/04_Boye_Summary_File.R", echo=TRUE)
dir <- 'Z:/00_Cross-SFA_ESSDIVE-Data-Package-Upload/01_Study-Data-Package-Folders/WHONDRS_WROL2019_Data_Package_v3/v3_WHONDRS_WROL2019_Data_Package/FTICR/FTICR_Data'
list <- list.files(dir, '.xml', recursive = T)
tibble <- tibble(File = list)%>%
mutate(File = str_remove(File, '-.+'))%>%
distinct()
library(tidyverse)
# ================================= User inputs ================================
dir <- 'Z:/00_Cross-SFA_ESSDIVE-Data-Package-Upload/01_Study-Data-Package-Folders/WHONDRS_WROL2019_Data_Package_v3/v3_WHONDRS_WROL2019_Data_Package/FTICR/FTICR_Data'
# ================================ make tibble =================================
list <- list.files(dir, '.xml', recursive = T)
tibble <- tibble(File = list)%>%
mutate(File = str_remove(File, '-.+'))%>%
distinct()
View(tibble)
tibble <- tibble(File = list)%>%
# mutate(File = str_remove(File, '-.+'))%>%
distinct()
write_csv(tibble, 'C:/Users/forb086/OneDrive - PNNL/Desktop/WROL_TEMP.csv')
library(tidyverse) #keep it tidy
library(raster) # work with rasters, NOTE: masks dplyr::select
library(janitor) # clean_names()
library(ggthemes) # theme_map()
library(ggsflabel) # add labels to sf objects
library(ggnewscale) # set multiple color scales
library(ggspatial) # add north arrow and scale bar
library(nhdplusTools) # get watershed boundary/flowlines
library(elevatr) # pull elevation maps
library(sf) # tidy spatial
library(spData)
library(cowplot)
library(rstudioapi)
library(viridis)
rm(list=ls(all=T))
# Setting wd to parent folder
current_path <- rstudioapi::getActiveDocumentContext()$path
setwd(dirname(current_path))
setwd("./..")
# ================================= User inputs ================================
metadata_file <- './v2_RCSFA_Geospatial_Data_Package/v2_RCSFA_Geospatial_Site_Information.csv'
data_file <- './SSS_Ecosystem_Respiration_Data_Package/SSS_Water_Sediment_Total_Respiration.csv'
shp_dir <- './Maps/YakimaRiverBasin_Boundary'
ER <- './SSS_Ecosystem_Respiration_Data_Package/SSS_Water_Sediment_Total_Respiration.csv'
modelled_ER <- './SSS_Ecosystem_Respiration_Data_Package/SSS_ER_d50_TotalOxygenConsumed.csv'
common_crs = 4326
# ============================== read in and merge =============================
metadata <- read_csv(metadata_file) %>%
dplyr::select(Site_ID, Latitude, Longitude)
data <- read_csv(data_file, skip = 8)
merge <- data %>%
left_join(metadata)
# ============================ read in YRB shp file ============================
YRB_shp <- list.files(shp_dir, 'shp', full.names = T)
YRB_boundary <- read_sf(YRB_shp) %>%
st_transform(common_crs)
# ============================ convert to sf object ============================
sites <- st_as_sf(merge, coords = c('Longitude','Latitude'), crs = common_crs)
# ======================== pull NHD data and elevation =========================
YRB_flowlines <- get_nhdplus(AOI = YRB_boundary$geometry, streamorder = 3)
elevation_raw <- get_elev_raster(YRB_boundary$geometry, z = 10)
elevation_crop <- mask(elevation_raw, YRB_boundary)
elevation <- as.data.frame(elevation_crop, xy = T) %>%
as_tibble() %>%
rename("long" = x,
"lat" = y,
"elevation" = 3) %>% #column index > name (changing resolution changes colname)
filter(!is.na(elevation))
# ============================= create map of sites ===========================
data("us_states", package = "spData")
us_states_4326 = st_transform(us_states, crs = 4326)
wa <- us_states_4326 %>% filter(NAME == "Washington")
insert <- ggplot() +
geom_sf(data = us_states_4326, fill = "white") +
geom_sf(data = wa, fill = "black",colour = "black")+
geom_sf(data = YRB_boundary, colour = "red", fill = 'red') +
labs(x = "", y = "")+
theme_map()
merge_ER <- read_csv(ER, skip = 8, na = '-9999') %>%
full_join(read_csv(modelled_ER, skip = 4, na = '-9999')) %>%
left_join(metadata)
ER_sf <- merge_ER %>%
filter(Total_Ecosystem_Respiration_Square <= 0) %>%
st_as_sf(coords = c('Longitude','Latitude'), crs = common_crs)
ER_sf <- merge_ER %>%
# filter(Total_Ecosystem_Respiration_Square <= 0) %>%
st_as_sf(coords = c('Longitude','Latitude'), crs = common_crs)
ER_tot_obs_map <- ggplot()+
geom_sf(data = YRB_boundary)+
geom_raster(data = elevation, aes(long, lat, fill = elevation), show.legend = F, alpha = 0.4)+
scale_fill_gradient(low = 'white', high = 'black')+
geom_sf(data = YRB_flowlines, color = "royalblue", alpha = 0.8)+
new_scale_fill()+
geom_sf(data = ER_sf, aes(color = Total_Ecosystem_Respiration_Square, size = Total_Ecosystem_Respiration_Square), show.legend = T) +
scale_fill_viridis(option = 'B', begin = 0.3)+
scale_color_viridis(option = 'B', begin = 0.3)+
scale_size(range = c(3, 8), trans = 'reverse')+
theme_map() +
labs(x = "", y = "", color = "Total Ecosystem\nRespiration\n(g O2 m2 day-1)") +
ggspatial::annotation_scale(
location = "br",
pad_x = unit(0.5, "in"),
bar_cols = c("black", "white")) +
ggspatial::annotation_north_arrow(
location = "tr", which_north = "true",
pad_x = unit(2, "in"),
# pad_y = unit(0.5, "in"),
style = ggspatial::north_arrow_nautical(
fill = c("black", "white"),
line_col = "grey20"))
ggsave('./Maps/SSS_ER_Total_Observed_Map.pdf',
ER_tot_obs_map,
width = 10,
height = 5
)
colnames(merge_ER)
merge_ER <- read_csv(ER, skip = 8, na = '-9999') %>%
full_join(read_csv(modelled_ER, skip = 4, na = '-9999')) %>%
left_join(metadata) %>%
arrange(Total_Ecosystem_Respiration_Square)
ER_sf <- merge_ER %>%
# filter(Total_Ecosystem_Respiration_Square <= 0) %>%
st_as_sf(coords = c('Longitude','Latitude'), crs = common_crs)
ER_tot_obs_map <- ggplot()+
geom_sf(data = YRB_boundary)+
geom_raster(data = elevation, aes(long, lat, fill = elevation), show.legend = F, alpha = 0.4)+
scale_fill_gradient(low = 'white', high = 'black')+
geom_sf(data = YRB_flowlines, color = "royalblue", alpha = 0.8)+
new_scale_fill()+
geom_sf(data = ER_sf, aes(color = Total_Ecosystem_Respiration_Square, size = Total_Ecosystem_Respiration_Square), show.legend = T) +
scale_fill_viridis(option = 'B', begin = 0.3)+
scale_color_viridis(option = 'B', begin = 0.3)+
scale_size(range = c(3, 8), trans = 'reverse')+
theme_map() +
labs(x = "", y = "", color = "Total Ecosystem\nRespiration\n(g O2 m2 day-1)") +
ggspatial::annotation_scale(
location = "br",
pad_x = unit(0.5, "in"),
bar_cols = c("black", "white")) +
ggspatial::annotation_north_arrow(
location = "tr", which_north = "true",
pad_x = unit(2, "in"),
# pad_y = unit(0.5, "in"),
style = ggspatial::north_arrow_nautical(
fill = c("black", "white"),
line_col = "grey20"))
ggsave('./Maps/SSS_ER_Total_Observed_Map.pdf',
ER_tot_obs_map,
width = 10,
height = 5
)
merge_ER <- read_csv(ER, skip = 8, na = '-9999') %>%
full_join(read_csv(modelled_ER, skip = 4, na = '-9999')) %>%
left_join(metadata) %>%
arrange(Total_Oxygen_Consumed_g_per_m2_per_day)
ER_sf <- merge_ER %>%
# filter(Total_Ecosystem_Respiration_Square <= 0) %>%
st_as_sf(coords = c('Longitude','Latitude'), crs = common_crs)
ER_tot_pred_map <- ggplot()+
geom_sf(data = YRB_boundary)+
geom_raster(data = elevation, aes(long, lat, fill = elevation), show.legend = F, alpha = 0.4)+
scale_fill_gradient(low = 'white', high = 'black')+
geom_sf(data = YRB_flowlines, color = "royalblue", alpha = 0.8)+
new_scale_fill()+
geom_sf(data = ER_sf, aes(color = Total_Oxygen_Consumed_g_per_m2_per_day, size = Total_Oxygen_Consumed_g_per_m2_per_day), show.legend = T) +
scale_fill_viridis(option = 'B', begin = 0.3)+
scale_color_viridis(option = 'B', begin = 0.3)+
scale_size(range = c(3, 8), trans = 'reverse')+
theme_map() +
labs(x = "", y = "", color = "Total Oxygen\nConsumed\n(g O2 m2 day-1)") +
ggspatial::annotation_scale(
location = "br",
pad_x = unit(0.5, "in"),
bar_cols = c("black", "white")) +
ggspatial::annotation_north_arrow(
location = "tr", which_north = "true",
pad_x = unit(2, "in"),
# pad_y = unit(0.5, "in"),
style = ggspatial::north_arrow_nautical(
fill = c("black", "white"),
line_col = "grey20"))
ggsave('./Maps/SSS_ER_Total_Predicted_Map.pdf',
ER_tot_pred_map,
width = 10,
height = 5
)
