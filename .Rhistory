transect_depth_datetime =  site_avg_summary$datetime,
hobo_reference_datetime = all_data_depth %>%
filter(DateTime >= site_avg_summary$datetime) %>%
head(1) %>%
pull(DateTime))
} else if(parent_ID == 'SSS010'|parent_ID == 'SSS023'|parent_ID == 'SSS036'){
# No data at time of transect depth, finding time closest
# these are the ones with the closest time BEFORE the transect was taken
hobo_data_reference_depth_m <- all_data_depth %>%
filter(DateTime <= site_avg_summary$datetime,
!is.na(depth_from_pressure_m)) %>%
tail(1) %>%
pull(depth_from_pressure_m)
hobo_time_difference_combine <- hobo_time_difference_combine %>%
add_row(Parent_ID = parent_ID,
transect_depth_datetime =  site_avg_summary$datetime,
hobo_reference_datetime = all_data_depth %>%
filter(DateTime <= site_avg_summary$datetime) %>%
tail(1) %>%
pull(DateTime))
}else{
hobo_data_reference_depth_m <- all_data_depth %>%
filter(DateTime <= site_avg_summary$datetime + 450 & DateTime >= site_avg_summary$datetime - 450,
!is.na(depth_from_pressure_m)) %>%
pull(depth_from_pressure_m)
}
if(parent_ID == 'SSS013'){
# SSS013 (site T07) is missing hobo data for the beginning of the time series
# resulting in the avg depth and the first hobo measurement being ~14 days apart.
# There is a very close USGS gage so we are using USGS discharge and a rating
# curve calculated from USGS historic data to estimate the depth
discharge <- './Stream_Metabolizer/Inputs/USGS_Kiona_Discharge.csv' %>%
read_csv(comment = '#') %>%
filter(agency_cd != '5s') %>%
mutate(date = mdy(datetime),
time = paste0(tz_cd, ":00"),
DateTime = as_datetime(paste(date, time)) - hours(1)) %>%
rename(Discharge = '151886_00060_cd') %>%
select(DateTime, Discharge)
depth <- discharge %>%
mutate(depth_ft = 0.0214 * (Discharge^0.6238),
time_series_average_depth_cm = depth_ft*30.48) %>%
select(DateTime, time_series_average_depth_cm)
all_data_depth <- all_data_depth %>%
left_join(depth)
rm(discharge)
rm(depth)
} else {
all_data_depth <- all_data_depth %>%
mutate(offset_cm = (hobo_data_reference_depth_m * 100) - site_avg_depth_cm,
time_series_average_depth_cm = (depth_from_pressure_m * 100)  - offset_cm) %>%
filter(!is.na(Dissolved_Oxygen))
}
if(parent_ID == 'SSS024'){ # missing hobo data, filling in depth with first value
all_data_depth <- all_data_depth  %>%
arrange(DateTime)%>%
mutate(time_series_average_depth_cm = replace_na(time_series_average_depth_cm, first(time_series_average_depth_cm[!is.na(time_series_average_depth_cm)])))
}
### ============================ output cleaned input file ===================
input_file <- all_data_depth %>%
rename(Depth = time_series_average_depth_cm,
Pressure = BaroTROLL_Barometric_Pressure_mBar) %>%
add_column(Latitude = metadata %>% filter(Parent_ID == parent_ID) %>% pull(Latitude),
Longitude = metadata %>% filter(Parent_ID == parent_ID) %>% pull(Longitude)) %>%
mutate(Depth = round(Depth/100, 2)) %>%
select(DateTime, Parent_ID, Site_ID, Latitude, Longitude, Temperature, Dissolved_Oxygen, Pressure, Depth) %>%
arrange(DateTime)
input_file_name <- str_c('./Stream_Metabolizer/Inputs/Sensor_Files/v2_', parent_ID, '_Temp_DO_Press_Depth.csv')
# after the data are outputted, header rows with metadata are added
write_csv(input_file, input_file_name, na = '-9999')
### ============================ plot all input data =============================
input_file_plot1 <- ggplot(input_file, aes(x = DateTime, y = Dissolved_Oxygen)) +
geom_point() +
labs(title = str_c("Parent ID: ", parent_ID, "                 Stream Metabolizer Input Data"),
x = "DateTime", y = "Dissolved Oxygen (mg/L)", color = NULL) +
scale_x_datetime(labels = date_format("%Y-%m-%d %H:%M"))
input_file_plot2 <- ggplot(input_file, aes(x = DateTime, y = Temperature)) +
geom_point() +
labs(title = str_c("Parent ID: ", parent_ID, "                 Stream Metabolizer Input Data"),
x = "DateTime", y = "Temperature (deg C)", color = NULL) +
scale_x_datetime(labels = date_format("%Y-%m-%d %H:%M"))
input_file_plot3 <- ggplot(input_file, aes(x = DateTime, y = Pressure)) +
geom_point() +
labs(title = str_c("Parent ID: ", parent_ID, "                 Stream Metabolizer Input Data"),
x = "DateTime", y = "Barometric Pressure (mBar)", color = NULL) +
scale_x_datetime(labels = date_format("%Y-%m-%d %H:%M"))
input_file_plot4 <- ggplot(input_file, aes(x = DateTime, y = Depth)) +
geom_point() +
labs(title = str_c("Parent ID: ", parent_ID, "                 Stream Metabolizer Input Data"),
x = "DateTime", y = "Depth (m)", color = NULL) +
scale_x_datetime(labels = date_format("%Y-%m-%d %H:%M"))
plotly <- subplot(input_file_plot1,
input_file_plot2,
input_file_plot3,
input_file_plot4,
nrows = 4,
shareY = TRUE,
shareX = TRUE)
#change wd to plot folder so it only outputs html and not additional files
setwd('./Stream_Metabolizer/Inputs/Sensor_Files/Plots/')
plotly_outname <-  str_c(parent_ID, '_Temp_DO_Press_Depth_Plot.html')
htmlwidgets::saveWidget(as_widget(plotly), plotly_outname, selfcontained = T)
#set back to original dir
setwd(dirname(current_path))
DO_files[10:20]
file <- DO_files[20]
file
data <- read_csv(file, comment = '#', na = c('', '-9999',-9999, NA, 'N/A')) %>%
select(-Dissolved_Oxygen_Saturation, -Battery)
parent_ID <- str_extract(file, "[A-Z]{3}\\d{3}")
# subset to 15 mins
subset <- data %>%
filter(minute(DateTime) %% 15 == 0)
## ============================ tsrobprep cleaning =============================
set.seed(7)
auto_clean <- auto_data_cleaning(
data = subset$Dissolved_Oxygen, # Dissolved Oxygen data
S = 96, # 96 intervals for daily seasonality (96 rows of 15 min data = 1 day of data)
tau = NULL, # Performs lasso to determine tau
no.of.last.indices.to.fix = nrow(subset), # Fix all data points
indices.to.fix = NULL, # Automatically fix indices
detect.outliers.pars = list(method = c("IQR"),  # Use IQR for outlier detection
threshold = c(1.5)  # Common threshold for IQR
))
clean_DO <- subset %>%
add_column(Cleaned_DO = auto_clean$clean.data[,1, drop = T])
## ============================ fix sample day ==============================
# use metadata to revert last two and first two points before and after sample day back to original data
site_metadata <- metadata %>%
filter(Parent_ID == parent_ID)
sample_day <- site_metadata$Sample_Date
datetime_to_fix <- c(as_datetime(paste0(sample_day-(1), '23:30:00')), # second to last point before sample day
as_datetime(paste0(sample_day-(1), '23:45:00')), # last point before sample day
as_datetime(paste0(sample_day+(1), '00:00:00')), # first point after sample day
as_datetime(paste0(sample_day+(1), '00:15:00'))) # second point after sample day
clean_DO <- clean_DO %>%
mutate(Dissolved_Oxygen_final = case_when(DateTime %in% datetime_to_fix ~ Dissolved_Oxygen,
TRUE ~ Cleaned_DO))
## ============================== check data ================================
# assuming the cleaning algorithm did not work when cleaned values are more than 20 mg/L
# in this case, use original data
# 17 mg/L is the reported maximum in the DO summary file, rounding up to 20 for the threshold
if(max(clean_DO$Cleaned_DO) > 20){
clean_DO <- clean_DO %>%
select(DateTime, Parent_ID, Site_ID, Temperature, Dissolved_Oxygen)
cat(red$bold(str_c(parent_ID, ' has data >20 mg/L. Removing cleaned DO and using original DO.')), "\n")
} else {
clean_DO <- clean_DO %>%
select(DateTime, Parent_ID, Site_ID, Temperature, Dissolved_Oxygen_final) %>%
rename(Dissolved_Oxygen = Dissolved_Oxygen_final)
}
## ============================ remove biofouling =============================
# biofouling identified with visual inspection
# SSS005 - remove everything after 8/15
# SSS016 - remove everything after 8/23
# SSS028 - remove 8/22-8/23 and 8/26-8/27
# SSS046 - remove everything after 8/26
if(parent_ID == 'SSS005'){
clean_DO <- clean_DO %>%
filter(DateTime<as_datetime('2022-08-16 00:00:00'))
}else if(parent_ID == 'SSS016'){
clean_DO <- clean_DO %>%
filter(DateTime<as_datetime('2022-08-24 00:00:00'))
}else if(parent_ID == 'SSS028'){
clean_DO <- clean_DO %>%
filter(date(DateTime) != '2022-08-22') %>%
filter(date(DateTime) != '2022-08-23') %>%
filter(date(DateTime) != '2022-08-26') %>%
filter(date(DateTime) != '2022-08-27')
}else if(parent_ID == 'SSS046'){
clean_DO <- clean_DO %>%
filter(DateTime<as_datetime('2022-08-27 00:00:00'))
}
## ============================ create input file =============================
### ============================ merge DO with baro and hobo ===================
baro_data <- baro_files[grepl(parent_ID, baro_files)] %>% # find baro file for same site
read_csv(comment = '#', na = c('', '-9999',-9999, NA, 'N/A')) %>%
select(-any_of("Air_Temperature")) %>%
rename(BaroTROLL_Barometric_Pressure_mBar = Pressure)
hobo_data <- hobo_files[grepl(parent_ID, hobo_files)] %>% # find hobo file for same site
read_csv(comment = '#', na = c('', '-9999',-9999, NA, 'N/A')) %>%
rename(HOBO_Temperature_degC = Temperature,
HOBO_Absolute_Pressure_mbar = Absolute_Pressure)
all_data <- hobo_data %>%
full_join(baro_data, by = c('DateTime', 'Parent_ID', 'Site_ID')) %>%
full_join(clean_DO, by = c('DateTime', 'Parent_ID', 'Site_ID'))
### ============================ calculate depth ===================
site_avg_summary <- depth_summary %>%
filter(Parent_ID == parent_ID)
site_avg_depth_cm <- site_avg_summary %>%
pull(Average_Depth)
all_data_depth <- all_data %>%
mutate(compensated_hobo_water_pressure_mbar = HOBO_Absolute_Pressure_mbar - BaroTROLL_Barometric_Pressure_mBar,
density_kg_per_m3 = (999.84847 + (0.06337563 * HOBO_Temperature_degC) - (0.008523829 * HOBO_Temperature_degC^2) + (0.0000694324 * HOBO_Temperature_degC^3) - (0.0000003821216 * HOBO_Temperature_degC^4)),
depth_from_pressure_m = (compensated_hobo_water_pressure_mbar*100)/(9.80  * density_kg_per_m3))
if(parent_ID == 'SSS003'|parent_ID == 'SSS005'|parent_ID == 'SSS014'|parent_ID == 'SSS015'|parent_ID == 'SSS016'|parent_ID == 'SSS017'|parent_ID == 'SSS024'|parent_ID == 'SSS011'){
# No data at time of transect depth, finding time closest,
# these are the ones with the closest time AFTER the transect was taken
hobo_data_reference_depth_m <- all_data_depth %>%
filter(DateTime >= site_avg_summary$datetime) %>%
head(1) %>%
pull(depth_from_pressure_m)
hobo_time_difference_combine <- hobo_time_difference_combine %>%
add_row(Parent_ID = parent_ID,
transect_depth_datetime =  site_avg_summary$datetime,
hobo_reference_datetime = all_data_depth %>%
filter(DateTime >= site_avg_summary$datetime) %>%
head(1) %>%
pull(DateTime))
} else if(parent_ID == 'SSS010'|parent_ID == 'SSS023'|parent_ID == 'SSS036'){
# No data at time of transect depth, finding time closest
# these are the ones with the closest time BEFORE the transect was taken
hobo_data_reference_depth_m <- all_data_depth %>%
filter(DateTime <= site_avg_summary$datetime,
!is.na(depth_from_pressure_m)) %>%
tail(1) %>%
pull(depth_from_pressure_m)
hobo_time_difference_combine <- hobo_time_difference_combine %>%
add_row(Parent_ID = parent_ID,
transect_depth_datetime =  site_avg_summary$datetime,
hobo_reference_datetime = all_data_depth %>%
filter(DateTime <= site_avg_summary$datetime) %>%
tail(1) %>%
pull(DateTime))
}else{
hobo_data_reference_depth_m <- all_data_depth %>%
filter(DateTime <= site_avg_summary$datetime + 450 & DateTime >= site_avg_summary$datetime - 450,
!is.na(depth_from_pressure_m)) %>%
pull(depth_from_pressure_m)
}
if(parent_ID == 'SSS013'){
# SSS013 (site T07) is missing hobo data for the beginning of the time series
# resulting in the avg depth and the first hobo measurement being ~14 days apart.
# There is a very close USGS gage so we are using USGS discharge and a rating
# curve calculated from USGS historic data to estimate the depth
discharge <- './Stream_Metabolizer/Inputs/USGS_Kiona_Discharge.csv' %>%
read_csv(comment = '#') %>%
filter(agency_cd != '5s') %>%
mutate(date = mdy(datetime),
time = paste0(tz_cd, ":00"),
DateTime = as_datetime(paste(date, time)) - hours(1)) %>%
rename(Discharge = '151886_00060_cd') %>%
select(DateTime, Discharge)
depth <- discharge %>%
mutate(depth_ft = 0.0214 * (Discharge^0.6238),
time_series_average_depth_cm = depth_ft*30.48) %>%
select(DateTime, time_series_average_depth_cm)
all_data_depth <- all_data_depth %>%
left_join(depth)
rm(discharge)
rm(depth)
} else {
all_data_depth <- all_data_depth %>%
mutate(offset_cm = (hobo_data_reference_depth_m * 100) - site_avg_depth_cm,
time_series_average_depth_cm = (depth_from_pressure_m * 100)  - offset_cm) %>%
filter(!is.na(Dissolved_Oxygen))
}
if(parent_ID == 'SSS024'){ # missing hobo data, filling in depth with first value
all_data_depth <- all_data_depth  %>%
arrange(DateTime)%>%
mutate(time_series_average_depth_cm = replace_na(time_series_average_depth_cm, first(time_series_average_depth_cm[!is.na(time_series_average_depth_cm)])))
}
### ============================ output cleaned input file ===================
input_file <- all_data_depth %>%
rename(Depth = time_series_average_depth_cm,
Pressure = BaroTROLL_Barometric_Pressure_mBar) %>%
add_column(Latitude = metadata %>% filter(Parent_ID == parent_ID) %>% pull(Latitude),
Longitude = metadata %>% filter(Parent_ID == parent_ID) %>% pull(Longitude)) %>%
mutate(Depth = round(Depth/100, 2)) %>%
select(DateTime, Parent_ID, Site_ID, Latitude, Longitude, Temperature, Dissolved_Oxygen, Pressure, Depth) %>%
arrange(DateTime)
input_file_name <- str_c('./Stream_Metabolizer/Inputs/Sensor_Files/v2_', parent_ID, '_Temp_DO_Press_Depth.csv')
# after the data are outputted, header rows with metadata are added
write_csv(input_file, input_file_name, na = '-9999')
### ============================ plot all input data =============================
input_file_plot1 <- ggplot(input_file, aes(x = DateTime, y = Dissolved_Oxygen)) +
geom_point() +
labs(title = str_c("Parent ID: ", parent_ID, "                 Stream Metabolizer Input Data"),
x = "DateTime", y = "Dissolved Oxygen (mg/L)", color = NULL) +
scale_x_datetime(labels = date_format("%Y-%m-%d %H:%M"))
input_file_plot2 <- ggplot(input_file, aes(x = DateTime, y = Temperature)) +
geom_point() +
labs(title = str_c("Parent ID: ", parent_ID, "                 Stream Metabolizer Input Data"),
x = "DateTime", y = "Temperature (deg C)", color = NULL) +
scale_x_datetime(labels = date_format("%Y-%m-%d %H:%M"))
input_file_plot3 <- ggplot(input_file, aes(x = DateTime, y = Pressure)) +
geom_point() +
labs(title = str_c("Parent ID: ", parent_ID, "                 Stream Metabolizer Input Data"),
x = "DateTime", y = "Barometric Pressure (mBar)", color = NULL) +
scale_x_datetime(labels = date_format("%Y-%m-%d %H:%M"))
input_file_plot4 <- ggplot(input_file, aes(x = DateTime, y = Depth)) +
geom_point() +
labs(title = str_c("Parent ID: ", parent_ID, "                 Stream Metabolizer Input Data"),
x = "DateTime", y = "Depth (m)", color = NULL) +
scale_x_datetime(labels = date_format("%Y-%m-%d %H:%M"))
plotly <- subplot(input_file_plot1,
input_file_plot2,
input_file_plot3,
input_file_plot4,
nrows = 4,
shareY = TRUE,
shareX = TRUE)
#change wd to plot folder so it only outputs html and not additional files
setwd('./Stream_Metabolizer/Inputs/Sensor_Files/Plots/')
plotly_outname <-  str_c(parent_ID, '_Temp_DO_Press_Depth_Plot.html')
htmlwidgets::saveWidget(as_widget(plotly), plotly_outname, selfcontained = T)
#set back to original dir
setwd(dirname(current_path))
DO_files
file <- DO_files[33]
file
data <- read_csv(file, comment = '#', na = c('', '-9999',-9999, NA, 'N/A')) %>%
select(-Dissolved_Oxygen_Saturation, -Battery)
parent_ID <- str_extract(file, "[A-Z]{3}\\d{3}")
# subset to 15 mins
subset <- data %>%
filter(minute(DateTime) %% 15 == 0)
## ============================ tsrobprep cleaning =============================
set.seed(7)
auto_clean <- auto_data_cleaning(
data = subset$Dissolved_Oxygen, # Dissolved Oxygen data
S = 96, # 96 intervals for daily seasonality (96 rows of 15 min data = 1 day of data)
tau = NULL, # Performs lasso to determine tau
no.of.last.indices.to.fix = nrow(subset), # Fix all data points
indices.to.fix = NULL, # Automatically fix indices
detect.outliers.pars = list(method = c("IQR"),  # Use IQR for outlier detection
threshold = c(1.5)  # Common threshold for IQR
))
clean_DO <- subset %>%
add_column(Cleaned_DO = auto_clean$clean.data[,1, drop = T])
## ============================ fix sample day ==============================
# use metadata to revert last two and first two points before and after sample day back to original data
site_metadata <- metadata %>%
filter(Parent_ID == parent_ID)
sample_day <- site_metadata$Sample_Date
datetime_to_fix <- c(as_datetime(paste0(sample_day-(1), '23:30:00')), # second to last point before sample day
as_datetime(paste0(sample_day-(1), '23:45:00')), # last point before sample day
as_datetime(paste0(sample_day+(1), '00:00:00')), # first point after sample day
as_datetime(paste0(sample_day+(1), '00:15:00'))) # second point after sample day
clean_DO <- clean_DO %>%
mutate(Dissolved_Oxygen_final = case_when(DateTime %in% datetime_to_fix ~ Dissolved_Oxygen,
TRUE ~ Cleaned_DO))
## ============================== check data ================================
# assuming the cleaning algorithm did not work when cleaned values are more than 20 mg/L
# in this case, use original data
# 17 mg/L is the reported maximum in the DO summary file, rounding up to 20 for the threshold
if(max(clean_DO$Cleaned_DO) > 20){
clean_DO <- clean_DO %>%
select(DateTime, Parent_ID, Site_ID, Temperature, Dissolved_Oxygen)
cat(red$bold(str_c(parent_ID, ' has data >20 mg/L. Removing cleaned DO and using original DO.')), "\n")
} else {
clean_DO <- clean_DO %>%
select(DateTime, Parent_ID, Site_ID, Temperature, Dissolved_Oxygen_final) %>%
rename(Dissolved_Oxygen = Dissolved_Oxygen_final)
}
## ============================ remove biofouling =============================
# biofouling identified with visual inspection
# SSS005 - remove everything after 8/15
# SSS016 - remove everything after 8/23
# SSS028 - remove 8/22-8/23 and 8/26-8/27
# SSS046 - remove everything after 8/26
if(parent_ID == 'SSS005'){
clean_DO <- clean_DO %>%
filter(DateTime<as_datetime('2022-08-16 00:00:00'))
}else if(parent_ID == 'SSS016'){
clean_DO <- clean_DO %>%
filter(DateTime<as_datetime('2022-08-24 00:00:00'))
}else if(parent_ID == 'SSS028'){
clean_DO <- clean_DO %>%
filter(date(DateTime) != '2022-08-22') %>%
filter(date(DateTime) != '2022-08-23') %>%
filter(date(DateTime) != '2022-08-26') %>%
filter(date(DateTime) != '2022-08-27')
}else if(parent_ID == 'SSS046'){
clean_DO <- clean_DO %>%
filter(DateTime<as_datetime('2022-08-27 00:00:00'))
}
## ============================ create input file =============================
### ============================ merge DO with baro and hobo ===================
baro_data <- baro_files[grepl(parent_ID, baro_files)] %>% # find baro file for same site
read_csv(comment = '#', na = c('', '-9999',-9999, NA, 'N/A')) %>%
select(-any_of("Air_Temperature")) %>%
rename(BaroTROLL_Barometric_Pressure_mBar = Pressure)
hobo_data <- hobo_files[grepl(parent_ID, hobo_files)] %>% # find hobo file for same site
read_csv(comment = '#', na = c('', '-9999',-9999, NA, 'N/A')) %>%
rename(HOBO_Temperature_degC = Temperature,
HOBO_Absolute_Pressure_mbar = Absolute_Pressure)
all_data <- hobo_data %>%
full_join(baro_data, by = c('DateTime', 'Parent_ID', 'Site_ID')) %>%
full_join(clean_DO, by = c('DateTime', 'Parent_ID', 'Site_ID'))
### ============================ calculate depth ===================
site_avg_summary <- depth_summary %>%
filter(Parent_ID == parent_ID)
site_avg_depth_cm <- site_avg_summary %>%
pull(Average_Depth)
all_data_depth <- all_data %>%
mutate(compensated_hobo_water_pressure_mbar = HOBO_Absolute_Pressure_mbar - BaroTROLL_Barometric_Pressure_mBar,
density_kg_per_m3 = (999.84847 + (0.06337563 * HOBO_Temperature_degC) - (0.008523829 * HOBO_Temperature_degC^2) + (0.0000694324 * HOBO_Temperature_degC^3) - (0.0000003821216 * HOBO_Temperature_degC^4)),
depth_from_pressure_m = (compensated_hobo_water_pressure_mbar*100)/(9.80  * density_kg_per_m3))
if(parent_ID == 'SSS003'|parent_ID == 'SSS005'|parent_ID == 'SSS014'|parent_ID == 'SSS015'|parent_ID == 'SSS016'|parent_ID == 'SSS017'|parent_ID == 'SSS024'|parent_ID == 'SSS011'){
# No data at time of transect depth, finding time closest,
# these are the ones with the closest time AFTER the transect was taken
hobo_data_reference_depth_m <- all_data_depth %>%
filter(DateTime >= site_avg_summary$datetime) %>%
head(1) %>%
pull(depth_from_pressure_m)
hobo_time_difference_combine <- hobo_time_difference_combine %>%
add_row(Parent_ID = parent_ID,
transect_depth_datetime =  site_avg_summary$datetime,
hobo_reference_datetime = all_data_depth %>%
filter(DateTime >= site_avg_summary$datetime) %>%
head(1) %>%
pull(DateTime))
} else if(parent_ID == 'SSS010'|parent_ID == 'SSS023'|parent_ID == 'SSS036'){
# No data at time of transect depth, finding time closest
# these are the ones with the closest time BEFORE the transect was taken
hobo_data_reference_depth_m <- all_data_depth %>%
filter(DateTime <= site_avg_summary$datetime,
!is.na(depth_from_pressure_m)) %>%
tail(1) %>%
pull(depth_from_pressure_m)
hobo_time_difference_combine <- hobo_time_difference_combine %>%
add_row(Parent_ID = parent_ID,
transect_depth_datetime =  site_avg_summary$datetime,
hobo_reference_datetime = all_data_depth %>%
filter(DateTime <= site_avg_summary$datetime) %>%
tail(1) %>%
pull(DateTime))
}else{
hobo_data_reference_depth_m <- all_data_depth %>%
filter(DateTime <= site_avg_summary$datetime + 450 & DateTime >= site_avg_summary$datetime - 450,
!is.na(depth_from_pressure_m)) %>%
pull(depth_from_pressure_m)
}
if(parent_ID == 'SSS013'){
# SSS013 (site T07) is missing hobo data for the beginning of the time series
# resulting in the avg depth and the first hobo measurement being ~14 days apart.
# There is a very close USGS gage so we are using USGS discharge and a rating
# curve calculated from USGS historic data to estimate the depth
discharge <- './Stream_Metabolizer/Inputs/USGS_Kiona_Discharge.csv' %>%
read_csv(comment = '#') %>%
filter(agency_cd != '5s') %>%
mutate(date = mdy(datetime),
time = paste0(tz_cd, ":00"),
DateTime = as_datetime(paste(date, time)) - hours(1)) %>%
rename(Discharge = '151886_00060_cd') %>%
select(DateTime, Discharge)
depth <- discharge %>%
mutate(depth_ft = 0.0214 * (Discharge^0.6238),
time_series_average_depth_cm = depth_ft*30.48) %>%
select(DateTime, time_series_average_depth_cm)
all_data_depth <- all_data_depth %>%
left_join(depth)
rm(discharge)
rm(depth)
} else {
all_data_depth <- all_data_depth %>%
mutate(offset_cm = (hobo_data_reference_depth_m * 100) - site_avg_depth_cm,
time_series_average_depth_cm = (depth_from_pressure_m * 100)  - offset_cm) %>%
filter(!is.na(Dissolved_Oxygen))
}
if(parent_ID == 'SSS024'){ # missing hobo data, filling in depth with first value
all_data_depth <- all_data_depth  %>%
arrange(DateTime)%>%
mutate(time_series_average_depth_cm = replace_na(time_series_average_depth_cm, first(time_series_average_depth_cm[!is.na(time_series_average_depth_cm)])))
}
### ============================ output cleaned input file ===================
input_file <- all_data_depth %>%
rename(Depth = time_series_average_depth_cm,
Pressure = BaroTROLL_Barometric_Pressure_mBar) %>%
add_column(Latitude = metadata %>% filter(Parent_ID == parent_ID) %>% pull(Latitude),
Longitude = metadata %>% filter(Parent_ID == parent_ID) %>% pull(Longitude)) %>%
mutate(Depth = round(Depth/100, 2)) %>%
select(DateTime, Parent_ID, Site_ID, Latitude, Longitude, Temperature, Dissolved_Oxygen, Pressure, Depth) %>%
arrange(DateTime)
input_file_name <- str_c('./Stream_Metabolizer/Inputs/Sensor_Files/v2_', parent_ID, '_Temp_DO_Press_Depth.csv')
# after the data are outputted, header rows with metadata are added
write_csv(input_file, input_file_name, na = '-9999')
### ============================ plot all input data =============================
input_file_plot1 <- ggplot(input_file, aes(x = DateTime, y = Dissolved_Oxygen)) +
geom_point() +
labs(title = str_c("Parent ID: ", parent_ID, "                 Stream Metabolizer Input Data"),
x = "DateTime", y = "Dissolved Oxygen (mg/L)", color = NULL) +
scale_x_datetime(labels = date_format("%Y-%m-%d %H:%M"))
input_file_plot2 <- ggplot(input_file, aes(x = DateTime, y = Temperature)) +
geom_point() +
labs(title = str_c("Parent ID: ", parent_ID, "                 Stream Metabolizer Input Data"),
x = "DateTime", y = "Temperature (deg C)", color = NULL) +
scale_x_datetime(labels = date_format("%Y-%m-%d %H:%M"))
input_file_plot3 <- ggplot(input_file, aes(x = DateTime, y = Pressure)) +
geom_point() +
labs(title = str_c("Parent ID: ", parent_ID, "                 Stream Metabolizer Input Data"),
x = "DateTime", y = "Barometric Pressure (mBar)", color = NULL) +
scale_x_datetime(labels = date_format("%Y-%m-%d %H:%M"))
input_file_plot4 <- ggplot(input_file, aes(x = DateTime, y = Depth)) +
geom_point() +
labs(title = str_c("Parent ID: ", parent_ID, "                 Stream Metabolizer Input Data"),
x = "DateTime", y = "Depth (m)", color = NULL) +
scale_x_datetime(labels = date_format("%Y-%m-%d %H:%M"))
plotly <- subplot(input_file_plot1,
input_file_plot2,
input_file_plot3,
input_file_plot4,
nrows = 4,
shareY = TRUE,
shareX = TRUE)
#change wd to plot folder so it only outputs html and not additional files
setwd('./Stream_Metabolizer/Inputs/Sensor_Files/Plots/')
plotly_outname <-  str_c(parent_ID, '_Temp_DO_Press_Depth_Plot.html')
htmlwidgets::saveWidget(as_widget(plotly), plotly_outname, selfcontained = T)
#set back to original dir
setwd(dirname(current_path))
