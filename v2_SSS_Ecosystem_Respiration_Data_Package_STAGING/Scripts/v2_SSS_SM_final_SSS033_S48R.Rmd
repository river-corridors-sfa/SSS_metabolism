---
title: "stream metabolizer SSS template"
author: "kaufman"
date: "`r Sys.Date()`"
output: html_document
#output_file: 'NA'
#params:
#  PARENT_ID: 'NA'
#  SITE_ID: 'NA'

---

```{r setup, include=FALSE}
#knitr::opts_chunk$set
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # Sets working Directory to the R project
#knitr::opts_chunk$set(eval = FALSE)
#PARENT_ID=params$PARENT_ID
#SITE_ID=params$SITE_ID
```
##Matt Kaufman, matthew.kaufman@pnnl.gov, Pacific Northwest National Laboratory
##This code is the a single-site file for a special case for stream metabolizer for one site in the 2022 spatial study dataset. It takes in time-series dissolved oxygen, depth, and barometric pressure data, as well as single-point location data and an estimate of k600. From all of that, it estimates GPP, ER, and k600 on a daily basis over the course of the deployment. In general, if there is a lot of correlation between k600 and ER results, it is acceptable to use the deployment-period-average values, but the day-to-day variation should not be trusted.


## 1. Install packages and loading libraries
If this is your first time running this code or streamMetabolizer, make sure to install the appropriate packages below. uncomment them in the chunk before Knitting.

```{r install, echo = TRUE, warning=FALSE, message=FALSE}
#install.packages(remotes); library(remotes)
# remotes::install_github('appling/unitted', force = TRUE)
# remotes::install_github("USGS-R/streamMetabolizer", force = TRUE)
#install.packages("rstan", dependencies = FALSE)
#install.packages(devtools)

# If you have trouble installing rstan, try the installation codes below: 

#devtools::install_github("stan-dev/rstan", ref = "develop", subdir = "rstan/rstan", force = TRUE)
#install.packages("rstan", type = "source")

# Run the line below if you have trouble installing devtools
#devtools::install_github("stan-dev/rstan", ref = "develop", subdir = "rstan/rstan")
```

### Loading libraries
```{r libraries, echo = FALSE, warning=FALSE, message=FALSE}
#rm(list=ls(all=T))

library(streamMetabolizer)
library(dplyr)
library(unitted)
library(ggplot2)
library(tidyr)
library(devtools)
library(rstan)
library(lubridate)
# Correctly installing rstan can be problematic, see GitHub for issues
```

## 2. Setting up the data
Set working directory and read in the data and change units to match the needs of stream metabolizer.
Note: Make sure to always double check that your date time column is in the date time format and not as character

```{r data, echo = TRUE, warning=FALSE, message=FALSE}

#-------------------------------------
#PARENT_ID='' #this is where you would specify a site if you were not running this as a loop
#-------------------------------------

print("PARENT_ID: ")
PARENT_ID<-'SSS033'

print("SITE_ID: ")
SITE_ID<-'S48R'
```

```{r data2, echo = TRUE, warning=FALSE, message=FALSE}
#bp<-bpcalc(29.9, 489/3.28)# elevation at satus (alt in meters) #---------------------------------------------------------------------

data.path = "Inputs/"

dat = read.csv(paste0(data.path,'Sensor_Files/v2_',PARENT_ID,"_Temp_DO_Press_Depth.csv"),header=T,skip=8)
 
 K600estimates=read.csv(paste0(data.path,'v2_SSS_K600.csv'),header=T,skip=3)
 K600estimate<-K600estimates[K600estimates$Site_ID==SITE_ID,2]
 
print("K600 estimate: ")
K600estimate

 output.path="Outputs/"


 #---------------------------------------------------------------------------------------------
  file.name = paste('v2_SSS_SM_',PARENT_ID,'_',SITE_ID,'_final',sep='')
  #dat = na.omit(dat)
  #dat=dat[450:33311-300,] #-------------------------------------------------------------------------------------------------------------
 # Change date time format  

#DOWNSAMPLE
samplingmins=15
dat = dat[seq(1, nrow(dat), samplingmins), ]


#colnames(dat)[10]="Unix.Timestamp" 
#dat$timeUTC<-as_datetime(dat$Unix.Timestamp)

#data is always collected in pacific-standard-time, so conversion to UTC is +8 hours
dat$timeUTC<-as.POSIXct(dat$DateTime)+hours(8)
dat$timeUTC<-force_tz(dat$timeUTC,tzone='UTC')
dat$solar.time<-convert_UTC_to_solartime(dat$timeUTC, longitude= dat$Longitude[1], time.type="mean solar")

#trim out aug 4-10 for biofouling
dat1<-dat[dat$solar.time<"2022-08-04 00:00:00",]
dat2<-dat[dat$solar.time>"2022-08-10 00:00:00",]
dat<-rbind(dat1,dat2)
#----------------------------------


dat$light<- calc_light(dat$solar.time, latitude=dat$Latitude[1], longitude=dat$Longitude[1], max.PAR =2300, attach.units = F) #------------------

# cal_DO_stat requieres barometric pressure in millibars, or a unitted object of barometric pressure.
dat$DO.sat=calc_DO_sat(dat$Temperature,dat$Pressure, model = 'garcia-benson') 


# Selecting the data types that are needed for stream metabolizer and changing header names. Running the model with K600_pooling = normal does not require discharge input

dat = dat %>% dplyr::select("solar.time" = solar.time,"DO.obs" = Dissolved_Oxygen,"DO.sat" = DO.sat,"temp.water" = Temperature,"light" = light,"depth" = Depth)

```

Check the number of cores you have in your computer. Based on the number that it prints, set the number of cores you want to dedicate to the metabolism run. It is recommended to set 2-4 cores less than you have in your computer to minimize the chances of R crashing. It is also recommended to select a pair number for your run. 

```{r core, echo = TRUE, warning=FALSE, message=FALSE}
parallel::detectCores()

```

## 3. Inspect Data
```{r inspect, echo = FALSE, warning=FALSE, message=FALSE}

dat %>% unitted::v() %>%
  mutate(DO.pctsat = 100 * (DO.obs / DO.sat)) %>%
  select(solar.time, starts_with('DO')) %>%
  gather(type, DO.value, starts_with('DO')) %>%
  mutate(units=ifelse(type == 'DO.pctsat', 'DO\n(% sat)', 'DO\n(mg/L)')) %>%
  ggplot(aes(x=solar.time, y=DO.value, color=type)) + geom_line() + 
  facet_grid(units ~ ., scale='free_y') + theme_bw() +
  scale_color_discrete('variable')

labels = c(temp.water='water temp\n(deg C)', light='PAR\n(umol m^-2 s^-1)')
dat %>% unitted::v() %>%
  select(solar.time, temp.water, light) %>%
  gather(type, value, temp.water, light) %>%
  mutate(
    type=ordered(type, levels=c('temp.water','light')),
    units=ordered(labels[type], unname(labels))) %>%
  ggplot(aes(x=solar.time, y=value, color=type)) + geom_line() +
  facet_grid(units ~ ., scale='free_y') + theme_bw() +
  scale_color_discrete('variable')

labels = c(temp.water='water temp\n(deg C)', depth='water depth')
dat %>% unitted::v() %>%
  select(solar.time, temp.water, depth) %>%
  gather(type, value, temp.water, depth) %>%
  mutate(
    type=ordered(type, levels=c('temp.water','depth')),
    units=ordered(labels[type], unname(labels))) %>%
  ggplot(aes(x=solar.time, y=value, color=type)) + geom_line() +
  facet_grid(units ~ ., scale='free_y') + theme_bw() +
  scale_color_discrete('variable')
```

## 4. Configure the model
We will select a Bayesian model. Then we will configure the specs of the model depending on the needs of our run.    
You can play around with the number of iterations (e.g., 100 burnin iterations , also called warm up) and 50 saved steps. You can adjust the number of iterations based on the convergence of the convergence of the model.  
For example, you may start with 1000 and 500 and then up the numbers to 2000 and 1000 if the model results don't seem to converge. 

If you have already adjusted the number of steps multiple times and your model fits are still not good (e.g., negative GPP values) you might have to consider changing other specs in the model. Some examples of variables that you may change are: GPP_daily_lower = 0.01,ER_daily_upper = -0.01. However, you should consult the Help for more information.

Use the command plot_distribs if you want to observe the distribution of the specs if they were changed. 


```{r modelsetup, echo = TRUE, warning=TRUE}
# Set the model

bayes_name = mm_name(type='bayes',
                     pool_K600='normal', 
                     err_obs_iid=TRUE, 
                     err_proc_iid=TRUE)
bayes_name
 # Options for pool K600 are binned, linear, none and normal. If normal is specified, discharge doesn't need to be provided

# Changing the specs
bayes_specs = specs(bayes_name, K600_daily_meanlog_meanlog=log(K600estimate), K600_daily_meanlog_sdlog=0.7, K600_daily_sdlog_sigma=0.02, burnin_steps=1000, 
                  saved_steps=1000)

```

## 5. Fit the model and save the results
Fitting the model might take hours or days depending on the dataset.
Some times R crashes while you are running the model so you want to make sure to save your results in each run. Create an output folder inside of the path where you are storing the data, the output path will update automatically here once you change your data path in step 2. 

Some key parameters to look at in the mm output are:  
- $daily (includes metabolism estimates) 
- $overall (includes error information) 
- $KQ_overall (included the relationship between K600-Q)

```{r fit, echo = TRUE, warning=TRUE}
mm = metab(bayes_specs, data=dat)# 
#load("~/GitHub/gitlab/SSS_metabolism/initial_SM_testing/test_15min.RData")

#Extracting the data from the model output the outputs are in a S4
#class of data and you'll need to operators to extract the daily
#time series of estimates
#get_fit(mm) %>%
  #lapply(names)

# Saving key data
#output.path = paste0(data.path,"/Output/")
preds = mm@fit$daily
preds_output<-preds
#str(preds)
write.csv(preds_output,paste0(output.path,'v2_',PARENT_ID,"_",SITE_ID,"_SM_final_daily_prediction_results.csv"),row.names = FALSE) 

instant = mm@fit$inst
instant_output<-instant
#str(instant)
write.csv(instant_output,paste0(output.path,'v2_',PARENT_ID,"_",SITE_ID,"_SM_final_instant_fit_results.csv"),row.names = FALSE)

Overall = mm@fit$overall
Overall_output<-Overall
#str(Overall)
write.csv(Overall_output,paste0(output.path,'v2_',PARENT_ID,"_",SITE_ID,"_SM_final_overall_fit_results.csv"),row.names = FALSE)

full = predict_DO(mm)
full_output<-full
#str(KQ)
write.csv(full_output,paste0(output.path,'v2_',PARENT_ID,"_",SITE_ID,"_SM_final_full_prediction_results.csv"),row.names = FALSE)


#get_data(mm)# Shows a table with all the data + DO modeled
#get_data_daily(mm) #daily fitting data. It shows values for Q for now
#get_params(mm)

```

## 6. Inspect GPP, ER and K600

### Daily predictions of modeled GPP and ER

The goal is for the predictions (lines) and observations (points) to be very similar. 

```{r inspect1, echo = FALSE, warning=FALSE, message=FALSE}
 
predict_metab(mm)# Daily predictions of GPP and ER for the model

  plot_metab_preds(mm)
  
  get_params(mm)#to inspect more of the fitted daily parameters
  predict_DO(mm) %>% head()
  plot_DO_preds(mm)
  

```

Ideally, good model results should have n_eff > 100 and Rhat < = 1.1. Below is a summary of these metrics for daily GPP, ER and K600. 


```{r inspect2, echo = FALSE, warning=FALSE, message=FALSE}
  get_fit(mm)$daily %>%
      select(ends_with('Rhat'))#Daily Rhat for GPP, ER y K600
    summary(preds$ER_Rhat)
    summary(preds$GPP_Rhat)
    summary(preds$K600_daily_Rhat)
    
    get_fit(mm)$daily %>%
      select(ends_with('n_eff'))#Daily n_eff for GPP, ER y K600
    summary(preds$ER_n_eff)
    summary(preds$GPP_n_eff)
    summary(preds$K600_daily_n_eff)
```

## 7. Inspect errors and their standard deviations

err_obs_iid_Rhat should have a value < 1.1. Additionally, in a conversation with Allison Appling she mentioned that they have seen pretty frequently err_obs_iid_sigma_Rhats much greater than 1.05, and mentioned it on the [JGR 2018 paper](https://doi.org/10.1002/2017JG004140). This is OK for that particular parameter because the values that the model continues to consider are usually quite similar in absolute magnitudes. 

### Inspect err_obs_iid_Rhat and err_obs_iid_sigma_Rhat
```{r inspect3, echo = TRUE, warning=FALSE, message=FALSE}

#err_obs_iid_Rhat
get_fit(mm)$inst %>%
  select(ends_with('Rhat'))

# err_obs_iid_sigma_Rhat. See comment from Appling
get_fit(mm)$overall %>%
  select(ends_with('Rhat'))

get_fit(mm)$inst %>%
  select(ends_with('n_eff'))
get_fit(mm)$overall %>%
  select(ends_with('n_eff'))


summary(instant$err_obs_iid_n_eff)
summary(instant$err_obs_iid_Rhat)
summary(instant$err_proc_iid_n_eff)
summary(instant$err_proc_iid_Rhat)  

summary(Overall$err_obs_iid_sigma_Rhat)# There is only one err_obs_iid_sigma_Rhat per run
summary(Overall$err_proc_iid_sigma_Rhat)
summary(Overall$err_obs_iid_sigma_n_eff)
summary(Overall$err_proc_iid_sigma_n_eff)
```

## 8. Inspect the relationships between variables

### Relationship between k600 and Q: we do not have Q in this experiment, so use depth as a proxy

The first step is to plot k600 and depth. To find this relationship we have to calculate the mean depth per day and then plot against k600.

```{r ins, echo = TRUE, warning=FALSE, message=FALSE}
    dat$solar.time = as.Date(dat$solar.time)
    meanDday = aggregate(dat["depth"],by = dat["solar.time"],mean)
    meanDday=meanDday[meanDday$solar.time %in% preds$date,]
    K600D = lm(preds$K600_daily_mean~meanDday$depth)

    plot(preds$K600_daily_mean,meanDday$depth,
         ylab="D_daily_mean",xlab="Daily_mean_K600")
    abline(lm(meanDday$depth~preds$K600_daily_mean))
      legend("topright", bty="n", legend=paste("R2 =", format(summary(K600D)$r.squared, digits=4)))
legend("top", bty="n", legend=paste("p = ", format(summary(K600D)$coefficients[8], digits=4)))

    summary(K600D)

```

### Relationship between k600 and ER

This relationship is key. A primary metric is to look for covariance between ER and K.  These have high equifinality which means that any combination of ER or K can provide an equally good fit to the model. If these covary strongly (r=0.6 or higher) then that may not be good for examining controls on variation in ER. If you find **no relationship**, that is a **very good** thing. If there is a relationship, that is if R2 is high, it means that the model can't parse reaeration from respiration. In this case, ER and k600 are working in opposite for O2, so ideally there is no relationship in the model outputs. If they covary, you can still use the period-avarage data, but day-to-day variation may not be trustworthy.

```{r ins plot, echo = TRUE, warning=FALSE, message=FALSE}
K600ER = lm(K600_daily_mean~ER_mean, data=preds)
   
    plot(preds$K600_daily_mean,preds$ER_mean,ylab="ER_mean",xlab="Daily_mean_K600")
    abline(lm(ER_mean~K600_daily_mean, data=preds))
    legend("topright", bty="n", legend=paste("R2 =", format(summary(K600ER)$r.squared, digits=4)))
legend("top", bty="n", legend=paste("p = ", format(summary(K600ER)$coefficients[8], digits=4)))
    
 summary(K600ER)
```

### Relationship between k600 and GPP
High GPP is needed to estimate K well, so K should get more variable as GPP decreases.

```{r ins plot2, echo = TRUE, warning=FALSE, message=FALSE}
K600GPP = lm(GPP_mean~K600_daily_mean, data=preds)

    
    plot(preds$K600_daily_mean,preds$GPP_mean,ylab="GPP_mean",xlab="Daily_mean_K600")
    abline(lm(GPP_mean~K600_daily_mean, data=preds))
    legend("topright", bty="n", legend=paste("R2 =", format(summary(K600GPP)$r.squared, digits=4)))
legend("top", bty="n", legend=paste("p = ", format(summary(K600GPP)$coefficients[8], digits=4)))
     
    summary(K600GPP)

```

### Time series daily mean k600 
Because this work was carried out in generally consistent weather and flow conditions, we expect day-to-day variation in k600 to be small

```{r ins4, echo = TRUE, warning=FALSE, message=FALSE}
    preds$date = as.Date(preds$date)
  
    plot(preds$date,preds$K600_daily_mean,
         xlab="Date",ylab="Daily_mean_K600")
    
```
