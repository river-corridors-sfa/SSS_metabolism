data.path = "C:/Users/gara009/OneDrive - PNNL/Documents/01_MFC Manuscript/Data"
setwd(data.path)
dat = read.csv("SM_WY_2021 - manual_clean.csv",header=T)
file.name = 'MFC_2021_K600pete_Kfixed_MLE'
dat = na.omit(dat)
# Change date time format
dat$DATE_TIME = as.POSIXct(dat$DATE_TIME, format = "%m/%d/%Y %H:%M", tz="Etc/GMT+8")
#dat$DATE_TIME = as.POSIXct(dat$DATE_TIME, format = "%Y-%m-%d %H:%M:%S", tz="Etc/GMT+8")
# Transform date time into solar time
dat$DATE_TIME = calc_solar_time(dat$DATE_TIME, longitude = -117.1)
# Longitude is from your field site
# Change discharge from L/s to m3
dat$Q_m3 = dat$Q_L_per_s*0.001
# Calculate in mg/L for much DO you would have in the water at the current saturation conditions
dat$DOsat_mg_per_L =(dat$DO_mg_per_L*100)/dat$Dosat_pct
# Reducing the number of decimals after performing the saturation calculations
dat$Dosat_mg_per_L = round(dat$DOsat_mg_per_L,2)
# Selecting the data types that are needed for stream metabolizer and changing header names. Running the model with K600_pooling = normal does not require discharge input
temp = dat
dat = cbind.data.frame(temp$DATE_TIME,temp$DO_mg_per_L,
temp$DOsat_mg_per_L,temp$DEPTH_m,
temp$TEMP_degreesC,
temp$PAR_umol_per_m2_per_sec,
temp$Q_m3)
colnames(dat) = c("solar.time","DO.obs","DO.sat","depth","temp.water","light","discharge")
data.path = "C:/Users/gara009/OneDrive - PNNL/Documents/01_MFC Manuscript/Data"
setwd(data.path)
dat = read.csv("SM_WY_2021 - manual_clean.csv",header=T)
file.name = 'MFC_2021_K600pete_Kfixed_MLE_modeled_light'
dat = na.omit(dat)
# Change date time format
dat$DATE_TIME = as.POSIXct(dat$DATE_TIME, format = "%m/%d/%Y %H:%M", tz="Etc/GMT+8")
# Change discharge from L/s to m3
dat$Q_m3 = dat$Q_L_per_s*0.001
# Calculate in mg/L for much DO you would have in the water at the current saturation conditions
dat$DOsat_mg_per_L =(dat$DO_mg_per_L*100)/dat$Dosat_pct
# Reducing the number of decimals after performing the saturation calculations
dat$Dosat_mg_per_L = round(dat$DOsat_mg_per_L,2)
# Calculating light data based on lat and long using stream metabolizer
dat$li = calc_light(dat$solar.time,46.78,  -117.1,max.PAR =2300, attach.units = F)
View(dat)
data.path = "C:/Users/gara009/OneDrive - PNNL/Documents/01_MFC Manuscript/Data"
setwd(data.path)
dat = read.csv("SM_WY_2021 - manual_clean.csv",header=T)
file.name = 'MFC_2021_K600pete_Kfixed_MLE_modeled_light'
dat = na.omit(dat)
# Change date time format
dat$DATE_TIME = as.POSIXct(dat$DATE_TIME, format = "%m/%d/%Y %H:%M", tz="Etc/GMT+8")
#dat$DATE_TIME = as.POSIXct(dat$DATE_TIME, format = "%Y-%m-%d %H:%M:%S", tz="Etc/GMT+8")
# Transform date time into solar time
dat$solar.time= calc_solar_time(dat$DATE_TIME, longitude = -117.1)
# Change discharge from L/s to m3
dat$Q_m3 = dat$Q_L_per_s*0.001
# Calculate in mg/L for much DO you would have in the water at the current saturation conditions
dat$DOsat_mg_per_L =(dat$DO_mg_per_L*100)/dat$Dosat_pct
# Reducing the number of decimals after performing the saturation calculations
dat$Dosat_mg_per_L = round(dat$DOsat_mg_per_L,2)
# Calculating light data based on lat and long using stream metabolizer
dat$li = calc_light(dat$solar.time,46.78,  -117.1,max.PAR =2300, attach.units = F)
# Selecting the data types that are needed for stream metabolizer and changing header names. Running the model with K600_pooling = normal does not require discharge input
data = dat %>% dplyr::select('solar.time','DO_mg_per_L',
'DOsat_mg_per_L','DEPTH_m','TEMP_degreesC','li')
# Selecting the data types that are needed for stream metabolizer and changing header names. Running the model with K600_pooling = normal does not require discharge input
data = dat %>% dplyr::select('solar.time','DO_mg_per_L',
'DOsat_mg_per_L','DEPTH_m','TEMP_degreesC','li',"Q_m3")
colnames(dat) = c("solar.time","DO.obs","DO.sat","depth","temp.water","light","discharge")
K600estimate=read.csv('k600.csv',header=T)
data.path = "C:/Users/gara009/OneDrive - PNNL/Documents/01_MFC Manuscript/Data"
setwd(data.path)
dat = read.csv("SM_WY_2021 - manual_clean.csv",header=T)
file.name = 'MFC_2021_K600pete_Kfixed_MLE_modeled_light'
dat = na.omit(dat)
# Change date time format
dat$DATE_TIME = as.POSIXct(dat$DATE_TIME, format = "%m/%d/%Y %H:%M", tz="Etc/GMT+8")
#dat$DATE_TIME = as.POSIXct(dat$DATE_TIME, format = "%Y-%m-%d %H:%M:%S", tz="Etc/GMT+8")
# Transform date time into solar time
dat$solar.time= calc_solar_time(dat$DATE_TIME, longitude = -117.1)
# Longitude is from your field site
# Change discharge from L/s to m3
dat$Q_m3 = dat$Q_L_per_s*0.001
# Calculate in mg/L for much DO you would have in the water at the current saturation conditions
dat$DOsat_mg_per_L =(dat$DO_mg_per_L*100)/dat$Dosat_pct
# Reducing the number of decimals after performing the saturation calculations
dat$Dosat_mg_per_L = round(dat$DOsat_mg_per_L,2)
# Calculating light data based on lat and long using stream metabolizer
dat$li = calc_light(dat$solar.time,46.78,  -117.1,max.PAR =2300, attach.units = F)
# Selecting the data types that are needed for stream metabolizer and changing header names. Running the model with K600_pooling = normal does not require discharge input
data = dat %>% dplyr::select('solar.time','DO_mg_per_L',
'DOsat_mg_per_L','DEPTH_m','TEMP_degreesC','li',"Q_m3")
dat = data
colnames(dat) = c("solar.time","DO.obs","DO.sat","depth","temp.water","light","discharge")
K600estimate=read.csv('k600.csv',header=T)
K600estimate$k600_md
# Set the model
#create a dataframe for daily K values.
MFCdaily<-tibble(date=as_date(dat$solar.time) , K600.daily=K600estimate$k600_md) #this code matches up dates
#removes duplicates
MFCdaily<-distinct(MFCdaily)
#Run the model
MFCall_fit<-metab_mle(data=dat, data_daily = MFCdaily, info=(site='MFC, K =2.84'))
View(MFCdaily)
View(dat)
rm(list=ls(all=T))
library(sf)
library(tidyverse)
library(terra)
library(nhdplusTools)
library(mapview)
library(dataRetrieval)
library(lubridate)
library(prism)
library(ggspatial)
library(nngeo)# Added from OG code
library(stars)# Added from OG code
# this gives you an error, but it can be ignored:
try(plyr::ldply(list.files(path="src/",
pattern="*.R",
full.names=TRUE),
source))
# Rmarkdown options
knitr::opts_chunk$set(echo = T, warning = F, comment = F, message = F)
# mapview options
mapviewOptions(basemaps.color.shuffle=FALSE,basemaps='OpenTopoMap')
sf_use_s2(FALSE)
site_type = "comid" # This steps assumes you have checked your COMIDs
sites <- read_csv("data/WROL_comid.csv")
if(site_type == "comid"){
sites <- getNHDcomid(df = dplyr::select(sites, site, comid))
}
site_watersheds <- getWatersheds(df = sites, make_pretty = TRUE) %>%
inner_join(., select(sf::st_drop_geometry(sites), site, comid), by = "comid")
suppressWarnings(map2(sites$site, sites$comid, getMaps))
mapview(site_watersheds, col.regions = "#56B4E9", alpha.regions = 0.2, lwd = 3, layer.name = "Watershed") +
mapview(sites, cex = 5, col.regions = "black", layer.name = "Points") +
mapview(st_read('data/site_flowlines.gpkg', quiet = T), lwd = 3, color = "red", layer.name = "Flowline")
# ==== Set working directories and load data =======
s19s.path = 'C:/Users/gara009/Downloads/WHONDRS_S19S_Sediment_v7/'
data.path = 'C:/Users/gara009/OneDrive - PNNL/Documents - Core Richland and Sequim Lab-Field Team/Data Generation and Files/ICON_ModEx_SSS/03_Processed_Respiration_by_kit/'
norm.path = 'C:/Users/gara009/OneDrive - PNNL/Documents - Core Richland and Sequim Lab-Field Team/Data Generation and Files/ICON_ModEx_SSS/Subsampling/Sediment_Mass_Water_Calculations/'
# ==== Respiration data =====
files = list.files(pattern = '.csv',path = data.path, full.names = T)
icon.files = files[!grepl("SSS_|Fast_Rate",files)]
fast.files = files[grep("Fast_Rate",files)]
sss.files = files[grep("SSS_",files)]
# read in the lasted file with incubation compiled
data.icon = read.csv(icon.files[which.max(file.mtime(icon.files))])
data.fast = read.csv(fast.files[which.max(file.mtime(fast.files))])
data.sss = read.csv(sss.files[which.max(file.mtime(sss.files))])
data.s19s = read.csv(paste0(s19s.path,'WHONDRS_S19S_Sediment_Incubations_Respiration_Rates.csv'))
View(data.s19s)
View(data.icon)
icon.files = files[!grepl("SSS_|Fast_Rate|VGC",files)]
# read in the lasted file with incubation compiled
data.icon = read.csv(icon.files[which.max(file.mtime(icon.files))])
# Combine all the data
data = rbind(data.icon,data.fast,data.sss)
colnames(data)
colnames(data.s19s)
# Combine all the data
data = rbind(data.icon,data.fast,data.sss) %>% dplyr::select(-"number_of_points",-"No_points_removed",-"DO_time_zero" )
library(stringr); library(devtools);  library("plyr")
library("readr"); library(tidyverse); library(readxl);library(crayon);
library(tidyr)
# Combine all the data
data = rbind(data.icon,data.fast,data.sss) %>% dplyr::select(-"number_of_points",-"No_points_removed",-"DO_time_zero" )
data = rbind(data,data.s19s)
# === Normalization data ======
# Read in the normalization file
norm.files = list.files(pattern = 'Sediment_Incubations_Sediment',path = norm.path, full.names = T)
norm.path
# This brings in all of the sheets into one but requres that old versions of the sheet get archived
norm = do.call(rbind,lapply(norm.files,read.csv))
View(norm)
# fixing names
norm$Sample_ID = gsub('_INC',"",norm$Sample_ID)
norm$Sample_ID = gsub('CM-',"CM_",norm$Sample_ID)
View(data.s19s)
# fixing names
norm$Sample_ID = gsub('_INC',"",norm$Sample_ID)
norm$Sample_ID = gsub('SED_','SED_INC',norm$Sample_ID)
norm$Sample_ID = gsub('CM-',"CM_",norm$Sample_ID)
View(data)
View(norm)
# ==== Merge norm data with data =====
df = merge(data,norm, by = 'Sample_ID')
View(df)
View(norm)
norm$Sample_ID
data$Sample_ID
length(unique(df$Sample_ID))
length(unique(data$Sample_ID))
View(data)
View(data.icon)
# ==== Loading libraries =========
rm(list=ls(all=T))
library(stringr); library(devtools);  library("plyr")
library("readr"); library(tidyverse); library(readxl);library(crayon);
library(tidyr)
# ==== Set working directories and load data =======
s19s.path = 'C:/Users/gara009/Downloads/WHONDRS_S19S_Sediment_v7/'
data.path = 'C:/Users/gara009/OneDrive - PNNL/Documents - Core Richland and Sequim Lab-Field Team/Data Generation and Files/ICON_ModEx_SSS/03_Processed_Respiration_by_kit/'
norm.path = 'C:/Users/gara009/OneDrive - PNNL/Documents - Core Richland and Sequim Lab-Field Team/Data Generation and Files/ICON_ModEx_SSS/Subsampling/Sediment_Mass_Water_Calculations/'
# ==== Respiration data =====
files = list.files(pattern = '.csv',path = data.path, full.names = T)
icon.files = files[!grepl("SSS_|Fast_Rate|VGC",files)]
fast.files = files[grep("Fast_Rate",files)]
sss.files = files[grep("SSS_",files)]
# read in the lasted file with incubation compiled
data.icon = read.csv(icon.files[which.max(file.mtime(icon.files))])
data.fast = read.csv(fast.files[which.max(file.mtime(fast.files))])
#data.sss = read.csv(sss.files[which.max(file.mtime(sss.files))])
data.s19s = read.csv(paste0(s19s.path,'WHONDRS_S19S_Sediment_Incubations_Respiration_Rates.csv'))
data = rbind(data.icon,data.fast) %>% dplyr::select(-"number_of_points",-"No_points_removed",-"DO_time_zero" )
data = rbind(data,data.s19s)
# === Normalization data ======
# Read in the normalization file
norm.files = list.files(pattern = 'Sediment_Incubations_Sediment',path = norm.path, full.names = T)
# This brings in all of the sheets into one but requres that old versions of the sheet get archived
norm = do.call(rbind,lapply(norm.files,read.csv))
# fixing names
norm$Sample_ID = gsub('_INC',"",norm$Sample_ID)
norm$Sample_ID = gsub('SED_','SED_INC',norm$Sample_ID)
norm$Sample_ID = gsub('CM-',"CM_",norm$Sample_ID)
# ==== Merge norm data with data =====
df = merge(data,norm, by = 'Sample_ID')
View(norm)
names(norm)[4] = 'volume_of_water_incubated_mL'
# ==== Merge norm data with data =====
df = merge(data,norm, by = 'Sample_ID')
# ==== Normalization ====
df$rate_mg_per_L_per_h_scaled_per_volume_of_water_and_sediment = df$rate_mg_per_L_per_h * (df$volume_of_water_incubated_mL/df$sed_mL)
require(easycsv)
require(tidyverse)
# User Input
Sample_Name = "MFC" # Sample name for output
### Load in data ###
setwd(easycsv::choose_dir())
rm(list=ls(all=T))
library(dplyr); library(ggplot2); library(GGally)
library(tidyr); library (reshape2);library(lubridate)
library(FSA) # For DunnTest
# ==== Read data =====
data2021 = read.csv("Data/StreamMetabolizer/Results_MFC_2020-2021_all_data_sigma_sigma_pt2_no_june_may_2022-02-02.csv") %>% dplyr::select(date,GPP_2021 = GPP_mean,ER_2021 = ER_mean)
# ==== Loading libraries =========
rm(list=ls(all=T))
library(stringr); library(devtools);  library("plyr")
library("readr"); library(tidyverse); library(readxl);library(crayon);
library(tidyr)
# ==== Set working directories and load data =======
s19s.path = 'C:/Users/gara009/Downloads/WHONDRS_S19S_Sediment_v7/RespirationRateCalculation_R_Code_and_Plots/Inputs/'
data.path = 'C:/Users/gara009/OneDrive - PNNL/Documents - Core Richland and Sequim Lab-Field Team/Data Generation and Files/ICON_ModEx_SSS/03_Processed_Respiration_by_kit/'
norm.path = 'C:/Users/gara009/OneDrive - PNNL/Documents - Core Richland and Sequim Lab-Field Team/Data Generation and Files/ICON_ModEx_SSS/Subsampling/Sediment_Mass_Water_Calculations/'
# ==== Respiration data =====
# Reading the files with original respiration rates calculated with 00_ICON-ModEx...
files = list.files(pattern = '.csv',path = data.path, full.names = T)
icon.files = files[!grepl("SSS_|Fast_Rate|VGC",files)]
fast.files = files[grep("Fast_Rate",files)]
sss.files = files[grep("SSS_",files)] # This might not need to be read in if Brie has merged the file into the icon one
# read in the last file with incubation compiled
data.icon = read.csv(icon.files[which.max(file.mtime(icon.files))])
data.fast = read.csv(fast.files[which.max(file.mtime(fast.files))])
data.sss = read.csv(sss.files[which.max(file.mtime(sss.files))])
data.s19s = read.csv(paste0(s19s.path,'WHONDRS_S19S_Sediment_Incubations_Respiration_Rates_06-01-23_v2.csv'))
data.s19s$No_points_removed = 0
# Combine all the data
##
# Use this chunk if you are reading in SSS data
data = rbind(data.icon,data.fast,data.sss,data.s19s)
##
# === Scaling data ======
# Read in the scaling file
norm.files = list.files(pattern = 'Sediment_Incubations_Sediment',path = norm.path, full.names = T)
# This brings in all of the sheets into one but requires that old versions of the sheet get archived into archive folder
norm = do.call(rbind,lapply(norm.files[!grepl('S19S',norm.files)],read.csv))
names(norm)[grep('Water_Mass_g', colnames(norm))] = 'volume_of_water_incubated_mL'
# read in S19S
norm2 = do.call(rbind,lapply(norm.files[grep('S19S',norm.files)],read.csv))
names(norm2)[grep('Water_Mass_g', colnames(norm2))] = 'volume_of_water_incubated_mL'
# Select the same number of columns of both norm files and then rbind
norm = norm %>% dplyr::select(Sample_ID,volume_of_water_incubated_mL,Wet_Sediment_mL)
# # Note that Sample_ID names for the norm2 data file have to be fixed so we have all the digits before the kit number
# norm2$samples = gsub('S19S_','S19S_00',norm2$Sample_ID)
# norm2$samples = gsub('S19S_00100','S19S_0100',norm2$samples)
# norm2$samples = gsub('_INC','_SED_INC',norm2$samples)
norm2 = norm2 %>% dplyr::select(Sample_ID = Sample_ID,volume_of_water_incubated_mL,Wet_Sediment_mL)
norm = rbind(norm,norm2)
# fixing names
norm$Sample_ID = gsub('_INC',"",norm$Sample_ID)
norm$Sample_ID = gsub('SED','SED_INC',norm$Sample_ID)
norm$Sample_ID = gsub('CM-',"CM_",norm$Sample_ID)
# ==== Merge norm data with data =====
df = merge(data,norm, by = 'Sample_ID', all = T)
df = df[!is.na(df$DO_time_zero),] # removing rows with no time zero DO data
# ==== Calculate Theoretical rates ======
# On 5-11-23 and 6-14-23 it was decided that if the DO at time zero was lower than 5 ppm then we would replace the measured value with a theoretical rate. This rate is calculated assuming that we start at DO saturation conditions and it takes 30 seconds to go from that concentration to the measured value. Additionally, for CM and SSS, if two reps are triggered as theoretical, then trigger the third to also be theoretical. However if only 1 rep triggers as theoretical, then we exclude that rep and only estimate/report respiration from the other two reps.
# Max respiration calculations
DOmax = 8.5 # Assuming all the reactors start at this concentration
time = 0.00833333 # 30 seconds that need to be transformed to hours
#Select the reactors that had DO_time_zero below 3 and incubation time < 3 min for S19S or only collected one point of DO at time zero for S19S
#data = rbind(data,data2)
df$new_rate_mg_per_L_per_h = NA
df$Type = NA
for (i in 1:nrow(df)){
if (df$Wet_Sediment_mL[i]==10){
if (df$DO_time_zero[i]<5|df$number_of_points[i]==1|df$total_incubation_time_min[i]<3){
df$new_rate_mg_per_L_per_h[i] = round(abs((as.numeric(df$DO_time_zero[i])-DOmax)/time),2)
df$Type[i] = 'Theoretical'
}else{
df$new_rate_mg_per_L_per_h[i] = round(as.numeric(df$rate_mg_per_L_per_h[i]),2)
df$Type[i] = 'Measured_10mL'
}
}else{
df$new_rate_mg_per_L_per_h[i] = round(as.numeric(df$rate_mg_per_L_per_h[i]),2)
df$Type[i] = 'Measured_2.5mL'
}
}
# There are some S19S vials that don't have water volume calculated, for these we will just replace those -9999 values with the average water volume from the rest of the S19S vials
values.s19s = df$volume_of_water_incubated_mL[grep('S19S',df$Sample_ID)]
values.sss = df$volume_of_water_incubated_mL[grep('SSS',df$Sample_ID)]
df$Water_volume_Flag = FALSE
df$Water_volume_Flag[grep('-9999',df$volume_of_water_incubated_mL)] = TRUE
df$volume_of_water_incubated_mL[grep('SSS007-3',df$Sample_ID)] = mean(values.sss[values.sss!= -9999])
df$volume_of_water_incubated_mL[grep('-9999',df$volume_of_water_incubated_mL)] = mean(values.s19s[values.s19s!= -9999])
# ==== Normalizing ====
df$rate_mg_per_L_per_h_scaled_per_volume_of_water_and_sediment = round(df$new_rate_mg_per_L_per_h * ((df$volume_of_water_incubated_mL*0.001)/(0.001*df$Wet_Sediment_mL)),2)
# ==== Check for replicates ======
df$site = substr(df$Sample_ID,1,(nchar(df$Sample_ID)-2))
df$Notes = NA
for (i in 1:nrow(df)){
if (length(grep('CM|SSS', df$site[i]))==1){
temp = subset(df,df$site==df$site[i])
if (length(grep('Measured',temp$Type))==2){
sample = temp$Sample_ID[!grepl('Measured',temp$Type)]
df$rate_mg_per_L_per_h_scaled_per_volume_of_water_and_sediment[which(df$Sample_ID == sample)] = -9999
df$new_rate_mg_per_L_per_h[which(df$Sample_ID == sample)] = -9999
df$Type[(df$Sample_ID == sample)] = -9999
df$Notes[(df$Sample_ID == sample)] = 'Respiration removed because the other 2 reps were measured and this was originally theoretical.'
}
if (length(grep('Theoretical',temp$Type))==2){
sample = temp$Sample_ID[!grepl('Theoretical',temp$Type)]
df$new_rate_mg_per_L_per_h[which(df$Sample_ID == sample)] = round(abs((as.numeric(df$DO_time_zero[i])-DOmax)/time),2)
df$rate_mg_per_L_per_h_scaled_per_volume_of_water_and_sediment[which(df$Sample_ID == sample)] = round(df$new_rate_mg_per_L_per_h[which(df$Sample_ID == sample)] * ((df$volume_of_water_incubated_mL[which(df$Sample_ID == sample)]*0.001)/(0.001*df$Wet_Sediment_mL[which(df$Sample_ID == sample)])),2)
df$Notes[(df$Sample_ID == sample)] = 'Respiration was originally measured and it was replaced with a theoretical value based on the other 2 reps being theoretical.'
df$Type[(df$Sample_ID == sample)] = 'Theoretical'
}
}
}
# Export all the data
#write.csv(df,paste0(data.path,'Respiration_rates_ICON_082823.csv'),row.names = F)
# ==== Remove the kits with reps 1,2,3 and label reps 4,5,6
fast.rates.kits = unique(df$site[grep('-4|-5|-6',df$Sample_ID)])
modified.kits = df$Sample_ID[grep('-4|-5|-6',df$Sample_ID)]
# On 6-15-23 it was decided that when kits were incubated at both 10ml and 2.5 ml, the process to decide which volume remains in the final data will be dependent on weather or not the 10 ml presented any theoretical values. In the event there is at least one rep with a theoretical value in the 10 ml kits, those would be removed from the data and we would keep only the 2.5 ml. In the event that there is only one rep from a 10 ml incubation, the code also selects the 2.5 ml incubation volumes
fast.kits = NA
for (i in 1:length(fast.rates.kits)){
for (j in 1:3){
temp = paste0(fast.rates.kits[i],'-',j)
fast.kits = c(fast.kits,temp)
}
}
fast.kits = na.omit(fast.kits)
for (i in 1:length(fast.rates.kits)){
original.kits = fast.kits[grep(fast.rates.kits[i],fast.kits)]
regex_pattern = paste(original.kits, collapse = "|")
temp = df[grep(regex_pattern,df$Sample_ID),]
if(length(grep('Theoretical',temp$Type))>0|nrow(temp)<3){
df = filter(df, !Sample_ID %in% original.kits)
}else{
mod.kits = modified.kits[grep(fast.rates.kits[i],modified.kits)]
df = filter(df, !Sample_ID %in% mod.kits)
}
}
# === Plots =====
ggplot(df, aes(x=rate_mg_per_L_per_h_scaled_per_volume_of_water_and_sediment, fill = Type)) +
geom_histogram(bins = 10)+
labs(title = "")+
theme_bw()
#ggsave("S19S_CM_SSS_Rates_Resp_scaled.png")
ggplot(df, aes(x=rate_mg_per_L_per_h_scaled_per_volume_of_water_and_sediment)) +
geom_histogram(bins = 20)+
labs(title = "")+
theme_bw()+ facet_wrap(~Type, scales = 'free')
ggplot(df, aes(x=log10(rate_mg_per_L_per_h_scaled_per_volume_of_water_and_sediment))) +
geom_density()+
labs(title = "")+
theme_bw()
ggplot(df, aes(x=log10(rate_mg_per_L_per_h_scaled_per_volume_of_water_and_sediment))) +
geom_histogram()+
labs(title = "", x = expression(log10~(Respiration~rate~mg[DO]~L[sediment]^{-1}~h^{-1})))+
theme(axis.text=element_text(size=12, face = 'bold') ) +
theme_bw()
ggsave('respiration_histogram.pdf')
# Final data
# data.final = df %>% dplyr::select(colnames(df),volume_of_water_incubated_mL,volume_of_sediment_incubated_mL = "sed_mL", rate_mg_per_L_per_h_scaled_per_volume_of_water_and_sediment)
data.final = df
# === Export the data ===
write.csv(data.final,paste0(data.path,'VGC_scaled_and_theoretical_respiration_',Sys.Date(),'.csv'), row.names = F)
folder<-'Outputs/'
filenamelist<-list.files(folder, pattern='daily_prediction_results')
SITE_IDlist<-sapply(strsplit(filenamelist, "_"), "[", 3)
PARENT_IDlist<-sapply(strsplit(filenamelist, "_"), "[", 2)
depthfolder<-'Inputs/Sensor_Files/'
depthfilenamelist<-list.files(depthfolder, pattern='csv')
depthPARENT_IDlist<-  sapply(strsplit(depthfilenamelist, "_"), "[", 2)
#create output dataframe
SMresults <- data.frame(matrix(ncol = 11, nrow = 0))
colnames(SMresults) <- c("Parent_ID","Site_ID", "daysofdata", "ERdailymeanmean_gO2/m2day", "GPPdailymeanmean_gO2/m2day", "K600dailymeanmean_m/day","mean_depth_m","ERdailymeanmean_gO2/m3day","GPPdailymeanmean_gO2/m3day","K600vsERrsq","K600vsERp")
#start loop
for(i in 1:length(PARENT_IDlist)) {
#for(i in 1) {
data = read.csv(paste(folder,filenamelist[i],sep=''),header=T)
numdays<-length(data$ER_daily_mean[!is.na(data$ER_daily_mean)])
ERdailymeanmean<-mean(data$ER_daily_mean[!is.na(data$ER_daily_mean)])
GPPdailymeanmean<-mean(data$GPP_daily_mean[!is.na(data$GPP_daily_mean)])
K600dailymeanmean<-mean(data$K600_daily_mean[!is.na(data$K600_daily_mean)])
depthfilenumber<-which(depthPARENT_IDlist == PARENT_IDlist[i])
depthdata<-read.csv(paste(depthfolder,depthfilenamelist[depthfilenumber],sep=''),header=T,skip=8)
meandepthm<-mean(depthdata$Depth,na.rm=TRUE)
ERdailymeanmeanm3<-ERdailymeanmean/meandepthm
GPPdailymeanmeanm3<-GPPdailymeanmean/meandepthm
lmK600ER = lm(ER_daily_mean~K600_daily_mean, data = data) #Create the linear regression
K600vsERp=summary(lmK600ER)$coefficients[2,4]
K600vsERrsq=summary(lmK600ER)$r.squared
SMresults[nrow(SMresults) + 1,] = c(PARENT_IDlist[i],SITE_IDlist[i], numdays,ERdailymeanmean,GPPdailymeanmean,K600dailymeanmean,meandepthm,ERdailymeanmeanm3,GPPdailymeanmeanm3,K600vsERrsq,K600vsERp)
}
setwd("~/GitHub/SSS_metabolism/v2_SSS_Ecosystem_Respiration_Data_Package_STAGING")
folder<-'Outputs/'
filenamelist<-list.files(folder, pattern='daily_prediction_results')
SITE_IDlist<-sapply(strsplit(filenamelist, "_"), "[", 3)
PARENT_IDlist<-sapply(strsplit(filenamelist, "_"), "[", 2)
depthfolder<-'Inputs/Sensor_Files/'
depthfilenamelist<-list.files(depthfolder, pattern='csv')
depthPARENT_IDlist<-  sapply(strsplit(depthfilenamelist, "_"), "[", 2)
#create output dataframe
SMresults <- data.frame(matrix(ncol = 11, nrow = 0))
colnames(SMresults) <- c("Parent_ID","Site_ID", "daysofdata", "ERdailymeanmean_gO2/m2day", "GPPdailymeanmean_gO2/m2day", "K600dailymeanmean_m/day","mean_depth_m","ERdailymeanmean_gO2/m3day","GPPdailymeanmean_gO2/m3day","K600vsERrsq","K600vsERp")
#start loop
for(i in 1:length(PARENT_IDlist)) {
#for(i in 1) {
data = read.csv(paste(folder,filenamelist[i],sep=''),header=T)
numdays<-length(data$ER_daily_mean[!is.na(data$ER_daily_mean)])
ERdailymeanmean<-mean(data$ER_daily_mean[!is.na(data$ER_daily_mean)])
GPPdailymeanmean<-mean(data$GPP_daily_mean[!is.na(data$GPP_daily_mean)])
K600dailymeanmean<-mean(data$K600_daily_mean[!is.na(data$K600_daily_mean)])
depthfilenumber<-which(depthPARENT_IDlist == PARENT_IDlist[i])
depthdata<-read.csv(paste(depthfolder,depthfilenamelist[depthfilenumber],sep=''),header=T,skip=8)
meandepthm<-mean(depthdata$Depth,na.rm=TRUE)
ERdailymeanmeanm3<-ERdailymeanmean/meandepthm
GPPdailymeanmeanm3<-GPPdailymeanmean/meandepthm
lmK600ER = lm(ER_daily_mean~K600_daily_mean, data = data) #Create the linear regression
K600vsERp=summary(lmK600ER)$coefficients[2,4]
K600vsERrsq=summary(lmK600ER)$r.squared
SMresults[nrow(SMresults) + 1,] = c(PARENT_IDlist[i],SITE_IDlist[i], numdays,ERdailymeanmean,GPPdailymeanmean,K600dailymeanmean,meandepthm,ERdailymeanmeanm3,GPPdailymeanmeanm3,K600vsERrsq,K600vsERp)
}
View(SMresults)
your_data <- SMresults %>%
mutate(
ERdailymeanmean = ifelse(PARENT_ID %in% c("SSS010", "SSS023", "SSS028", "SSS038"), -9999, ERdailymeanmean),
GPPdailymeanmean = ifelse(PARENT_ID %in% c("SSS010", "SSS023", "SSS028", "SSS038"), -9999, GPPdailymeanmean),
K600dailymeanmean = ifelse(PARENT_ID %in% c("SSS010", "SSS023", "SSS028", "SSS038"), -9999, K600dailymeanmean),
ERdailymeanmeanm3 = ifelse(PARENT_ID %in% c("SSS010", "SSS023", "SSS028", "SSS038"), -9999, ERdailymeanmeanm3),
GPPdailymeanmeanm3 = ifelse(PARENT_ID %in% c("SSS010", "SSS023", "SSS028", "SSS038"), -9999, GPPdailymeanmeanm3),
K600vsERrsq = ifelse(PARENT_ID %in% c("SSS010", "SSS023", "SSS028", "SSS038"), -9999, K600vsERrsq),
K600vsERp = ifelse(PARENT_ID %in% c("SSS010", "SSS023", "SSS028", "SSS038"), -9999, K600vsERp)
)
library(tidyverse)
your_data <- SMresults %>%
mutate(
ERdailymeanmean = ifelse(PARENT_ID %in% c("SSS010", "SSS023", "SSS028", "SSS038"), -9999, ERdailymeanmean),
GPPdailymeanmean = ifelse(PARENT_ID %in% c("SSS010", "SSS023", "SSS028", "SSS038"), -9999, GPPdailymeanmean),
K600dailymeanmean = ifelse(PARENT_ID %in% c("SSS010", "SSS023", "SSS028", "SSS038"), -9999, K600dailymeanmean),
ERdailymeanmeanm3 = ifelse(PARENT_ID %in% c("SSS010", "SSS023", "SSS028", "SSS038"), -9999, ERdailymeanmeanm3),
GPPdailymeanmeanm3 = ifelse(PARENT_ID %in% c("SSS010", "SSS023", "SSS028", "SSS038"), -9999, GPPdailymeanmeanm3),
K600vsERrsq = ifelse(PARENT_ID %in% c("SSS010", "SSS023", "SSS028", "SSS038"), -9999, K600vsERrsq),
K600vsERp = ifelse(PARENT_ID %in% c("SSS010", "SSS023", "SSS028", "SSS038"), -9999, K600vsERp)
)
View(SMresults)
your_data <- SMresults %>%
mutate(
ERdailymeanmean = ifelse(Parent_ID %in% c("SSS010", "SSS023", "SSS028", "SSS038"), -9999, ERdailymeanmean),
GPPdailymeanmean = ifelse(Parent_ID %in% c("SSS010", "SSS023", "SSS028", "SSS038"), -9999, GPPdailymeanmean),
K600dailymeanmean = ifelse(Parent_ID %in% c("SSS010", "SSS023", "SSS028", "SSS038"), -9999, K600dailymeanmean),
ERdailymeanmeanm3 = ifelse(Parent_ID %in% c("SSS010", "SSS023", "SSS028", "SSS038"), -9999, ERdailymeanmeanm3),
GPPdailymeanmeanm3 = ifelse(Parent_ID %in% c("SSS010", "SSS023", "SSS028", "SSS038"), -9999, GPPdailymeanmeanm3),
K600vsERrsq = ifelse(Parent_ID %in% c("SSS010", "SSS023", "SSS028", "SSS038"), -9999, K600vsERrsq),
K600vsERp = ifelse(Parent_ID %in% c("SSS010", "SSS023", "SSS028", "SSS038"), -9999, K600vsERp)
)
View(your_data)
View(SMresults)
your_data <- SMresults %>%
mutate_at(
vars("ERdailymeanmean_gO2/m2day", "GPPdailymeanmean_gO2/m2day", "K600dailymeanmean_m/day", "ERdailymeanmean_gO2/m3day", "GPPdailymeanmean_gO2/m3day", "K600vsERrsq", "K600vsERp"),
funs(ifelse(PARENT_ID %in% c("SSS010", "SSS023", "SSS028", "SSS038"), -9999, .))
)
your_data <- SMresults %>%
mutate_at(
vars("ERdailymeanmean_gO2/m2day", "GPPdailymeanmean_gO2/m2day", "K600dailymeanmean_m/day", "ERdailymeanmean_gO2/m3day", "GPPdailymeanmean_gO2/m3day", "K600vsERrsq", "K600vsERp"),
funs(ifelse(PARENT_ID %in% c("SSS010", "SSS023", "SSS028", "SSS038"), -9999, .))
)
View(your_data)
rm(your_data)
your_data <- SMresults %>%
mutate_at(
vars("ERdailymeanmean_gO2/m2day", "GPPdailymeanmean_gO2/m2day", "K600dailymeanmean_m/day", "ERdailymeanmean_gO2/m3day", "GPPdailymeanmean_gO2/m3day", "K600vsERrsq", "K600vsERp"),
funs(ifelse(PARENT_ID %in% c("SSS010", "SSS023", "SSS028", "SSS038"), -9999, .))
)
your_data <- SMresults %>%
mutate_at(
vars("ERdailymeanmean_gO2/m2day", "GPPdailymeanmean_gO2/m2day", "K600dailymeanmean_m/day", "ERdailymeanmean_gO2/m3day", "GPPdailymeanmean_gO2/m3day", "K600vsERrsq", "K600vsERp"),
funs(ifelse(Parent_ID %in% c("SSS010", "SSS023", "SSS028", "SSS038"), -9999, .))
)
View(your_data)
