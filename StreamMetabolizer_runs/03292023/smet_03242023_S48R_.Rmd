---
title: "stream metabolizer SSS 021723 template"
author: "kaufman"
date: "`r Sys.Date()`"
output: html_document
#output_file: 'NA'
#params:
#  SITE: 'NA'

---

```{r setup, include=FALSE}
knitr::opts_chunk$set
#knitr::opts_chunk$set(eval = FALSE)
#SITE=params$SITE

```



## 1. Install packages and loading libraries
If this is your first time running this code or streamMetabolizer, make sure to install the appropriate packages below. uncomment them in the chunk before Knitting.

```{r install, echo = TRUE, warning=FALSE, message=FALSE}
#install.packages(remotes); library(remotes)
# remotes::install_github('appling/unitted', force = TRUE)
# remotes::install_github("USGS-R/streamMetabolizer", force = TRUE)
#install.packages("rstan", dependencies = FALSE)
#install.packages(devtools)

# If you have trouble installing rstan, try the installation codes below: 

#devtools::install_github("stan-dev/rstan", ref = "develop", subdir = "rstan/rstan", force = TRUE)
#install.packages("rstan", type = "source")

# Run the line below if you have trouble installing devtools
#devtools::install_github("stan-dev/rstan", ref = "develop", subdir = "rstan/rstan")
```

### Loading libraries
```{r libraries, echo = FALSE, warning=FALSE, message=FALSE}
#rm(list=ls(all=T))

library(streamMetabolizer)
library(dplyr)
library(unitted)
library(ggplot2)
library(tidyr)
library(devtools)
library(rstan)
library(lubridate)
# Correctly installing rstan can be problematic, see GitHub for issues
```

## 2. Setting up the data
Set working directory and read in the data and change units to match the needs of stream metabolizer.
Note: Make sure to always double check that your date time column is in the date time format and not as character

```{r data, echo = TRUE, warning=FALSE, message=FALSE}

#-------------------------------------
SITE='S48R'
#-------------------------------------

print("SITE: ")
SITE

osat<- function(temp, bp) {
  
  tstd<-log((298.15-temp) / (273.15 + temp))
  
  a0<-2.00907
  a1<-3.22014
  a2<-4.0501
  a3<-4.94457
  a4<- -0.256847
  a5<- 3.88767
  
  u<-10^(8.10765-(1750.286/(235+temp)))
  
  sato<-(exp(a0 + a1*tstd + a2*tstd^2 + a3*tstd^3 + a4*tstd^4+a5*tstd^5))*((bp-u)/(760-u))*1.42905
  sato
}

####function returns mm of Hg - not using because we have logged time-series BP
#bpcalc<- function(bpst, alt) {
#  bpst*25.4*exp((-9.80665*0.0289644*alt)/(8.31447*(273.15+15)))
#}
```

```{r data2, echo = TRUE, warning=FALSE, message=FALSE}
#bp<-bpcalc(29.9, 489/3.28)# elevation at satus (alt in meters) #---------------------------------------------------------------------

data.path = "C:/Users/kauf093/OneDrive - PNNL/Spatial Study 2022/SSS_Data_Processing/4 - SSS_MiniDOT_with_Depth" #----------------
setwd(data.path)
 dat = read.csv(paste("SSS_MiniDOT_",SITE,"_with_Depth.csv",sep=''),header=T)
 
data.path="C:/Users/kauf093/OneDrive - PNNL/Documents/GitHub/gitlab/SSS_metabolism/initial_SM_testing"
setwd(data.path)
 K600estimates=read.csv('k600.csv',header=T)
 K600estimate<-K600estimates[K600estimates$Site_ID==SITE,3]
 
 
output.path="C:/Users/kauf093/OneDrive - PNNL/Documents/GitHub/gitlab/SSS_metabolism/StreamMetabolizer_runs/03292023"
 
 #---------------------------------------------------------------------------------------------
  file.name = paste(SITE,'_032923',sep='')
  #dat = na.omit(dat)
  #dat=dat[450:33311-300,] #-------------------------------------------------------------------------------------------------------------
 # Change date time format  

#DOWNSAMPLE
samplingmins=15
dat = dat[seq(1, nrow(dat), samplingmins), ]
 
colnames(dat)[5]="Unix.Timestamp"
dat$timeUTC<-as_datetime(dat$Unix.Timestamp)
#dat$DATE_TIME = as.POSIXct(dat$DATE_TIME, format = "%Y-%m-%d %H:%M:%S", tz="Etc/GMT+8")
# Transform date time into solar time
dat$solar.time<-convert_UTC_to_solartime(dat$timeUTC, longitude= dat$Lon[1], time.type="mean solar") #----------------------------------
      # Longitude is from your field site

#trim out aug 4-10 for biofouling
dat1<-dat[dat$solar.time<"2022-08-04 00:00:00",]
dat2<-dat[dat$solar.time>"2022-08-10 00:00:00",]
dat<-rbind(dat1,dat2)

# Calculate in mg/L for much DO you would have in the water at the current saturation conditions
#      dat$DOsat_mg_per_L =(dat$Dissolved.Oxygen*100)/dat$Dosat_pct #can put calibration offset factor in here
#dat$Dissolved.Oxygen<-dat$Dissolved.Oxygen*1.018951498 #-----------------------------------------------------------------------  
  # Reducing the number of decimals after performing the saturation calculations    
#  dat$Dosat_mg_per_L = round(dat$DOsat_mg_per_L,2)

dat$light<- calc_light(dat$solar.time, latitude=dat$Lat[1], longitude=dat$Lon[1], max.PAR =2300, attach.units = F) #------------------
dat$DO.sat=osat(dat$TEMP_degreesC,dat$BP_mmhg) 

# Selecting the data types that are needed for stream metabolizer and changing header names. Running the model with K600_pooling = normal does not require discharge input


     temp = dat
     dat = cbind.data.frame(temp$solar.time,temp$DO_mg_per_L,temp$DO.sat,temp$TEMP_degreesC,temp$light,temp$DEPTH_m)
     colnames(dat) = c("solar.time","DO.obs","DO.sat","temp.water","light","depth") 
```

Check the number of cores you have in your computer. Based on the number that it prints, set the number of cores you want to dedicate to the metabolism run. It is recommended to set 2-4 cores less than you have in your computer to minimize the chances of R crashing. It is also recommended to select a pair number for your run. 

```{r core, echo = TRUE, warning=FALSE, message=FALSE}
parallel::detectCores()

```

## 3. Inspect Data
```{r inspect, echo = FALSE, warning=FALSE, message=FALSE}

dat %>% unitted::v() %>%
  mutate(DO.pctsat = 100 * (DO.obs / DO.sat)) %>%
  select(solar.time, starts_with('DO')) %>%
  gather(type, DO.value, starts_with('DO')) %>%
  mutate(units=ifelse(type == 'DO.pctsat', 'DO\n(% sat)', 'DO\n(mg/L)')) %>%
  ggplot(aes(x=solar.time, y=DO.value, color=type)) + geom_line() + 
  facet_grid(units ~ ., scale='free_y') + theme_bw() +
  scale_color_discrete('variable')

labels = c(temp.water='water temp\n(deg C)', light='PAR\n(umol m^-2 s^-1)')
dat %>% unitted::v() %>%
  select(solar.time, temp.water, light) %>%
  gather(type, value, temp.water, light) %>%
  mutate(
    type=ordered(type, levels=c('temp.water','light')),
    units=ordered(labels[type], unname(labels))) %>%
  ggplot(aes(x=solar.time, y=value, color=type)) + geom_line() +
  facet_grid(units ~ ., scale='free_y') + theme_bw() +
  scale_color_discrete('variable')

labels = c(temp.water='water temp\n(deg C)', depth='water depth')
dat %>% unitted::v() %>%
  select(solar.time, temp.water, depth) %>%
  gather(type, value, temp.water, depth) %>%
  mutate(
    type=ordered(type, levels=c('temp.water','depth')),
    units=ordered(labels[type], unname(labels))) %>%
  ggplot(aes(x=solar.time, y=value, color=type)) + geom_line() +
  facet_grid(units ~ ., scale='free_y') + theme_bw() +
  scale_color_discrete('variable')
```

## 4. Configure the model
We will select a Bayesian model. Then we will configure the specs of the model depending on the needs of our run.    
You can play around with the number of iterations (e.g., 100 burnin iterations , also called warm up) and 50 saved steps. You can adjust the number of iterations based on the convergence of the convergence of the model.  
For example, you may start with 1000 and 500 and then up the numbers to 2000 and 1000 if the model results don't seem to converge. 

If you have already adjusted the number of steps multiple times and your model fits are still not good (e.g., negative GPP values) you might have to consider changing other specs in the model. Some examples of variables that you may change are: GPP_daily_lower = 0.01,ER_daily_upper = -0.01. However, you should consult the Help for more information.

Use the command plot_distribs if you want to observe the distribution of the specs if they were changed. 


```{r modelsetup, echo = TRUE, warning=TRUE}
# Set the model

bayes_name = mm_name(type='bayes',
                     pool_K600='normal', 
                     err_obs_iid=TRUE, 
                     err_proc_iid=TRUE)
bayes_name
 # Options for pool K600 are binned, linear, none and normal. If normal is specified, discharge doesn't need to be provided

# Changing the specs
bayes_specs = specs(bayes_name, K600_daily_meanlog_meanlog=log(K600estimate), K600_daily_meanlog_sdlog=0.7, K600_daily_sdlog_sigma=0.02, burnin_steps=1000, 
                  saved_steps=1000,
                  n_cores=10)

```

## 5. Fit the model and save the results
Fitting the model might take hours or days depending on the dataset.
Some times R crashes while you are running the model so you want to make sure to save your results in each run. Create an output folder inside of the path where you are storing the data, the output path will update automatically here once you change your data path in step 2. 

Some key parameters to look at in the mm output are:  
- $daily (includes metabolism estimates) 
- $overall (includes error information) 
- $KQ_overall (included the relationship between K600-Q)

```{r fit, echo = TRUE, warning=TRUE}
mm = metab(bayes_specs, data=dat)# 
#load("~/GitHub/gitlab/SSS_metabolism/initial_SM_testing/test_15min.RData")

#Extracting the data from the model output the outputs are in a S4
#class of data and you'll need to operators to extract the daily
#time series of estimates
#get_fit(mm) %>%
  #lapply(names)

# Saving key data
#output.path = paste0(data.output.path,"/Output/")
preds = mm@fit$daily 
#str(preds)
write.csv(preds,paste0(output.path,"Results_",file.name,Sys.Date(),".csv")) 

instant = mm@fit$inst
#str(instant)
write.csv(instant,paste0(output.path,"Instant_",file.name,Sys.Date(),".csv"))

Overall = mm@fit$overall
#str(Overall)
write.csv(Overall,paste0(output.path,"Overall_",file.name,Sys.Date(),".csv"))

KQ = mm@fit$KQ_overall
#str(KQ)
write.csv(KQ,paste0(output.path,"KQ_ovearll",file.name,Sys.Date(),".csv"))


#get_data(mm)# Shows a table with all the data + DO modeled
#get_data_daily(mm) #daily fitting data. It shows values for Q for now
#get_params(mm)

```

## 6. Inspect GPP, ER and K600

### Daily predictions of modeled GPP and ER

The goal is for the predictions (lines) and observations (points) to be very similar. 

```{r inspect1, echo = FALSE, warning=FALSE, message=FALSE}
 
predict_metab(mm)# Daily predictions of GPP and ER for the model

  plot_metab_preds(mm)
  
  get_params(mm)#to inspect more of the fitted daily parameters
  predict_DO(mm) %>% head()
  plot_DO_preds(mm)
  

```

Ideally, good model results should have n_eff > 100 and Rhat < = 1.1. Below is a summary of these metrics for daily GPP, ER and K600. 


```{r inspect2, echo = FALSE, warning=FALSE, message=FALSE}
  get_fit(mm)$daily %>%
      select(ends_with('Rhat'))#Daily Rhat for GPP, ER y K600
    summary(preds$ER_Rhat)
    summary(preds$GPP_Rhat)
    summary(preds$K600_daily_Rhat)
    
    get_fit(mm)$daily %>%
      select(ends_with('n_eff'))#Daily n_eff for GPP, ER y K600
    summary(preds$ER_n_eff)
    summary(preds$GPP_n_eff)
    summary(preds$K600_daily_n_eff)
```

## 7. Inspect errors and their standard deviations

err_obs_iid_Rhat should have a value < 1.1. Additionally, in a conversation with Allison Appling she mentioned that they have seen pretty frequently err_obs_iid_sigma_Rhats much greater than 1.05, and mentioned it on the [JGR 2018 paper](https://doi.org/10.1002/2017JG004140). This is OK for that particular parameter because the values that the model continues to consider are usually quite similar in absolute magnitudes. 

### Inspect err_obs_iid_Rhat and err_obs_iid_sigma_Rhat
```{r inspect3, echo = TRUE, warning=FALSE, message=FALSE}

#err_obs_iid_Rhat
get_fit(mm)$inst %>%
  select(ends_with('Rhat'))

# err_obs_iid_sigma_Rhat. See comment from Appling
get_fit(mm)$overall %>%
  select(ends_with('Rhat'))

get_fit(mm)$inst %>%
  select(ends_with('n_eff'))
get_fit(mm)$overall %>%
  select(ends_with('n_eff'))


summary(instant$err_obs_iid_n_eff)
summary(instant$err_obs_iid_Rhat)
summary(instant$err_proc_iid_n_eff)
summary(instant$err_proc_iid_Rhat)  

summary(Overall$err_obs_iid_sigma_Rhat)# There is only one err_obs_iid_sigma_Rhat per run
summary(Overall$err_proc_iid_sigma_Rhat)
summary(Overall$err_obs_iid_sigma_n_eff)
summary(Overall$err_proc_iid_sigma_n_eff)
```

## 8. Inspect the relationships between variables

### Relationship between K600 and Q: we do not have Q in this experiment, so use depth as a proxy

The first step is to plot K600 and depth, this relationship should be linear??. To find this relationship we have to calculate the mean depth per day and then plot against K600.

```{r ins, echo = TRUE, warning=FALSE, message=FALSE}
    dat$solar.time = as.Date(dat$solar.time)
    meanDday = aggregate(dat["depth"],by = dat["solar.time"],mean)
    meanDday=meanDday[meanDday$solar.time %in% preds$date,]
    K600D = lm(preds$K600_daily_mean~meanDday$depth)

    plot(preds$K600_daily_mean,meanDday$depth,
         ylab="D_daily_mean",xlab="Daily_mean_K600")
    abline(lm(meanDday$depth~preds$K600_daily_mean))
      legend("topright", bty="n", legend=paste("R2 =", format(summary(K600D)$r.squared, digits=4)))
legend("top", bty="n", legend=paste("p = ", format(summary(K600D)$coefficients[8], digits=4)))

    summary(K600D)

```

### Relationship between K600 and ER

This relationship is key. Bob Hall says: 
My primary metric is to look for covariance between ER and K.  These have high equifinality which means that any combination of ER or K can provide an equally good fit to the model. If these covary strongly (r=0.6 or higher) then that may not be good for examining controls on variation in ER. If you find **no relationship**, that is a **very good** thing. If there is a relationship, that is if R2 is high, it means that the model can't parse reaeration from respiration. In this case, ER and K600 are working in opposite for O2, so ideally there is no relationship in the model outputs.

```{r ins plot, echo = TRUE, warning=FALSE, message=FALSE}
K600ER = lm(K600_daily_mean~ER_mean, data=preds)
   
    plot(preds$K600_daily_mean,preds$ER_mean,ylab="ER_mean",xlab="Daily_mean_K600")
    abline(lm(ER_mean~K600_daily_mean, data=preds))
    legend("topright", bty="n", legend=paste("R2 =", format(summary(K600ER)$r.squared, digits=4)))
legend("top", bty="n", legend=paste("p = ", format(summary(K600ER)$coefficients[8], digits=4)))
    
 summary(K600ER)
```

### Relationship between K600 and GPP
High GPP is needed to estimate K well, so K should get more variable as GPP decreases.

```{r ins plot2, echo = TRUE, warning=FALSE, message=FALSE}
K600GPP = lm(GPP_mean~K600_daily_mean, data=preds)

    
    plot(preds$K600_daily_mean,preds$GPP_mean,ylab="GPP_mean",xlab="Daily_mean_K600")
    abline(lm(GPP_mean~K600_daily_mean, data=preds))
    legend("topright", bty="n", legend=paste("R2 =", format(summary(K600GPP)$r.squared, digits=4)))
legend("top", bty="n", legend=paste("p = ", format(summary(K600GPP)$coefficients[8], digits=4)))
     
    summary(K600GPP)

```

### Time series daily mean K600 

```{r ins4, echo = TRUE, warning=FALSE, message=FALSE}
    preds$date = as.Date(preds$date)
  
    plot(preds$date,preds$K600_daily_mean,
         xlab="Date",ylab="Daily_mean_K600")
    
```
