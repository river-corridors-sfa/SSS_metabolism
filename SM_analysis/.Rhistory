data.folder = "N, P extracts, ions (Kuo Lab)/"
working.dir = paste0(input.dir,data.folder)
output.dir = paste0(input.dir,"VGC/")
# ===== Reading in data =====
files = list.files(working.dir,full.names = T, recursive = T)
file.names = list.files(working.dir,full.names = F, recursive = T)
data = files[!grepl("ions|Ion|SampleList",file.names)]
# ===== N and P =========
tdf = lapply(data , read_excel, col_names = FALSE, skip = 12, sheet = "Summary") # Read all the dataframes into lists
for (i in 1:length(tdf)){
colnames(tdf[[i]]) = paste0(as.character(tdf[[i]][1,]), as.character(tdf[[i]][2,]))
}
df = do.call("bind_rows", tdf) # binds only common rows across data frames
# Removing extra rows and columns that only have NA
df = df[!grepl("Depth|Inches|Start",df$DepthInches),]
df = df[,colSums(is.na(df))<nrow(df)]
df = as.data.frame(df)
# fixing column headers
columns = c("Lab","Depth_start_in","Depth_end_in","Sample_ID","NO3-N","NO3_mg_per_L_as_N","NH4-N","NH4_mg_per_L_as_N","P_Bicarb_mg_per_L")
# Pasting the column names underneath the headers to verify
df[1,] = columns
# after verification make columns official and remove the extra row
colnames(df) = columns
df = df[-1,]
#remove rows with NA value in Sample_ID
df = df %>%
filter(!is.na(Sample_ID))
# ======= Fixing sample names ========
df$Location = NA ; df$rep = NA
df$rep = str_sub(df$Sample_ID,nchar(df$Sample_ID),nchar(df$Sample_ID))
df$Location = str_sub(df$Sample_ID,nchar(df$Sample_ID)-4,nchar(df$Sample_ID)-2)
df$Sample_OG = df$Sample_ID
df$Sample_ID = str_sub(df$Sample_OG,1,10)
# Making numeric columns numeric and rounding
df[5:9] = sapply(df[5:9], as.numeric)
df = df%>% mutate_if(is.numeric,round, digits = 3)
# Making a df for n and another one for P so we can merge them into one after taking care of the reps
df.n = df[grep("Next|N_ext",df$Sample_OG),]
df.n = df.n %>% select(Lab,Depth_start_in,Depth_end_in,Sample_ID,Location,rep,"NO3-N",NO3_mg_per_L_as_N,"NH4-N",NH4_mg_per_L_as_N)
df.p = df[grep("Pext|P_ext",df$Sample_OG),]
df.p = df.p %>% select(Lab,Depth_start_in,Depth_end_in,Sample_ID,Location,rep,P_Bicarb_mg_per_L)
# ======= Creating summary file with average of the reps =====
df.n2 = df.n %>% group_by(Sample_ID,Location) %>% summarise_at(vars("NO3-N","NO3_mg_per_L_as_N","NH4-N","NH4_mg_per_L_as_N"),  funs(mean, median,standard_deviation = sd,standard_error=sd(.)/sqrt(n()))) %>% mutate_if(is.numeric,round, digits = 2)
# df.n2$`NO3-N` = round(df.n2$`NO3-N`, digits = 0)
# df.n2$`NH4-N` =round(df.n2$`NH4-N`, digits = 0)
df.p2 = df.p %>% group_by(Sample_ID,Location)%>% summarise_at(vars("P_Bicarb_mg_per_L"),funs(P_Bicarb_mg_per_L_mean = mean,P_Bicarb_mg_per_L_median= median,P_Bicarb_mg_per_L_standard_deviation = sd,P_Bicarb_mg_per_L_standard_error=sd(.)/sqrt(n()))) %>% mutate_if(is.numeric,round, digits = 2)
df.final = merge(df.n2,df.p2, by = c("Sample_ID","Location"))
df.final$Location = gsub("ext","Reference_Soil", df.final$Location)
df.final$Sample_ID = gsub("SSR1","SRR1", df.final$Sample_ID)
df.final$Sample_ID = gsub("SREI","STEI", df.final$Sample_ID)
df.final$Sample_ID = gsub("SCIB","SCBI", df.final$Sample_ID)
# ======== Export N and P =======
write.csv(df.final,paste0(output.dir,"N_and_P_formatted_",Sys.Date(),".csv"), row.names = F)
rm(list=ls());graphics.off()
library("tidyverse");library("readxl")
# ======= Setting working directories ======
input.dir = "//e0.emsl.pnl.gov/monet/Rey 1000 Soils/vgc and qian - data product/Biotic Measurements/"
data.folder = "respiration, enzymes, texture (OSU, Regen Ag)/"
working.dir = paste0(input.dir,data.folder)
output.dir = paste0(input.dir,"VGC/")
# ===== Reading in data =====
files = list.files(working.dir,full.names = T, recursive = T)
file.names = list.files(working.dir,full.names = F, recursive = T)
texture = files[grep("Texture",file.names)]
data = files[grep("Final Report",file.names)][!grepl("Texture|archive|Preliminary",files[grep("Final Report",file.names)])]
# ===== Texture =========
#Note that right now we only have one excel file with all the data. The code might need to be revised when more data sheets are added to merge all of them into one
tdf = lapply(texture , read_excel, col_names = FALSE, skip = 1, sheet = "Data") # Read all the dataframes into lists
for (i in 1:length(tdf)){
colnames(tdf[[i]]) = paste0(as.character(tdf[[i]][1,]), as.character(tdf[[i]][2,]))
}
df.texture = do.call("bind_rows", tdf) # binds only common rows across data frames
# Removing extra rows and columns that only have NA
#NEED TO ADD ONCE MORE SHEETS ARE ADDED. Example below
# df = df[!grepl("Depth|Inches|Start",df$DepthInches),]
# df = df[,colSums(is.na(df))<nrow(df)]
View(df.texture)
# Fixing column names and numeric rounding
# Fixing column headers
columns = c("Sample_ID","cal","Sand_pct","Silt_pct","Clay_pct")
# Pasting the column names underneath the headers to verify
df.texture[1,] = columns
# after verification make columns official and remove the extra row
colnames(df.texture) = columns
View(df.texture)
df.texture = df.texture[-1,]
#remove rows with NA value in Sample_ID and making numeric rows that need to be numeric and rounding
df.texture[3:5] = sapply(df.texture[3:5], as.numeric)
df.texture = df.texture %>%
filter(!is.na(Sample_ID))%>%
mutate_if(is.numeric,round, digits = 3)
View(df.texture)
df.texture = df.texture %>%
filter(!is.na(Sample_ID))%>%
mutate_if(is.numeric,round, digits = 2)
# Fixing sample names and location
df.texture$Sample_OG = NA
df.texture$Location = NA
# Formatting the location
for (i in 1:nrow(df.texture)){
if(length(grep("REF",df.texture$Sample_ID[i]))>0&is.na(df.texture$Sample_ID[i])==F){
df.texture$Location[i] = "Reference_Soil"
}else if (nchar(df.texture$Location[i])< 3|is.na(df.texture$Location[i])==T&is.na(df.texture$Sample_ID[i])==F){
df.texture$Location[i] = str_sub(df.texture$Sample_ID[i],nchar(df.texture$Sample_ID[i])-2,nchar(df.texture$Sample_ID[i]))
df.texture$Sample_OG[i] = df.texture$Sample_ID[i]
df.texture$Sample_ID[i] = str_sub(df.texture$Sample_ID[i],1,nchar(df.texture$Sample_ID[i])-4)
}
}
colnames(df.texture)
# Picking final columns in the texture dataset
df.texture2 = df.texture %>% select(Sample_ID,Location,Sand_pct,Silt_pct,Clay_pct)
View(df.texture2)
tdf = lapply(data , read_excel, col_names = FALSE, skip = 5, sheet = "Report") # Read all the dataframes into lists
tdf = lapply(data , read_excel, col_names = FALSE, skip = 5, sheet = "Report") # Read all the dataframes into lists
for (i in 1:length(tdf)){
colnames(tdf[[i]]) = paste0(as.character(tdf[[i]][1,]), as.character(tdf[[i]][2,]))
}
df = do.call("bind_rows", tdf) # binds only common rows across data frames
View(df)
tdf = lapply(data , read_excel, col_names = FALSE, skip = 13, sheet = "Report") # Read all the dataframes into lists
for (i in 1:length(tdf)){
colnames(tdf[[i]]) = paste0(as.character(tdf[[i]][1,]), as.character(tdf[[i]][2,]))
}
df = do.call("bind_rows", tdf) # binds only common rows across data frames
View(df)
colnames(df)
# Fixing column headers
columns = c("Sample_ID","Lab","B-glucosidase_activity_nmol_B-gluc_per_g_soil_per_hour","Respiration_24_h_ug_CO2-C_per_g_soil_per_day","Respiration_96_h_ug_CO2-C_per_g_soil_per_day","","Sample_ID","Lab","B-glucosidase_activity_nmol_B-gluc_per_g_soil_per_hour","Respiration_24_h_ug_CO2-C_per_g_soil_per_day","Respiration_96_h_ug_CO2-C_per_g_soil_per_day")
# Pasting the column names underneath the headers to verify
df[1,] = columns
View(df)
columns
df[1,]
df = as.data.frame(df)
# Pasting the column names underneath the headers to verify
df[1,] = columns
# after verification make columns official and remove the extra row
colnames(df) = columns
df = df[-1:2,]
df = df[-1,]
df = df[-1,]
colnames(df)
# Splitting the 2 datasets in 2
df1 = df[1:5]
df2 = df[7:10]
View(df1)
df2 = df[7:11]
View(df2)
df.resp = rbind(df1,df12)
df.resp = rbind(df1,df2)
View(df.resp)
#remove rows with NA value in Sample_ID and making numeric rows that need to be numeric and rounding
df.resp[3:5] = sapply(df.resp[3:5], as.numeric)
View(df2)
View(df1)
# Removin methods descriptions from the end of df1
df1 = df[1:20,] # NOTE change manually as new datasets come in
View(df1)
# Splitting the 2 datasets in 2
df1 = df[1:5]
View(df1)
# Removin methods descriptions from the end of df1
df1 = df1[1:20,] # NOTE change manually as new datasets come in
# Splitting the 2 datasets in 2
df1 = df[1:5]
# Removing methods descriptions from the end of df1
df1 = df1[1:20,] # NOTE change manually as new datasets come in
View(df1)
# Removing methods descriptions from the end of df1
df1 = df1[1:19,] # NOTE change manually as new datasets come in
df2 = df[7:11]
df.resp = rbind(df1,df2)
#remove rows with NA value in Sample_ID and making numeric rows that need to be numeric and rounding
df.resp[3:5] = sapply(df.resp[3:5], as.numeric)
View(df.resp)
df.resp = df.resp %>%
filter(!is.na(Sample_ID))%>%
mutate_if(is.numeric,round, digits = 2)
# Fixing sample names and location
df.resp$Sample_OG = NA
df.resp$Location = NA
# Formatting the location
for (i in 1:nrow(df.resp)){
if(length(grep("REF",df.resp$Sample_ID[i]))>0&is.na(df.resp$Sample_ID[i])==F){
df.resp$Location[i] = "Reference_Soil"
}else if (nchar(df.resp$Location[i])< 3|is.na(df.resp$Location[i])==T&is.na(df.resp$Sample_ID[i])==F){
df.resp$Location[i] = str_sub(df.resp$Sample_ID[i],nchar(df.resp$Sample_ID[i])-2,nchar(df.resp$Sample_ID[i]))
df.resp$Sample_OG[i] = df.resp$Sample_ID[i]
df.resp$Sample_ID[i] = str_sub(df.resp$Sample_ID[i],1,nchar(df.resp$Sample_ID[i])-4)
}
}
# Picking final columns in the texture dataset
df.resp2 = df.resp %>% select(Sample_ID,Location,"B-glucosidase_activity_nmol_B-gluc_per_g_soil_per_hour","Respiration_24_h_ug_CO2-C_per_g_soil_per_day","Respiration_96_h_ug_CO2-C_per_g_soil_per_day")
View(df.resp2)
rm(list=ls());graphics.off()
library("tidyverse");library("readxl")
# ======= Setting working directories ======
input.dir = "//e0.emsl.pnl.gov/monet/Rey 1000 Soils/vgc and qian - data product/Biotic Measurements/"
data.folder = "respiration, enzymes, texture (OSU, Regen Ag)/"
working.dir = paste0(input.dir,data.folder)
output.dir = paste0(input.dir,"VGC/")
# ===== Reading in data =====
files = list.files(working.dir,full.names = T, recursive = T)
file.names = list.files(working.dir,full.names = F, recursive = T)
texture = files[grep("Texture",file.names)]
data = files[grep("Final Report",file.names)][!grepl("Texture|archive|Preliminary",files[grep("Final Report",file.names)])]
# ===== Texture =========
#Note that right now we only have one excel file with all the data. The code might need to be revised when more data sheets are added to merge all of them into one
tdf = lapply(texture , read_excel, col_names = FALSE, skip = 1, sheet = "Data") # Read all the dataframes into lists
for (i in 1:length(tdf)){
colnames(tdf[[i]]) = paste0(as.character(tdf[[i]][1,]), as.character(tdf[[i]][2,]))
}
df.texture = do.call("bind_rows", tdf) # binds only common rows across data frames
# Removing extra rows and columns that only have NA
#NEED TO ADD ONCE MORE SHEETS ARE ADDED. Example below
# df = df[!grepl("Depth|Inches|Start",df$DepthInches),]
# df = df[,colSums(is.na(df))<nrow(df)]
# Fixing column names and numeric rounding
# Fixing column headers
columns = c("Sample_ID","cal","Sand_pct","Silt_pct","Clay_pct")
# Pasting the column names underneath the headers to verify
df.texture[1,] = columns
install.packages(c("bayesplot", "BiocManager", "bit", "brew", "bslib", "callr", "car", "classInt", "cli", "colourpicker", "commonmark", "confintr", "cpp11", "crayon", "data.table", "dataRetrieval", "digest", "DT", "e1071", "evaluate", "fontawesome", "gert", "ggplot2", "ggpmisc", "ggpp", "ggpubr", "ggrepel", "ggsignif", "ggspatial", "gtools", "Hmisc", "htmltools", "isoband", "jpeg", "jsonify", "jsonlite", "knitr", "ks", "lifecycle", "lme4", "lubridate", "mapproj", "maps", "maptools", "markdown", "Matrix", "matrixStats", "mclust", "minqa", "modelr", "network", "nhdplusTools", "openssl", "pbapply", "pkgbuild", "pkgload", "plotly", "plyr", "png", "polyclip", "processx", "ps", "purrr", "R.utils", "ragg", "raster", "RcppEigen", "RCurl", "readr", "rgdal", "rlang", "rmarkdown", "roxygen2", "rstatix", "s2", "sass", "segmented", "servr", "sf", "shiny", "sp", "spatstat.data", "spatstat.geom", "spatstat.linnet", "spatstat.random", "spatstat.sparse", "spatstat.utils", "spData", "stringr", "sys", "terra", "tidyselect", "tigris", "tinytex", "V8", "vctrs", "vegan", "vroom", "whisker", "wk", "xfun", "xts", "yaml", "zip"))
install.packages(c("bayesplot", "BiocManager", "bit", "brew", "bslib", "callr", "car", "classInt", "cli", "colourpicker", "commonmark", "confintr", "cpp11", "crayon", "data.table", "dataRetrieval", "digest", "DT", "e1071", "evaluate", "fontawesome", "gert", "ggplot2", "ggpmisc", "ggpp", "ggpubr", "ggrepel", "ggsignif", "ggspatial", "gtools", "Hmisc", "htmltools", "isoband", "jpeg", "jsonify", "jsonlite", "knitr", "ks", "lifecycle", "lme4", "lubridate", "mapproj", "maps", "maptools", "markdown", "Matrix", "matrixStats", "mclust", "minqa", "modelr", "network", "nhdplusTools", "openssl", "pbapply", "pkgbuild", "pkgload", "plotly", "plyr", "png", "polyclip", "processx", "ps", "purrr", "R.utils", "ragg", "raster", "RcppEigen", "RCurl", "readr", "rgdal", "rlang", "rmarkdown", "roxygen2", "rstatix", "s2", "sass", "segmented", "servr", "sf", "shiny", "sp", "spatstat.data", "spatstat.geom", "spatstat.linnet", "spatstat.random", "spatstat.sparse", "spatstat.utils", "spData", "stringr", "sys", "terra", "tidyselect", "tigris", "tinytex", "V8", "vctrs", "vegan", "vroom", "whisker", "wk", "xfun", "xts", "yaml", "zip"))
### Script to process FTICR-MS reports generated by Formularity (Tolić et al, 2017 - Anal. Chem.)
devtools::install_github("EMSL-Computing/ftmsRanalysis")
install.packages("rlang")
install.packages("rlang")
install.packages("rlang")
install.packages(c("cli", "rlang", "roxygen2"))
### Script to process FTICR-MS reports generated by Formularity (Tolić et al, 2017 - Anal. Chem.)
devtools::install_github("EMSL-Computing/ftmsRanalysis")
### Script to process FTICR-MS reports generated by Formularity (Tolić et al, 2017 - Anal. Chem.)
devtools::install_github("EMSL-Computing/ftmsRanalysis")
?fit_power_law
library(sf)
library(tidyverse)
library(terra)
library(nhdplusTools)
library(mapview)
library(dataRetrieval)
library(lubridate)
library(prism)
library(ggspatial)
library(nngeo)# Added from OG code
library(stars)# Added from OG code
# this gives you an error, but it can be ignored:
try(plyr::ldply(list.files(path="src/",
pattern="*.R",
full.names=TRUE),
source))
# Rmarkdown options
knitr::opts_chunk$set(echo = T, warning = F, comment = F, message = F)
# mapview options
mapviewOptions(basemaps.color.shuffle=FALSE,basemaps='OpenTopoMap')
sf_use_s2(FALSE)
setwd('C:/Users/gara009/OneDrive - PNNL/Documents/Geospatial_dataset/')
site_type = "xy" # OR site_type = "comid"
sites <- read_csv("data/SSS_testonly_latlong.csv")
if(site_type == "xy"){
sites <- sites %>%
dplyr::select(site, latitude, longitude) %>%
sf::st_as_sf(coords = c("longitude","latitude"), crs = 4269) # 4269 = NAD83 CRS
if(sf::st_crs(sites) != sf::st_crs(4269)){
sites <- sites %>% st_transform(., crs = 4269)
}
mapview(sites)
}
if(site_type == "xy"){
sites <- getNHDxy(df = sites)
}
if(site_type == "comid"){
sites <- getNHDcomid(df = dplyr::select(sites, site, comid))
}
site_watersheds <- getWatersheds(df = sites, make_pretty = TRUE) %>%
inner_join(., select(sf::st_drop_geometry(sites), site, comid), by = "comid")
# Extract dominant Omernik ecoregion within each site's watershed
sites <- getOmernikWs(df = sites, sf = site_watersheds)
View(site_watersheds)
#rm(list=ls());graphics.off()
# Install packages
# install.packages("cowplot"); install.packages("googleway");install.packages("ggplot2"); install.packages("ggrepel"); install.packages("ggspatial"); install.packages("libwgeom"); install.packages("sf");install.packages("rnaturalearth");install.packages("rnaturalearthdata")
#Load packages
library("ggplot2")
theme_set(theme_bw())
library("sf")
library("rnaturalearth")
library("rnaturalearthdata")
library("rgdal")
library("tidyverse")
library("ggspatial")
library(spData)
library(ggplot2)
library(ggspatial)
library(ggmap)
library(ggsn)
library(cowplot)
# Load shp file of the YRB
setwd("C:/Users/gara009/OneDrive - PNNL/Documents - Core Richland and Sequim Lab-Field Team/Data Generation and Files/VGC/Map/")
shp <- readOGR(dsn = paste0(getwd(),"/YRB_shp_files_from_KS/HUC4_Yakima_GCS_NA1983.shp"), stringsAsFactors = F)
github = "C:/Users/gara009/OneDrive - PNNL/Documents/GitHub/SSS_metabolism/SM_analysis/"
world <- ne_countries(scale = "medium", returnclass = "sf")
class(world)
# Load in the data and mapping file
data = read.csv("SM_for_RC2_mar_2023_for_maps.csv")
names(data)[1] = "Site_ID"
# Read in metadata
mapping = read.csv(paste0(github,"combined_results_updated_040623.csv"))
# Load in the data and mapping file
data = read.csv(paste0(github,"combined_results_updated_040623"))
# Load in the data and mapping file
data = read.csv(paste0(github,"combined_results_updated_040623.csv"))
View(data)
names(data)[1] = "Site_ID"
# Read in metadata
mapping = read.csv("SSS Sample Team Metadata.csv")
View(mapping)
colnames(data)
df_map <- left_join(data, mapping, by = "Site_ID") %>% dplyr::select(Site_ID,Site_Vial_ID,ERdailymeanmean_gO2.m2day,ERwaterdaily_gO2.m2day,ERseddaily_gO2.m2day,Sediment_Latitude,Sediment_Longitude) %>% st_as_sf(., coords = c("Sediment_Longitude", "Sediment_Latitude"), crs = common_crs)
point_size = 2
common_crs <- 4326
df_map <- left_join(data, mapping, by = "Site_ID") %>% dplyr::select(Site_ID,Site_Vial_ID,ERdailymeanmean_gO2.m2day,ERwaterdaily_gO2.m2day,ERseddaily_gO2.m2day,Sediment_Latitude,Sediment_Longitude) %>% st_as_sf(., coords = c("Sediment_Longitude", "Sediment_Latitude"), crs = common_crs)
point_size = 2
df_map = na.omit(df_map)
View(df_map)
ggmap(get_stamenmap(us_bbox,maptype = "terrain-background", zoom = 6))+
geom_polygon(data = shp, aes(x = long, y = lat, group = group), colour = "black", fill = NA)+
geom_sf(data = df_map,
aes_string(color = df_map$ERdailymeanmean_gO2.m2day), size = point_size * 2.5) +
coord_sf(xlim = c(-121.7, -119.15), ylim = c(45.7, 47.79), expand = FALSE)+
theme(legend.background = element_rect(fill = alpha("white", 0.0)), legend.key = element_rect(fill = "transparent"))
#create bounding box for us map
us_bbox <- c(left = -140, bottom = 25, right = -60, top = 60)
ggmap(get_stamenmap(us_bbox,maptype = "terrain-background", zoom = 6))+
geom_polygon(data = shp, aes(x = long, y = lat, group = group), colour = "black", fill = NA)+
geom_sf(data = df_map,
aes_string(color = df_map$ERdailymeanmean_gO2.m2day), size = point_size * 2.5) +
coord_sf(xlim = c(-121.7, -119.15), ylim = c(45.7, 47.79), expand = FALSE)+
theme(legend.background = element_rect(fill = alpha("white", 0.0)), legend.key = element_rect(fill = "transparent"))
shp <- readOGR(dsn = paste0(getwd(),"/YRB_shp_files_from_KS/HUC4_Yakima_GCS_NA1983.shp"), stringsAsFactors = F)
View(world)
View(shp)
ggmap(get_stamenmap(us_bbox,maptype = "terrain-background", zoom = 6))+
geom_polygon(data = shp, aes(x = long, y = lat, group = group), colour = "black", fill = NA)
ggmap(get_stamenmap(us_bbox,maptype = "terrain-background", zoom = 6))+
geom_polygon(data = shp, aes(x = long, y = lat, group = group), colour = "black", fill = NA)+
geom_sf(data = df_map,
aes_string(color = df_map$ERdailymeanmean_gO2.m2day), size = point_size * 2.5) +
coord_sf(xlim = c(-121.7, -119.15), ylim = c(45.7, 47.79), expand = FALSE)
#rm(list=ls());graphics.off()
# Install packages
# install.packages("cowplot"); install.packages("googleway");install.packages("ggplot2"); install.packages("ggrepel"); install.packages("ggspatial"); install.packages("libwgeom"); install.packages("sf");install.packages("rnaturalearth");install.packages("rnaturalearthdata")
#Load packages
library("ggplot2")
theme_set(theme_bw())
library("sf")
library("rnaturalearth")
library("rnaturalearthdata")
library("rgdal")
library("tidyverse")
library("ggspatial")
library(spData)
library(ggplot2)
library(ggspatial)
library(ggmap)
library(ggsn)
library(cowplot)
# Load shp file of the YRB
setwd("C:/Users/gara009/OneDrive - PNNL/Documents - Core Richland and Sequim Lab-Field Team/Data Generation and Files/VGC/Map/")
shp <- readOGR(dsn = paste0(getwd(),"/YRB_shp_files_from_KS/HUC4_Yakima_GCS_NA1983.shp"), stringsAsFactors = F)
github = "C:/Users/gara009/OneDrive - PNNL/Documents/GitHub/SSS_metabolism/SM_analysis/"
# read rivers
# ogrListLayers(paste0(getwd(),"/ECY_WAT_NHDWAMajor.gdb"))
# rivers <- readOGR(dsn = paste0(getwd(),"/ECY_WAT_NHDWAMajor.gdb"),layer = "ECY_WAT_NHDWAMajor_NHDStreams", stringsAsFactors = F)
# # Changing projection to map the shp file with the YRB
# rivers = spTransform(rivers, CRS("+proj=longlat +datum=NAD83 +no_defs"))
world <- ne_countries(scale = "medium", returnclass = "sf")
class(world)
# Load in the data and mapping file
data = read.csv(paste0(github,"combined_results_updated_040623.csv"))
names(data)[1] = "Site_ID"
# Extract only site name
#data = data %>% mutate(Site_Vial_ID = stringr::str_extract(Sample_ID,"SSS[0-9]{3}"))
# Read in metadata
mapping = read.csv("SSS Sample Team Metadata.csv")
common_crs <- 4326
df_map <- left_join(data, mapping, by = "Site_ID") %>% dplyr::select(Site_ID,Site_Vial_ID,ERdailymeanmean_gO2.m2day,ERwaterdaily_gO2.m2day,ERseddaily_gO2.m2day,Sediment_Latitude,Sediment_Longitude) %>% st_as_sf(., coords = c("Sediment_Longitude", "Sediment_Latitude"), crs = common_crs)
point_size = 2
df_map = na.omit(df_map)
#create bounding box for us map
us_bbox <- c(left = -140, bottom = 25, right = -60, top = 60)
ggmap(get_stamenmap(us_bbox,maptype = "terrain-background", zoom = 6))+
geom_polygon(data = shp, aes(x = long, y = lat, group = group), colour = "black", fill = NA)+
geom_sf(data = df_map,
aes_string(color = df_map$ERdailymeanmean_gO2.m2day), size = point_size * 2.5)
View(df_map)
ggmap(get_stamenmap(us_bbox,maptype = "terrain-background", zoom = 6))+
geom_polygon(data = shp, aes(x = long, y = lat, group = group), colour = "black", fill = NA)
View(mapping)
ggmap(get_stamenmap(us_bbox,maptype = "terrain-background", zoom = 6))+
geom_polygon(data = shp, aes(x = long, y = lat, group = group), colour = "black", fill = NA)+
#geom_sf(data = df_map,
#  aes_string(color = df_map$ERdailymeanmean_gO2.m2day), size = point_size * 2.5) +
coord_sf(xlim = c(-121.7, -119.15), ylim = c(45.7, 47.79), expand = FALSE)
ggmap(get_stamenmap(us_bbox,maptype = "terrain-background", zoom = 6))+
geom_polygon(data = shp, aes(x = long, y = lat, group = group), colour = "black", fill = NA)+
#geom_sf(data = df_map,
#  aes_string(color = df_map$ERdailymeanmean_gO2.m2day), size = point_size * 2.5) +
coord_sf(xlim = c(-121.7, -119.15), ylim = c(45.7, 47.79), expand = FALSE)
ggmap(get_stamenmap(us_bbox,maptype = "terrain-background", zoom = 6))+
geom_polygon(data = shp, aes(x = long, y = lat, group = group), colour = "black", fill = NA)
cropped <- st_crop(worldmap, xmin = -121.7, xmax = -119.15,
ymin = 45.7, ymax = 47.79)
ggmap(get_stamenmap(us_bbox,maptype = "terrain-background", zoom = 6))+
geom_polygon(data = shp, aes(x = long, y = lat, group = group), colour = "black", fill = NA)+
geom_sf(data = cropped)
#create bounding box for us map
wa_bbox <- c(left = -121.7, bottom = 45.7, right = -119.15, top = 47.79)
ggmap(get_stamenmap(wa_bbox,maptype = "terrain-background", zoom = 6))+
geom_polygon(data = shp, aes(x = long, y = lat, group = group), colour = "black", fill = NA)
rm(list=ls());graphics.off()
setwd("C:/Users/gara009/OneDrive - PNNL/Documents/GitHub/SSS_metabolism/SM_analysis/")
library(tidyverse)
data = read.csv("combined_results_updated_040623.csv")
data = data %>% dplyr::select(ERdailymeanmean_gO2.m2day,ERwaterdaily_gO2.m2day,ERseddaily_gO2.m2da
View(data)
data
data = data %>% dplyr::select(ERdailymeanmean_gO2.m2day,ERwaterdaily_gO2.m2day,ERseddaily_gO2.m2day)
ggplot(data, aes(x=ERdailymeanmean_gO2.m2day)) +
geom_histogram()
ggplot(data, aes(x=ERdailymeanmean_gO2.m2day)) +
geom_histogram()+
theme_bw()
ggplot(data, aes(x=ERdailymeanmean_gO2.m2day)) +
geom_histogram(fill = 'white')+
theme_bw()
ggplot(data, aes(x=ERdailymeanmean_gO2.m2day)) +
geom_histogram(fill = 'white', color = 'black')+
theme_bw()
ggsave("Hist_ERtot.png")
ggplot(data, aes(x=ERdailymeanmean_gO2.m2day)) +
geom_histogram(fill = 'white', color = 'black')+
geom_density(alpha=0.6)+
theme_bw()
ggplot(data, aes(x=ERdailymeanmean_gO2.m2day)) +
geom_histogram(aes(y=..density..), position="identity", alpha=0.5,fill = 'white', color = 'black')+
geom_density(alpha=0.6)+
theme_bw()
ggplot(data, aes(x=ERwaterdaily_gO2.m2day)) +
geom_histogram(aes(y=..density..), position="identity", alpha=0.5,fill = 'white', color = 'black')+
geom_density(alpha=0.6)+
theme_bw()
ggplot(data, aes(x=ERwaterdaily_gO2.m2day)) +
geom_histogram(aes(y=..density..), position="identity", alpha=0.5,fill = 'white', color = 'black', bins = 50)+
geom_density(alpha=0.6)+
theme_bw()
ggplot(data, aes(x=ERseddaily_gO2.m2day)) +
geom_histogram(aes(y=..density..), position="identity", alpha=0.5,fill = 'white', color = 'black', bins = 50)+
geom_density(alpha=0.6)+
theme_bw()
ggplot(data, aes(x=ERseddaily_gO2.m2day)) +
geom_histogram(aes(y=..density..), position="identity", alpha=0.5,fill = 'white', color = 'black', bins = 20)+
geom_density(alpha=0.6)+
theme_bw()
ggplot(data, aes(x=ERseddaily_gO2.m2day)) +
geom_histogram(aes(y=..density..), position="identity", alpha=0.5,fill = 'white', color = 'black', bins = 40)+
geom_density(alpha=0.6)+
theme_bw()
ggplot(data, aes(x=ERdailymeanmean_gO2.m2day)) +
geom_histogram(aes(y=..density..), position="identity", alpha=0.5,fill = 'white', color = 'black', bins = 40)+
geom_density(alpha=0.6)+
theme_bw()
rm(list=ls());graphics.off()
setwd("C:/Users/gara009/OneDrive - PNNL/Documents/GitHub/SSS_metabolism/SM_analysis/")
library(tidyverse)
data = read.csv("combined_results_updated_040623.csv")
data = data %>% dplyr::select(ERdailymeanmean_gO2.m2day,ERwaterdaily_gO2.m2day,ERseddaily_gO2.m2day)
ggplot(data, aes(x=ERdailymeanmean_gO2.m2day)) +
geom_histogram(aes(y=..density..), position="identity", alpha=0.5,fill = 'white', color = 'black', bins = 40)+
geom_density(alpha=0.6)+
theme_bw()
ggsave("Hist_ERtot.png")
ggplot(data, aes(x=ERwaterdaily_gO2.m2day)) +
geom_histogram(aes(y=..density..), position="identity", alpha=0.5,fill = 'white', color = 'black', bins = 50)+
geom_density(alpha=0.6)+
theme_bw()
ggsave("Hist_ERwater.png")
ggplot(data, aes(x=ERseddaily_gO2.m2day)) +
geom_histogram(aes(y=..density..), position="identity", alpha=0.5,fill = 'white', color = 'black', bins = 40)+
geom_density(alpha=0.6)+
theme_bw()
ggsave("Hist_ERsed.png")
rm(list=ls());graphics.off()
setwd("C:/Users/gara009/OneDrive - PNNL/Documents/GitHub/SSS_metabolism/SM_analysis/")
library(tidyverse)
data = read.csv("combined_results_updated_040623.csv")
data = data %>% dplyr::select(ERdailymeanmean_gO2.m2day,ERwaterdaily_gO2.m2day,ERseddaily_gO2.m2day)
ggplot(data, aes(x=ERdailymeanmean_gO2.m2day)) +
geom_histogram(aes(y=..density..), position="identity", alpha=0.5,fill = 'white', color = 'black', bins = 40)+
geom_density(alpha=0.6)+
theme_bw()
ggsave("Hist_ERtot.png")
ggplot(data, aes(x=ERwaterdaily_gO2.m2day)) +
geom_histogram(aes(y=..density..), position="identity", alpha=0.5,fill = 'white', color = 'black', bins = 50)+
geom_density(alpha=0.6)+
theme_bw()
ggsave("Hist_ERwater.png")
ggplot(data, aes(x=ERseddaily_gO2.m2day)) +
geom_histogram(aes(y=..density..), position="identity", alpha=0.5,fill = 'white', color = 'black', bins = 40)+
geom_density(alpha=0.6)+
theme_bw()
ggsave("Hist_ERsed.png")
