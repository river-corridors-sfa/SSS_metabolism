<<<<<<< HEAD
plot(lasso_cv)
optimal_lambda <- lasso_cv$lambda.min
#lasso
la.eq <- glmnet(Xs, ys, lambda=optimal_lambda, family='gaussian', intercept = F, alpha=1)
la.eq$beta
summary(la.eq)
la.eq$beta
hist(sqrt(sdata$PctMxFst2019Ws))
View(sdata)
cdata
library(segmented)
fit <- lm(ERsed_Square~GPP_Square, data=sdata)
segmented.fit <- segmented(fit, seg.Z = ~GPP_Square, psi=9,
control =seg.control(display = TRUE, maxit.glm=3))
summary(segmented.fit)
View(sdata)
sdata = cdata[c(yvar,xvars,'Site_ID')];#
sdata =sdata[sdata$ERsed_Square<=0,]
sdata0 <-cdata[c('GPP_Square','ERsed_Square','Site_ID')]
View(sdata0)
View(sdata0)
names(cdata)
sdata0 <-cdata[c('GPP_Square',"ERtotal_Square",'ERsed_Square','Site_ID')]
View(cdata)
View(sdata0)
##############################################################################################################
# read in data
cdata <- data_merge()
# remove positive ERsed
sdata =cdata[cdata$ERsed_Square<0,]
yvar ='ERsed_Square'
xvars0 = c("HOBO_Temp",'Mean_Depth',"Slope","Velocity" , "AridityWs","TSS","Discharge", 'NPOC',
"totdasqkm","PctFst","PctAg",'PctShrb2019Ws','D50_m',
"hz_spring","Chlorophyll_A",'streamorde','GPP_Square','TN')
########################
#log transform all variables
ldata<- sdata[c(yvar,vars0)]
vars <-names(ldata)
########################
#log transform all variables
ldata<- sdata[c(yvar,xvars0)]
vars <-names(ldata)
#log transform variables
for ( v in 1:length(vars)){
if(vars[v] %in% c("ERsed_Square")){
ldata[vars[v]] <- log10(abs(ldata[vars[v]]))
}else if (vars[v] %in% c('hz_spring')){
ldata[vars[v]] <- ldata[vars[v]]
}
else if(vars[v] %in% c("PctMxFst2019Ws",'PctCrop2019Ws',"Chlorophyll_A","GPP_Square")){
ldata[vars[v]] <- log10(ldata[,vars[v]]+1)
}else{
ldata[vars[v]] <- log10(ldata[vars[v]])
}
png(file.path(outdir,'ERsed','check_normality',paste0(vars[v],'_orig_vs_log',".png")),
width = 3, height = 4, units = 'in', res = 600)
par(mfrow=c(2,1),mgp=c(2,1,0),mar=c(3.4,3.4,1,1.5))
hist(sdata[,vars[v]],xlab=vars[v],main='')
hist(ldata[,vars[v]],xlab=paste0(vars[v],'_log'),main='')
dev.off()
# else{
#   sdata[xvars[v]] <- scale(sdata[xvars[v]], center = TRUE, scale = TRUE)
# }
}
hist(cdata$PctFst)
hist(cdata$PctAg)
yvar ='ERsed_Square'
xvars = c("HOBO_Temp",'Mean_Depth',"Slope","Velocity" ,"Discharge","TSS", 'TN','NPOC',
"totdasqkm","PctFst","PctAg",'PctShrb2019Ws',"AridityWs",
'D50_m',"hz_spring","Chlorophyll_A",'streamorde','GPP_Square') #"PctMxFst2019Ws","PctCrop2019Ws"
sdata = cdata[c(yvar,xvars)];#
sdata =sdata[sdata$ERsed_Square<=0,]
#############################################################
#log transform variables
ldata<- sdata[c(yvar,xvars)]
vars <-names(ldata)
#log transform variables
for ( v in 1:length(vars)){
if(vars[v] %in% c("ERsed_Square")){
ldata[vars[v]] <- log10(abs(ldata[vars[v]])+1)
}else if (vars[v] %in% c('hz_spring',"AridityWs","HOBO_Temp",'Velocity','streamorde',"PctFst")){
ldata[vars[v]] <- ldata[vars[v]]
}
else if(vars[v] %in% c("Chlorophyll_A","GPP_Square")){
ldata[vars[v]] <- log10(ldata[,vars[v]]+1)
}else{
ldata[vars[v]] <- log10(ldata[vars[v]])
}
}
#############################################################
#log transform variables
ldata<- sdata[c(yvar,xvars)]
vars <-names(ldata)
#############################################################
#log transform variables
ldata<- sdata[c(yvar,xvars)]
vars <-names(ldata)
#log transform variables
for ( v in 1:length(vars)){
if(vars[v] %in% c("ERsed_Square")){
ldata[vars[v]] <- log10(abs(ldata[vars[v]])+1)
}else if (vars[v] %in% c('hz_spring',"AridityWs","HOBO_Temp",'Velocity',"PctFst")){
ldata[vars[v]] <- ldata[vars[v]]
}
else if(vars[v] %in% c("Chlorophyll_A","GPP_Square")){
ldata[vars[v]] <- log10(ldata[,vars[v]]+1)
}else{
ldata[vars[v]] <- log10(ldata[vars[v]])
}
}
#############################################################
#log transform variables
ldata<- sdata[c(yvar,xvars)]
vars <-names(ldata)
#log transform variables
for ( v in 1:length(vars)){
if(vars[v] %in% c("ERsed_Square")){
ldata[vars[v]] <- log10(abs(ldata[vars[v]])+1)
}else if (vars[v] %in% c('hz_spring',"AridityWs","HOBO_Temp",'Velocity',"PctFst")){
ldata[vars[v]] <- ldata[vars[v]]
}
else if(vars[v] %in% c("Chlorophyll_A","GPP_Square")){
ldata[vars[v]] <- log10(ldata[,vars[v]]+1)
}else{
ldata[vars[v]] <- log10(ldata[vars[v]])
}
}
View(ldata)
########################
#log transform all variables
ldata<- sdata[c(yvar,xvars0)]
vars <-names(ldata)
#log transform variables
for ( v in 1:length(vars)){
if(vars[v] %in% c("ERsed_Square")){
ldata[vars[v]] <- log10(abs(ldata[vars[v]]))
}else if (vars[v] %in% c('hz_spring')){
ldata[vars[v]] <- ldata[vars[v]]
}
else if(vars[v] %in% c("PctMxFst2019Ws",'PctCrop2019Ws',"PctFst","PctAg","Chlorophyll_A","GPP_Square")){
ldata[vars[v]] <- log10(ldata[,vars[v]]+1)
}else{
ldata[vars[v]] <- log10(ldata[vars[v]])
}
png(file.path(outdir,'ERsed','check_normality',paste0(vars[v],'_orig_vs_log',".png")),
width = 3, height = 4, units = 'in', res = 600)
par(mfrow=c(2,1),mgp=c(2,1,0),mar=c(3.4,3.4,1,1.5))
hist(sdata[,vars[v]],xlab=vars[v],main='')
hist(ldata[,vars[v]],xlab=paste0(vars[v],'_log'),main='')
dev.off()
# else{
#   sdata[xvars[v]] <- scale(sdata[xvars[v]], center = TRUE, scale = TRUE)
# }
}
cdata$PctMxFst2019Ws
cdata$PctFst
View(cdata)
yvar ='ERsed_Square'
xvars = c("HOBO_Temp",'Mean_Depth',"Slope","Velocity" ,"Discharge","TSS", 'TN','NPOC',
"totdasqkm","PctFst","PctAg",'PctShrb2019Ws',"AridityWs",
'D50_m',"hz_spring","Chlorophyll_A",'streamorde','GPP_Square') #"PctMxFst2019Ws","PctCrop2019Ws"
sdata = cdata[c(yvar,xvars)];#
sdata =sdata[sdata$ERsed_Square<=0,]
#############################################################
#log transform variables
ldata<- sdata[c(yvar,xvars)]
vars <-names(ldata)
#log transform variables
for ( v in 1:length(vars)){
if(vars[v] %in% c("ERsed_Square")){
ldata[vars[v]] <- log10(abs(ldata[vars[v]])+1)
}else if (vars[v] %in% c('hz_spring',"AridityWs","HOBO_Temp",'Velocity',"PctFst")){
ldata[vars[v]] <- ldata[vars[v]]
}
else if(vars[v] %in% c("Chlorophyll_A","GPP_Square","PctAg")){
ldata[vars[v]] <- log10(ldata[,vars[v]]+1)
}else{
ldata[vars[v]] <- log10(ldata[vars[v]])
}
}
################################################################################################
#
xvars2 <- c("HOBO_Temp",'Mean_Depth',"Slope","Velocity" ,"Discharge","TSS", 'NPOC',
"totdasqkm","PctFst",'PctShrb2019Ws',"AridityWs",
'D50_m',"hz_spring","Chlorophyll_A",'streamorde','GPP_Square')
library(glmnet)
lambdas_to_try <- 10^seq(-3, 7, length.out = 100)
set.seed(100)
Xs <-as.matrix(ldata[,xvars2])
ys <-as.matrix(ldata[,yvar])
lasso_cv <- cv.glmnet(xs, ys, alpha = 1, lambda = lambdas_to_try)
plot(lasso_cv)
optimal_lambda <- lasso_cv$lambda.min
#lasso
la.eq <- glmnet(Xs, ys, lambda=optimal_lambda, family='gaussian', intercept = F, alpha=1)
la.eq$beta
hist(cdata$PctFst)
#############################################################
#log transform variables
ldata<- sdata[c(yvar,xvars)]
vars <-names(ldata)
#log transform variables
for ( v in 1:length(vars)){
if(vars[v] %in% c("ERsed_Square")){
ldata[vars[v]] <- log10(abs(ldata[vars[v]])+1)
}else if (vars[v] %in% c('hz_spring',"AridityWs","HOBO_Temp",'Velocity',"PctFst")){
ldata[vars[v]] <- ldata[vars[v]]
}
else if(vars[v] %in% c("Chlorophyll_A","GPP_Square","PctAg")){
ldata[vars[v]] <- log10(ldata[,vars[v]]+1)
}else{
ldata[vars[v]] <- log10(ldata[vars[v]])
}
}
################################################################################################
#
xvars2 <- c("HOBO_Temp",'Mean_Depth',"Slope","Velocity" ,"Discharge","TSS", 'NPOC',
"totdasqkm","PctFst",'PctShrb2019Ws',"AridityWs",
'D50_m',"hz_spring","Chlorophyll_A",'streamorde','GPP_Square')
library(glmnet)
lambdas_to_try <- 10^seq(-3, 7, length.out = 100)
set.seed(100)
Xs <-as.matrix(ldata[,xvars2])
ys <-as.matrix(ldata[,yvar])
lasso_cv <- cv.glmnet(xs, ys, alpha = 1, lambda = lambdas_to_try)
plot(lasso_cv)
optimal_lambda <- lasso_cv$lambda.min
#lasso
la.eq <- glmnet(Xs, ys, lambda=optimal_lambda, family='gaussian', intercept = F, alpha=1)
la.eq$beta
la.eq$nobs
la.eq$df
la.eq$a0
data_merge<-function(){
# read in ER data
sdata<-read.csv('./SSS_Ecosystem_Respiration_Data_Package/SSS_Water_Sediment_Total_Respiration.csv',skip=8)
#model data from kyongho's model
mdata<-read.csv('./SSS_Ecosystem_Respiration_Data_Package/SSS_ER_d50_TotalOxygenConsumed.csv',skip=4)
sdata<-merge(sdata,mdata,by= c('Parent_ID','Site_ID'))
sdata[sdata==-9999] = NA
names(sdata) <-c("Parent_ID","Site_ID","ERtotal_Square","ERtotal_Cubic","ERwc_Square",  "ERwc_Cubic",
"ERsed_Square", "ERsed_Cubic","D50_m","Total_Oxygen_Consumed")
# Slope, Discharge, Velocity from national hydrography dataset (NHD)
nhd_data<-read.csv('./SSS_Ecosystem_Respiration_Data_Package/Inputs/SSS_Slope_Discharge_Velocity.csv',skip=6)
# streamcat data
scat <-read.csv('./v2_RCSFA_Extracted_Geospatial_Data_2023-06-21.csv')
cols<-c('site',"totdasqkm","PctMxFst2019Ws","PctConif2019Ws","PctGrs2019Ws",'PctShrb2019Ws',
"PctCrop2019Ws","PctHay2019Ws","AridityWs",'streamorde')
scat <- scat[scat$site%in%sdata$Site_ID,grep(paste(cols, collapse = "|"),names(scat))]
names(scat)[1]<-'Site_ID'
#%Ag = %Crop + %Pasture/Hay
scat['PctAg']<-scat$PctCrop2019Ws+scat$PctHay2019Ws
#%Forest = %Coniferous+%Evergreen + %Mix
scat['PctFst']<-scat$PctConif2019Ws+scat$PctGrs2019Ws+scat$PctMxFst2019Ws
# # minidot temperature
# mdot <-read.csv('./SSS_Data_Package/miniDOT/Plots_and_Summary_Statistics/SSS_Water_DO_Temp_Summary.csv',skip=9)
# mdot <-mdot[,c(2:4)]; names(mdot)[3]<-"Minidot_Temp"
# HOBO temperature
hobo <-read.csv('./SSS_Data_Package/DepthHOBO/Plots_and_Summary_Statistics/SSS_Water_Press_Temp_Summary.csv',skip=7)
hobo <-hobo[,c(3:4)]; names(hobo)[2]<-"HOBO_Temp"
hobo$Site_ID[hobo$Site_ID=='S55'] ='S55N' ; hobo$Site_ID[hobo$Site_ID=='S56'] ='S56N'
# # average depth
# depth <- read.csv('./SSS_Data_Package/SSS_Water_Depth_Summary.csv',skip=8)
# depth <-depth[,c(1,5)]
## tss data
tss <-read.csv('./SSS_Data_Package/SSS_Water_TSS.csv',skip=2)
tss <- tss[grep('TSS',tss$Sample_Name),]
tss <-tss[,c(2,4)];
names(tss)<-c('Parent_ID','TSS')
tss$Parent_ID <-sapply(strsplit(as.character(tss$Parent_ID), "_"), `[`, 1)
tss$TSS <- as.numeric(tss$TSS)
# chemical data NPOC and TN
chemdata <- read.csv(file.path('./v2_CM_SSS_Data_Package','v2_CM_SSS_Water_NPOC_TN_Summary.csv'),skip=2)
chemdata <-chemdata[grep('SSS',chemdata$Sample_Name),]
chemdata <-chemdata[,c(2,4,5)];
names(chemdata)<-c('Parent_ID','NPOC','TN')
chemdata[chemdata=='-9999'] =NA
chemdata[c('NPOC','TN')] <- sapply(chemdata[c('NPOC','TN')],as.numeric)
chemdata$Parent_ID <-sapply(strsplit(as.character(chemdata$Parent_ID), "_"), `[`, 1)
#Chlorophyll_A
chla <-read.csv('./SSS_Data_Package/MantaRiver/Plots_and_Summary_Statistics/SSS_Water_Temp_SpC_Turb_pH_ChlA_Summary.csv',skip=15)
chla <-chla[,c(2,12)]; names(chla)[2]<-"Chlorophyll_A"
## GPP
gpp<-read.csv('./SSS_Ecosystem_Respiration_Data_Package/Outputs/SSS_combined_SM_results.csv')
gpp<- gpp[,c(1,5,7)]
names(gpp)[2:3] <-c('GPP_Square','Mean_Depth')
#Hyporheic exchange flux
aflux<-readRDS(file='nhd_CR_stream_annual_resp_inputs_outputs.rda')[c('COMID','logq_hz_total_m_s')]
names(aflux)<-c('COMID','hz_annual')
sflux<-readRDS(file='nhd_CR_stream_spring_resp_inputs_outputs.rda')[c('COMID','logq_hz_total_m_s')]
names(sflux)<-c('COMID','hz_spring')
wflux<-readRDS(file='nhd_CR_stream_summer_resp_inputs_outputs.rda')[c('COMID','logq_hz_total_m_s')]
names(wflux)<-c('COMID','hz_winter')
#fluxs <- Reduce(function(x, y) merge(x, y,by ='COMID',all =TRUE), list(aflux,sflux,wflux))
gspatial<- read.csv('./v2_RCSFA_Geospatial_Data_Package/v2_RCSFA_Geospatial_Site_Information.csv')[c('COMID','Site_ID')]
gdata<- gspatial[gspatial$Site_ID%in%sdata$Site_ID,]
gdata <- Reduce(function(x, y) merge(x, y,by ='COMID',all.x =TRUE), list(gdata,aflux,sflux,wflux))
gdata <- gdata[c("Site_ID","hz_annual", "hz_spring", "hz_winter")]
## merge the data
cdata <- Reduce(function(x, y) merge(x, y,by ='Parent_ID',all =TRUE), list(sdata,tss,chemdata,gpp,chla))
cdata <- Reduce(function(x, y) merge(x, y,by ='Site_ID',all.x =TRUE), list(cdata,scat,hobo,nhd_data,gdata))
return(cdata)
}
# RC2 spatial study - Multiple linear regression
# ER_sed
# X Lin April 18 2023
################################################################################################
# Read in data
################################################################################################
rm(list=ls(all=TRUE))
source('./COde/SSS_Sed_Resp_data_merging.R')
outdir<-'./MLR_Analysis_Figures'
##############################################################################################################
# read in data
cdata <- data_merge()
##############################################################################################################
# read in data
cdata <- data_merge()
# remove positive ERsed
sdata =cdata[cdata$ERsed_Square<0,]
yvar ='ERsed_Square'
# remove positive ERsed
sdata =cdata[cdata$ERsed_Square<0,]
yvar ='ERsed_Square'
xvars0 = c("HOBO_Temp",'Mean_Depth',"Slope","Velocity" , "AridityWs","TSS","Discharge", 'NPOC',
"totdasqkm","PctFst","PctAg",'PctShrb2019Ws','D50_m',
"hz_spring","Chlorophyll_A",'streamorde','GPP_Square','TN')
################################################################################################
# Stepwise Regression for ERsed
# fit <- lm(DO_Slope ~ DIC + NPOC + TN + TSS+T_mean+TOT_BASIN_AREA+StreamOrde, data = na.omit(cdata))
# step <- stepAIC(fit, direction="both")
# step$anova # display results
yvar ='ERsed_Square'
# xvars = c("HOBO_Temp",'Mean_Depth',"Slope","Velocity" , "AridityWs","TSS","Discharge", 'NPOC','TN',
#           "totdasqkm","PctMxFst2019Ws","PctCrop2019Ws",'PctShrb2019Ws','D50_m',
#           "hz_spring","Chlorophyll_A",'streamorde','GPP_Square')
## %Ag(PctAg) = %Crop + %Pasture/Hay;   #%Forest(PctFst) = %Coniferous+%Evergreen + %Mix
xvars = c("HOBO_Temp",'Mean_Depth',"Slope","Velocity" , "AridityWs","TSS","Discharge", 'NPOC','TN',
"totdasqkm","PctFst","PctAg",'PctShrb2019Ws','D50_m',
"hz_spring","Chlorophyll_A",'streamorde','GPP_Square','Ratio')
sdata = cdata[c(yvar,xvars,'Site_ID')];#
sdata =sdata[sdata$ERsed_Square<=0,]
cdata['Ratio'] <- cdata$Mean_Depth/cdata$D50_m
cdata$TN[is.na(cdata$TN)]<-min(cdata$TN,na.rm=TRUE)/2
# remove positive ERsed
sdata =cdata[cdata$ERsed_Square<=0,]
sapply(cdata, function(x) sum(is.na(x)))
################################################################################################
# Stepwise Regression for ERsed
# fit <- lm(DO_Slope ~ DIC + NPOC + TN + TSS+T_mean+TOT_BASIN_AREA+StreamOrde, data = na.omit(cdata))
# step <- stepAIC(fit, direction="both")
# step$anova # display results
yvar ='ERsed_Square'
# xvars = c("HOBO_Temp",'Mean_Depth',"Slope","Velocity" , "AridityWs","TSS","Discharge", 'NPOC','TN',
#           "totdasqkm","PctMxFst2019Ws","PctCrop2019Ws",'PctShrb2019Ws','D50_m',
#           "hz_spring","Chlorophyll_A",'streamorde','GPP_Square')
## %Ag(PctAg) = %Crop + %Pasture/Hay;   #%Forest(PctFst) = %Coniferous+%Evergreen + %Mix
xvars = c("HOBO_Temp",'Mean_Depth',"Slope","Velocity" , "AridityWs","TSS","Discharge", 'NPOC','TN',
"totdasqkm","PctFst","PctAg",'PctShrb2019Ws','D50_m',
"hz_spring","Chlorophyll_A",'streamorde','GPP_Square','Ratio')
sdata = cdata[c(yvar,xvars,'Site_ID')];#
sdata =sdata[sdata$ERsed_Square<=0,]
#############################################################
#plotting the segments
library(segmented)
fit <- lm(ERsed_Square~GPP_Square, data=sdata)
segmented.fit <- segmented(fit, seg.Z = ~GPP_Square, psi=9,
control =seg.control(display = TRUE, maxit.glm=3))
summary(segmented.fit)
par(mgp=c(2.2,1,0),mar=c(3.1,4.1,2,1.5))
#plot original data
plot(sdata$GPP_Square,sdata$ERsed_Square, pch=16, col='black',
xlab=expression(paste("GPP (g O"[2]*" m"^-2*" day"^-1*")")),ylab=expression(paste("ER"[sed]*" (g O"[2]*" m"^-2*" day"^-1*")")))
abline(v=11.8,lty=2)
#add segmented regression model
plot(segmented.fit, add=T)
View(sdata)
#sdata0 <-cdata[c('GPP_Square',"ERtotal_Square",'ERsed_Square','Site_ID')]
################################################################################################
## random forest analysis
################################################################################################
# sdata1 =sdata
#sdata1$ERsed_Square[sdata1$ERsed_Square>0] = 0
yvar ='ERsed_Square'
xvars = c("HOBO_Temp",'Mean_Depth',"Slope","Velocity" , "AridityWs","TSS","Discharge", 'NPOC',
"totdasqkm","PctMxFst2019Ws","PctCrop2019Ws",'PctShrb2019Ws','D50_m',
"hz_spring","Chlorophyll_A",'streamorde','GPP_Square','TN')
#
xvars = c("HOBO_Temp",'Mean_Depth',"Slope","Velocity" , "AridityWs","TSS","Discharge", 'NPOC',
"totdasqkm","PctFst","PctAg",'PctShrb2019Ws','D50_m',
"hz_spring","Chlorophyll_A",'streamorde','GPP_Square','TN','Ratio')
sdata = cdata[c(yvar,xvars)];#
#sdata$TN[is.na(sdata$TN)]<-min(sdata$TN,na.rm=TRUE)/2
sdata =sdata[sdata$ERsed_Square<=0,]
sdata =na.omit(sdata)
names(sdata)[-1]<-c("Temperature",'Mean_Depth',"Slope","Velocity" , "AridityWs","TSS","Discharge", 'NPOC',
"Drainage_Area","PctMxFst","PctCrop",'PctShrb','D50',
"Hflux","ChlA",'streamorde','GPP','TN')
fit <- lm(ERsed_Square~GPP_Square, data=sdata)
segmented.fit <- segmented(fit, seg.Z = ~GPP_Square, psi=9,
control =seg.control(display = TRUE, maxit.glm=3))
summary(segmented.fit)
par(mgp=c(2.2,1,0),mar=c(3.1,4.1,2,1.5))
#plot original data
plot(sdata$GPP_Square,sdata$ERsed_Square, pch=16, col='black',
xlab=expression(paste("GPP (g O"[2]*" m"^-2*" day"^-1*")")),ylab=expression(paste("ER"[sed]*" (g O"[2]*" m"^-2*" day"^-1*")")))
abline(v=11.8,lty=2)
#add segmented regression model
plot(segmented.fit, add=T)
#sdata0 <-cdata[c('GPP_Square',"ERtotal_Square",'ERsed_Square','Site_ID')]
################################################################################################
## random forest analysis
################################################################################################
# sdata1 =sdata
#sdata1$ERsed_Square[sdata1$ERsed_Square>0] = 0
yvar ='ERsed_Square'
xvars = c("HOBO_Temp",'Mean_Depth',"Slope","Velocity" , "AridityWs","TSS","Discharge", 'NPOC',
"totdasqkm","PctMxFst2019Ws","PctCrop2019Ws",'PctShrb2019Ws','D50_m',
"hz_spring","Chlorophyll_A",'streamorde','GPP_Square','TN')
#
# xvars = c("HOBO_Temp",'Mean_Depth',"Slope","Velocity" , "AridityWs","TSS","Discharge", 'NPOC',
#           "totdasqkm","PctFst","PctAg",'PctShrb2019Ws','D50_m',
#           "hz_spring","Chlorophyll_A",'streamorde','GPP_Square','TN','Ratio')
sdata = cdata[c(yvar,xvars)];#
#sdata$TN[is.na(sdata$TN)]<-min(sdata$TN,na.rm=TRUE)/2
sdata =sdata[sdata$ERsed_Square<=0,]
sdata =na.omit(sdata)
names(sdata)[-1]<-c("Temperature",'Mean_Depth',"Slope","Velocity" , "AridityWs","TSS","Discharge", 'NPOC',
"Drainage_Area","PctMxFst","PctCrop",'PctShrb','D50',
"Hflux","ChlA",'streamorde','GPP','TN')
palettes <- c(brewer.pal(9,name = 'Set1'),brewer.pal(length(xvars)-9,name = 'Set3'))
colors<-data.frame(color = palettes, xvars=names(sdata)[-1])
colors$xlabel <-names(sdata)[-1]
mtry <- tuneRF(sdata[,-1],sdata[,1], ntreeTry=1000,
stepFactor=1.5,improve=0.01, trace=TRUE, plot=TRUE)
best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]
####################################################################################
##  full data and all variables
set.seed(11)  #set.seed(42)
# rf_fit <- randomForest(ERsed_Square ~ ., ntree=100,nodesize=5,#nPerm=5, #maxnodes=6,nPerm=5,
#                        mtry=9, data=sdata, importance=TRUE, do.trace=100) #
rf_fit <- randomForest(ERsed_Square ~ ., ntree=100,nodesize=5,nPerm=5, #maxnodes=6,#nPerm=5,
mtry=9, data=sdata, importance=TRUE, do.trace=100) #
predicted <- unname(predict(rf_fit, data=sdata))
R2 <- 1 - (sum((sdata$ERsed_Square-predicted)^2)/sum((sdata$ERsed_Square-mean(sdata$ERsed_Square))^2))
R2
plot(sdata$ERsed_Square,predicted)
par(mgp=c(2,0.5,0),mar=c(6.8,3.1,2.1,0.5))
#rimp <- importance(rf_fit)
#vimp <- setNames(as.data.frame(rf_fit$importance)$IncNodePurity, row.names(as.data.frame(rf_fit$importance)))
vimp<-data.frame(vimp=as.data.frame(rf_fit$importance)$IncNodePurity,
xvars=row.names(as.data.frame(rf_fit$importance)))
vimp<- Reduce(function(x, y) merge(x, y,by ='xvars',all.x =TRUE), list(vimp,colors))
#vimp<- sort(vimp,decreasing = TRUE)
vimp<-vimp[order(vimp$vimp,decreasing = TRUE),]
barplot(vimp$vimp/sum(vimp$vimp),col =vimp$color , names.arg=vimp$xlabel,
horiz = FALSE,las=3,cex.lab=1.3, cex.axis=1.1, ylim=c(0,0.2),
cex.main=2,cex.names=1.1,ylab="Relative Importance")#,main=paste0("RF Feature Importance (PoreWater)"))
dev.off()
par(mgp=c(2,0.5,0),mar=c(6.8,3.1,2.1,0.5))
#rimp <- importance(rf_fit)
#vimp <- setNames(as.data.frame(rf_fit$importance)$IncNodePurity, row.names(as.data.frame(rf_fit$importance)))
vimp<-data.frame(vimp=as.data.frame(rf_fit$importance)$IncNodePurity,
xvars=row.names(as.data.frame(rf_fit$importance)))
vimp<- Reduce(function(x, y) merge(x, y,by ='xvars',all.x =TRUE), list(vimp,colors))
#vimp<- sort(vimp,decreasing = TRUE)
vimp<-vimp[order(vimp$vimp,decreasing = TRUE),]
barplot(vimp$vimp/sum(vimp$vimp),col =vimp$color , names.arg=vimp$xlabel,
horiz = FALSE,las=3,cex.lab=1.3, cex.axis=1.1, ylim=c(0,0.2),
cex.main=2,cex.names=1.1,ylab="Relative Importance")#,main=paste0("RF Feature Importance (PoreWater)"))
##############################################################################################################
# read in data
cdata <- data_merge()
# add a coloumn 'ratio'
cdata['Ratio'] <- cdata$Mean_Depth/cdata$D50_m
cdata$TN[is.na(cdata$TN)]<-min(cdata$TN,na.rm=TRUE)/2
# remove positive ERsed
sdata =cdata[cdata$ERsed_Square<=0,]
sapply(cdata, function(x) sum(is.na(x)))
View(cdata)
View(sdata)
################################################################################################
# Stepwise Regression for ERsed
# fit <- lm(DO_Slope ~ DIC + NPOC + TN + TSS+T_mean+TOT_BASIN_AREA+StreamOrde, data = na.omit(cdata))
# step <- stepAIC(fit, direction="both")
# step$anova # display results
yvar ='ERsed_Square'
# xvars = c("HOBO_Temp",'Mean_Depth',"Slope","Velocity" , "AridityWs","TSS","Discharge", 'NPOC','TN',
#           "totdasqkm","PctMxFst2019Ws","PctCrop2019Ws",'PctShrb2019Ws','D50_m',
#           "hz_spring","Chlorophyll_A",'streamorde','GPP_Square')
## %Ag(PctAg) = %Crop + %Pasture/Hay;   #%Forest(PctFst) = %Coniferous+%Evergreen + %Mix
xvars = c("HOBO_Temp",'Mean_Depth',"Slope","Velocity" , "AridityWs","TSS","Discharge", 'NPOC','TN',
"totdasqkm","PctFst","PctAg",'PctShrb2019Ws','D50_m',
"hz_spring","Chlorophyll_A",'streamorde','GPP_Square','Ratio')
sdata = cdata[c(yvar,xvars,'Site_ID')];#
sdata =sdata[sdata$ERsed_Square<=0,]
library(segmented)
fit <- lm(ERsed_Square~GPP_Square, data=sdata)
segmented.fit <- segmented(fit, seg.Z = ~GPP_Square, psi=9,
control =seg.control(display = TRUE, maxit.glm=3))
summary(segmented.fit)
png(file.path(outdir,'ERsed',paste0('segmented_regression_GPP',".png")),
width = 5, height = 4, units = 'in', res = 600)
par(mgp=c(2.2,1,0),mar=c(3.1,4.1,2,1.5))
#plot original data
plot(sdata$GPP_Square,sdata$ERsed_Square, pch=16, col='black',
xlab=expression(paste("GPP (g O"[2]*" m"^-2*" day"^-1*")")),ylab=expression(paste("ER"[sed]*" (g O"[2]*" m"^-2*" day"^-1*")")))
abline(v=11.8,lty=2)
#add segmented regression model
plot(segmented.fit, add=T)
dev.off()
hist(sdata$ERsed_Square)
=======
boye_dir
dir <- 'C:/Users/forb086/OneDrive - PNNL/Data Generation and Files/RC4/Boye_Files/WROL/'
files <- list.files(boye_dir, 'Check_for_Duplicates', full.names = T)
boye_dir <- paste0(dir, RC, '/Boye_Files/', study_code, '/')
files <- list.files(boye_dir, 'Check_for_Duplicates', full.names = T)
boye_dir
dir <- 'C:/Users/forb086/OneDrive - PNNL/Data Generation and Files/'
boye_dir <- paste0(dir, RC, '/Boye_Files/', study_code, '/')
files <- list.files(boye_dir, 'Check_for_Duplicates', full.names = T)
boye_dir
hub <- read_excel(hub_dir)
colnames_lookup <- read_csv(colnames_lookup_dir, skip = 1)
for (file in files) {
data <- read_csv(file)
if('TRUE' %in% data$duplicate){
cat(
red$bold(
'Wait! You have duplicates in your data.\n',
'You will need to remove your duplicates \n',
'before you are able to proceed\n'
)
)
} else{
data <- data %>%
select(-Date_of_Run, -Method_Notes, -duplicate)
data_type <- unlist(str_split(basename(file), '_'))[2]
data_columns <- data %>%
select(-Sample_ID, -Methods_Deviation)%>%
colnames()
# ========================= get typical codes ==============================
filter_hub <- hub %>%
filter(RC == RC,
Study_Code == study_code,
Data_Package_Unit == material)
typical_code_number <-  filter_hub %>%
select(contains(data_type)) %>%
pull(1)
# ========================= build header rows ==============================
boye_file_headers <- tibble(
'Field_Name' = c('Unit', 'Unit_Basis', 'MethodID_Analysis', 'MethodID_Inspection',
'MethodID_Storage', 'MethodID_Preservation', 'MethodID_Preparation',
'MethodID_DataProcessing', 'Analysis_DetectionLimit',
'Analysis_Precision', 'Data_Status'),
'Sample_Name' = c('N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A',
'-9999', '-9999', 'N/A'),
'Material' = c('N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A',
'-9999', '-9999', 'N/A')
)
typical_codes <- read_excel(typical_codes_dir, sheet = paste0(data_type,"_Typical"))
for (column in data_columns) {
order <- c('MethodID_Analysis', 'MethodID_Inspection',
'MethodID_Storage', 'MethodID_Preservation',
'MethodID_Preparation', 'MethodID_DataProcessing')
column_typical_codes <- typical_codes %>%
filter(str_detect(Method_ID, typical_code_number)) %>%
slice(match(order, Method_Type))%>%
select(Method_ID)%>%
pull(n = 1)
unit <- colnames_lookup %>%
filter(col.in == column) %>%
select(Unit)%>%
pull(n=1)
unit_basis <- colnames_lookup %>%
filter(col.in == column) %>%
select(Unit_Basis)%>%
pull(n=1)
data_status <- colnames_lookup %>%
filter(col.in == column) %>%
select(Data_Status)%>%
pull(n=1)
boye_file_headers <- boye_file_headers %>%
add_column(
!!column := c(unit, unit_basis, column_typical_codes, '-9999', '-9999', data_status))
}
boye_file_headers <- boye_file_headers %>%
add_column('Methods_Deviation' = 'N/A')
#=============================== get LOD ==================================
if (data_type == "TSS"){
LOD <- read_xlsx(tss_LOD_file)
LOD_dates_file_info <-file.info(list.files(path = boye_dir,pattern = "TSS_LOD", full.names = T)) %>%
rownames_to_column('File_Name') %>%
tibble()
# get most recent LOD file
LOD_dates_file <- LOD_dates_file_info %>%
arrange(mtime) %>%
tail(1)%>%
select('File_Name')%>%
pull(1)
LOD_dates <- read_csv(LOD_dates_file)
LOD_min <- LOD %>%
filter(Date_start_YYYYMMDD <= LOD_dates$Date_start[1] & Date_end_YYYYMMDD >= LOD_dates$Date_start[1]) %>%
select(LOD_TSS_mg_per_L)%>%
pull(1)%>%
round(2)
LOD_max <- LOD %>%
filter(Date_start_YYYYMMDD <= LOD_dates$Date_end[1] & Date_end_YYYYMMDD >= LOD_dates$Date_end[1]) %>%
select(LOD_TSS_mg_per_L)%>%
pull(1)%>%
round(2)
if(LOD_min == LOD_max){
LOD_final = LOD_max
} else{
LOD_final = paste0(LOD_min[1],"-",LOD_max[1])
}
boye_file_headers <- boye_file_headers %>%
assign_in(list(column, 9), LOD_final)
} else {
if(data_type == 'Ions'){
LOD <- read_csv(ion_LOD_file)
LOD_dates_file_info <-file.info(list.files(path = boye_dir,pattern = "LOD", full.names = T)) %>%
rownames_to_column('File_Name') %>%
tibble()
# get most recent LOD file
LOD_dates_file <- LOD_dates_file_info %>%
arrange(mtime) %>%
tail(1)%>%
select('File_Name')%>%
pull(1)
LOD_dates <- read_csv(LOD_dates_file)
for (ion_column in data_columns) {
analyte <- unlist(str_split(ion_column, '_'))[1]
LOD_min <- LOD %>%
filter(Analyte == analyte,
Date_start_YYYYMMDD <= LOD_dates$Date_start[1] & Date_end_YYYYMMDD >= LOD_dates$Date_start[1]) %>%
select(LOD_ppm)%>%
pull(1)%>%
round(2)
LOD_max <- LOD %>%
filter(Analyte == analyte,
Date_start_YYYYMMDD <= LOD_dates$Date_end[1] & Date_end_YYYYMMDD >= LOD_dates$Date_end[1]) %>%
select(LOD_ppm)%>%
pull(1)%>%
round(2)
if(LOD_min == LOD_max){
LOD_final = LOD_max
} else{
LOD_final = paste0(LOD_min[1],"-",LOD_max[1])
}
boye_file_headers <- boye_file_headers %>%
assign_in(list(ion_column, 9), LOD_final)
}
} else {
LOD <- read_excel(LOD_file_dir)
LOD_dates_file_info <-file.info(list.files(path = boye_dir,pattern = "LOD", full.names = T)) %>%
rownames_to_column('File_Name') %>%
tibble()
# get most recent LOD file
LOD_dates_file <- LOD_dates_file_info %>%
arrange(mtime) %>%
tail(1)%>%
select('File_Name')%>%
pull(1)
LOD_dates <- read_csv(LOD_dates_file)
for (data_column in data_columns) {
analyte <- unlist(str_split(data_column, '_'))[1]
LOD_min <- LOD %>%
select(contains(analyte) & contains('LOD'), Date_start_YYYYMMDD, Date_end_YYYYMMDD) %>%
filter(Date_start_YYYYMMDD <= LOD_dates$Date_start[1] & Date_end_YYYYMMDD >= LOD_dates$Date_start[1]) %>%
pull(1)%>%
round(2)
LOD_max <- LOD %>%
select(contains(analyte) & contains('LOD'), Date_start_YYYYMMDD, Date_end_YYYYMMDD) %>%
filter(Date_start_YYYYMMDD <= LOD_dates$Date_end[1] & Date_end_YYYYMMDD >= LOD_dates$Date_end[1]) %>%
pull(1)%>%
round(2)
if(LOD_min == LOD_max){
LOD_final = LOD_max
} else{
LOD_final = paste0(LOD_min[1],"-",LOD_max[1])
}
boye_file_headers <- boye_file_headers %>%
assign_in(list(data_column, 9), LOD_final)
}
}
}
# ======================== fix column headers ==============================
for (colname in data_columns) {
colnames_lookup_filter <- colnames_lookup %>%
dplyr::filter(col.in == colname)
new_name <- colnames_lookup_filter$col.out
data <- data %>%
dplyr::rename(!!new_name := all_of(colname))
boye_file_headers <- boye_file_headers %>%
dplyr::rename(!!new_name := all_of(colname))
}
# ========================== finish formatting =============================
if (material == 'Water'){
data <- data %>%
add_column(Material = 'Liquid>aqueous', .after = 'Sample_ID')%>%
add_column(Field_Name = 'N/A', .before = 'Sample_ID')
} else {
data <- data %>%
add_column(Material = material, .after = 'Sample_ID')%>%
add_column(Field_Name = 'N/A', .before = 'Sample_ID')
}
data <- data %>%
dplyr::mutate(across(everything(), as.character))%>%
dplyr::mutate(across(contains('X'), replace_na, replace = '-9999'))%>%
dplyr::mutate(across(!contains('X'), replace_na, replace = 'N/A'))%>%
arrange(Sample_ID)
data$Field_Name[1] <- '#Start_Data'
colnames(data) <- gsub('X', '', colnames(data))
colnames(boye_file_headers) <- gsub('X', '', colnames(boye_file_headers))
data[nrow(data)+1,1] = "#End_Data"
columns <- length(data)-1
header_rows <- length(boye_file_headers$Field_Name) + 1
top <- tibble('one' = as.character(),
'two' = as.numeric()) %>%
add_row(one = '#Columns',
two = columns) %>%
add_row(one = '#Header_Rows',
two = header_rows)
# =================================== Write File ===============================
out_name <- glue('{boye_dir}{study_code}_{data_type}_Boye_{Sys.Date()}.csv' )
write_csv(top, out_name, col_names = F)
write_csv(boye_file_headers, out_name, append = T, col_names = T)
write_csv(data, out_name, append = T, na = '')
}
}
source("C:/Users/forb086/OneDrive - PNNL/Data Generation and Files/R code/Data_Package_Code/Boye_Code/02_Boye_Formatting.R", echo=TRUE)
source("C:/Users/forb086/OneDrive - PNNL/Data Generation and Files/R code/Data_Package_Code/Boye_Code/03_Methods_Codes.R", echo=TRUE)
library(tidyverse)
library(janitor)
rm(list=ls(all=T))
dir <- 'C:/Users/forb086/OneDrive - PNNL/Data Generation and Files/'
RC <- 'RC4'
study_code <- 'WROL'
material <- 'Water'
boye_dir <- paste0(dir, RC, '/Boye_Files/', study_code, '/')
analyte_files <- list.files(boye_dir, 'Boye', full.names = T)
analyte_files <- analyte_files[!grepl('ReadyForBoye',analyte_files)]
qaqc_files <- list.files(boye_dir, 'CombinedQAQC', full.names = T)
combine <- read_csv(analyte_files[1], skip = 2)%>%
filter(!Sample_Name %in% c('N/A', '-9999')) %>%
select("Field_Name", "Sample_Name", "Material") %>%
mutate(Field_Name = NA,
Sample_Name = str_remove(Sample_Name, '-1[:alpha:]'),
Sample_Name = str_remove(Sample_Name, '-2[:alpha:]'),
Sample_Name = str_remove(Sample_Name, '-3[:alpha:]'),
Sample_Name = str_remove(Sample_Name, '-4[:alpha:]'),
Sample_Name = str_remove(Sample_Name, '-5[:alpha:]'),
Sample_Name = str_remove(Sample_Name, '-6[:alpha:]'),
Sample_Name = str_remove(Sample_Name, '-1'),
Sample_Name = str_remove(Sample_Name, '-2'),
Sample_Name = str_remove(Sample_Name, '-3'),
Sample_Name = str_remove(Sample_Name, '_DIC'),
Sample_Name = str_remove(Sample_Name, '_ION'),
Sample_Name = str_remove(Sample_Name, '_OCN'),
Sample_Name = str_remove(Sample_Name, '_TN'),
Sample_Name = str_remove(Sample_Name, '_ICR'),
Sample_Name = str_remove(Sample_Name, '_TSS'),
Sample_Name = str_remove(Sample_Name, '_SFE'),
Sample_Name = str_remove(Sample_Name, '_INC'))%>%
add_column(Mean_Missing_Reps = NA) %>%
distinct()
source("C:/Users/forb086/OneDrive - PNNL/Data Generation and Files/R code/Data_Package_Code/Boye_Code/04_Boye_Summary_File.R", echo=TRUE)
library(tidyverse)
library(janitor)
rm(list=ls(all=T))
# ================================= User inputs ================================
dir <- 'C:/Users/forb086/OneDrive - PNNL/Data Generation and Files/'
RC <- 'RC4'
study_code <- 'WROL'
material <- 'Water'
# ====================================== Build dir name ========================
boye_dir <- paste0(dir, RC, '/Boye_Files/', study_code, '/')
# ================================= Wrangle data and summarize =================
analyte_files <- list.files(boye_dir, 'Boye', full.names = T)
analyte_files <- analyte_files[!grepl('ReadyForBoye',analyte_files)]
qaqc_files <- list.files(boye_dir, 'CombinedQAQC', full.names = T)
# ====================== create combined data frames ===========================
combine <- read_csv(analyte_files[1], skip = 2)%>%
filter(!Sample_Name %in% c('N/A', '-9999')) %>%
select("Field_Name", "Sample_Name", "Material") %>%
mutate(Field_Name = NA,
Sample_Name = str_remove(Sample_Name, '-1[:alpha:]'),
Sample_Name = str_remove(Sample_Name, '-2[:alpha:]'),
Sample_Name = str_remove(Sample_Name, '-3[:alpha:]'),
Sample_Name = str_remove(Sample_Name, '-4[:alpha:]'),
Sample_Name = str_remove(Sample_Name, '-5[:alpha:]'),
Sample_Name = str_remove(Sample_Name, '-6[:alpha:]'),
Sample_Name = str_remove(Sample_Name, '-1'),
Sample_Name = str_remove(Sample_Name, '-2'),
Sample_Name = str_remove(Sample_Name, '-3'),
Sample_Name = str_remove(Sample_Name, '_DIC'),
Sample_Name = str_remove(Sample_Name, '_ION'),
Sample_Name = str_remove(Sample_Name, '_OCN'),
Sample_Name = str_remove(Sample_Name, '_TN'),
Sample_Name = str_remove(Sample_Name, '_ICR'),
Sample_Name = str_remove(Sample_Name, '_TSS'),
Sample_Name = str_remove(Sample_Name, '_SFE'),
Sample_Name = str_remove(Sample_Name, '_INC'))%>%
add_column(Mean_Missing_Reps = NA) %>%
distinct()
combine_headers <- read_csv(analyte_files[1], n_max = 11, skip = 2) %>%
select("Field_Name", "Sample_Name", "Material")
# ======================================= DIC ==================================
NPOC_TN_file <- analyte_files[grepl("NPOC_TN", analyte_files)]
length(NPOC_TN_file) > 0
NPOC_file <- analyte_files[grepl("NPOC_Boye", analyte_files)]
combine <- combine %>%
full_join(NPOC_TN_summary, by = c("Field_Name", "Sample_Name", "Material")) %>%
arrange(Sample_Name) %>%
unite(Mean_Missing_Reps, Mean_Missing_Reps.x, Mean_Missing_Reps.y, remove = T, na.rm = T)%>%
mutate(Mean_Missing_Reps = ifelse(str_detect(Mean_Missing_Reps, 'TRUE'), TRUE, FALSE))%>%
filter(!is.na(Sample_Name))
length(NPOC_file) > 0
NPOC_boye_headers <- read_csv(NPOC_file, n_max = 11, skip = 2)%>%
select(-'Methods_Deviation')
NPOC_NPOC_qaqc <- qaqc_files[grepl("NPOC", qaqc_files)] %>%
read_csv() %>%
clean_names(replace = c('outlier' = 'Outlier'), case = 'none') %>%
filter(NPOC_Outlier == T)
qaqc_files[grepl("NPOC", qaqc_files)]
View(NPOC_NPOC_qaqc)
NPOC_data <- read_csv(NPOC_file, skip = 2, na = '-9999') %>%
filter(!Sample_Name %in% c('N/A', '-9999', NA),
Field_Name != '#End_Data') %>%
mutate(Field_Name = 'N/A',
`00681_NPOC_mg_per_L_as_C` = ifelse(Sample_Name %in% NPOC_NPOC_qaqc$Sample_ID, NA, as.numeric(`00681_NPOC_mg_per_L_as_C`)),
Sample_Name = str_remove(Sample_Name, '-1'),
Sample_Name = str_remove(Sample_Name, '-2'),
Sample_Name = str_remove(Sample_Name, '-3'),
Sample_Name = str_remove(Sample_Name, '_OCN'),
Sample_Name = str_remove(Sample_Name, '_ICR')
)
View(NPOC_data)
source("C:/Users/forb086/OneDrive - PNNL/Data Generation and Files/R code/Data_Package_Code/Boye_Code/04_Boye_Summary_File.R", echo=TRUE)
dir <- 'Z:/00_Cross-SFA_ESSDIVE-Data-Package-Upload/01_Study-Data-Package-Folders/WHONDRS_WROL2019_Data_Package_v3/v3_WHONDRS_WROL2019_Data_Package/FTICR/FTICR_Data'
list <- list.files(dir, '.xml', recursive = T)
tibble <- tibble(File = list)%>%
mutate(File = str_remove(File, '-.+'))%>%
distinct()
library(tidyverse)
# ================================= User inputs ================================
dir <- 'Z:/00_Cross-SFA_ESSDIVE-Data-Package-Upload/01_Study-Data-Package-Folders/WHONDRS_WROL2019_Data_Package_v3/v3_WHONDRS_WROL2019_Data_Package/FTICR/FTICR_Data'
# ================================ make tibble =================================
list <- list.files(dir, '.xml', recursive = T)
tibble <- tibble(File = list)%>%
mutate(File = str_remove(File, '-.+'))%>%
distinct()
View(tibble)
tibble <- tibble(File = list)%>%
# mutate(File = str_remove(File, '-.+'))%>%
distinct()
write_csv(tibble, 'C:/Users/forb086/OneDrive - PNNL/Desktop/WROL_TEMP.csv')
library(tidyverse) #keep it tidy
library(raster) # work with rasters, NOTE: masks dplyr::select
library(janitor) # clean_names()
library(ggthemes) # theme_map()
library(ggsflabel) # add labels to sf objects
library(ggnewscale) # set multiple color scales
library(ggspatial) # add north arrow and scale bar
library(nhdplusTools) # get watershed boundary/flowlines
library(elevatr) # pull elevation maps
library(sf) # tidy spatial
library(spData)
library(cowplot)
library(rstudioapi)
library(viridis)
rm(list=ls(all=T))
# Setting wd to parent folder
current_path <- rstudioapi::getActiveDocumentContext()$path
setwd(dirname(current_path))
setwd("./..")
# ================================= User inputs ================================
metadata_file <- './v2_RCSFA_Geospatial_Data_Package/v2_RCSFA_Geospatial_Site_Information.csv'
data_file <- './SSS_Ecosystem_Respiration_Data_Package/SSS_Water_Sediment_Total_Respiration.csv'
shp_dir <- './Maps/YakimaRiverBasin_Boundary'
ER <- './SSS_Ecosystem_Respiration_Data_Package/SSS_Water_Sediment_Total_Respiration.csv'
modelled_ER <- './SSS_Ecosystem_Respiration_Data_Package/SSS_ER_d50_TotalOxygenConsumed.csv'
common_crs = 4326
# ============================== read in and merge =============================
metadata <- read_csv(metadata_file) %>%
dplyr::select(Site_ID, Latitude, Longitude)
data <- read_csv(data_file, skip = 8)
merge <- data %>%
left_join(metadata)
# ============================ read in YRB shp file ============================
YRB_shp <- list.files(shp_dir, 'shp', full.names = T)
YRB_boundary <- read_sf(YRB_shp) %>%
st_transform(common_crs)
# ============================ convert to sf object ============================
sites <- st_as_sf(merge, coords = c('Longitude','Latitude'), crs = common_crs)
# ======================== pull NHD data and elevation =========================
YRB_flowlines <- get_nhdplus(AOI = YRB_boundary$geometry, streamorder = 3)
elevation_raw <- get_elev_raster(YRB_boundary$geometry, z = 10)
elevation_crop <- mask(elevation_raw, YRB_boundary)
elevation <- as.data.frame(elevation_crop, xy = T) %>%
as_tibble() %>%
rename("long" = x,
"lat" = y,
"elevation" = 3) %>% #column index > name (changing resolution changes colname)
filter(!is.na(elevation))
# ============================= create map of sites ===========================
data("us_states", package = "spData")
us_states_4326 = st_transform(us_states, crs = 4326)
wa <- us_states_4326 %>% filter(NAME == "Washington")
insert <- ggplot() +
geom_sf(data = us_states_4326, fill = "white") +
geom_sf(data = wa, fill = "black",colour = "black")+
geom_sf(data = YRB_boundary, colour = "red", fill = 'red') +
labs(x = "", y = "")+
theme_map()
merge_ER <- read_csv(ER, skip = 8, na = '-9999') %>%
full_join(read_csv(modelled_ER, skip = 4, na = '-9999')) %>%
left_join(metadata)
ER_sf <- merge_ER %>%
filter(Total_Ecosystem_Respiration_Square <= 0) %>%
st_as_sf(coords = c('Longitude','Latitude'), crs = common_crs)
ER_sf <- merge_ER %>%
# filter(Total_Ecosystem_Respiration_Square <= 0) %>%
st_as_sf(coords = c('Longitude','Latitude'), crs = common_crs)
ER_tot_obs_map <- ggplot()+
geom_sf(data = YRB_boundary)+
geom_raster(data = elevation, aes(long, lat, fill = elevation), show.legend = F, alpha = 0.4)+
scale_fill_gradient(low = 'white', high = 'black')+
geom_sf(data = YRB_flowlines, color = "royalblue", alpha = 0.8)+
new_scale_fill()+
geom_sf(data = ER_sf, aes(color = Total_Ecosystem_Respiration_Square, size = Total_Ecosystem_Respiration_Square), show.legend = T) +
scale_fill_viridis(option = 'B', begin = 0.3)+
scale_color_viridis(option = 'B', begin = 0.3)+
scale_size(range = c(3, 8), trans = 'reverse')+
theme_map() +
labs(x = "", y = "", color = "Total Ecosystem\nRespiration\n(g O2 m2 day-1)") +
ggspatial::annotation_scale(
location = "br",
pad_x = unit(0.5, "in"),
bar_cols = c("black", "white")) +
ggspatial::annotation_north_arrow(
location = "tr", which_north = "true",
pad_x = unit(2, "in"),
# pad_y = unit(0.5, "in"),
style = ggspatial::north_arrow_nautical(
fill = c("black", "white"),
line_col = "grey20"))
ggsave('./Maps/SSS_ER_Total_Observed_Map.pdf',
ER_tot_obs_map,
width = 10,
height = 5
)
colnames(merge_ER)
merge_ER <- read_csv(ER, skip = 8, na = '-9999') %>%
full_join(read_csv(modelled_ER, skip = 4, na = '-9999')) %>%
left_join(metadata) %>%
arrange(Total_Ecosystem_Respiration_Square)
ER_sf <- merge_ER %>%
# filter(Total_Ecosystem_Respiration_Square <= 0) %>%
st_as_sf(coords = c('Longitude','Latitude'), crs = common_crs)
ER_tot_obs_map <- ggplot()+
geom_sf(data = YRB_boundary)+
geom_raster(data = elevation, aes(long, lat, fill = elevation), show.legend = F, alpha = 0.4)+
scale_fill_gradient(low = 'white', high = 'black')+
geom_sf(data = YRB_flowlines, color = "royalblue", alpha = 0.8)+
new_scale_fill()+
geom_sf(data = ER_sf, aes(color = Total_Ecosystem_Respiration_Square, size = Total_Ecosystem_Respiration_Square), show.legend = T) +
scale_fill_viridis(option = 'B', begin = 0.3)+
scale_color_viridis(option = 'B', begin = 0.3)+
scale_size(range = c(3, 8), trans = 'reverse')+
theme_map() +
labs(x = "", y = "", color = "Total Ecosystem\nRespiration\n(g O2 m2 day-1)") +
ggspatial::annotation_scale(
location = "br",
pad_x = unit(0.5, "in"),
bar_cols = c("black", "white")) +
ggspatial::annotation_north_arrow(
location = "tr", which_north = "true",
pad_x = unit(2, "in"),
# pad_y = unit(0.5, "in"),
style = ggspatial::north_arrow_nautical(
fill = c("black", "white"),
line_col = "grey20"))
ggsave('./Maps/SSS_ER_Total_Observed_Map.pdf',
ER_tot_obs_map,
width = 10,
height = 5
)
merge_ER <- read_csv(ER, skip = 8, na = '-9999') %>%
full_join(read_csv(modelled_ER, skip = 4, na = '-9999')) %>%
left_join(metadata) %>%
arrange(Total_Oxygen_Consumed_g_per_m2_per_day)
ER_sf <- merge_ER %>%
# filter(Total_Ecosystem_Respiration_Square <= 0) %>%
st_as_sf(coords = c('Longitude','Latitude'), crs = common_crs)
ER_tot_pred_map <- ggplot()+
geom_sf(data = YRB_boundary)+
geom_raster(data = elevation, aes(long, lat, fill = elevation), show.legend = F, alpha = 0.4)+
scale_fill_gradient(low = 'white', high = 'black')+
geom_sf(data = YRB_flowlines, color = "royalblue", alpha = 0.8)+
new_scale_fill()+
geom_sf(data = ER_sf, aes(color = Total_Oxygen_Consumed_g_per_m2_per_day, size = Total_Oxygen_Consumed_g_per_m2_per_day), show.legend = T) +
scale_fill_viridis(option = 'B', begin = 0.3)+
scale_color_viridis(option = 'B', begin = 0.3)+
scale_size(range = c(3, 8), trans = 'reverse')+
theme_map() +
labs(x = "", y = "", color = "Total Oxygen\nConsumed\n(g O2 m2 day-1)") +
ggspatial::annotation_scale(
location = "br",
pad_x = unit(0.5, "in"),
bar_cols = c("black", "white")) +
ggspatial::annotation_north_arrow(
location = "tr", which_north = "true",
pad_x = unit(2, "in"),
# pad_y = unit(0.5, "in"),
style = ggspatial::north_arrow_nautical(
fill = c("black", "white"),
line_col = "grey20"))
ggsave('./Maps/SSS_ER_Total_Predicted_Map.pdf',
ER_tot_pred_map,
width = 10,
height = 5
)
>>>>>>> c3cab82291f6d8321f139e93f84d485ccd7a53e7
